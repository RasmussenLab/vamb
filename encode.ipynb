{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lessons from gridsearch\n",
    "\n",
    "No dropout\n",
    "\n",
    "No warmup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch.Autograd'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4028f89df1b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_np\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_torchdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_Variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__package__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m__package__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch.Autograd'"
     ]
    }
   ],
   "source": [
    "import pandas as _pd\n",
    "import torch as _torch\n",
    "import numpy as _np\n",
    "import torch.utils.data as _torchdata\n",
    "\n",
    "if __package__ is None or __package__ == '':\n",
    "    import vambtools as _vambtools\n",
    "    \n",
    "else:\n",
    "    import vamb.vambtools as _vambtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(depthspath, tnfpath, use_cuda, batchsize, shuffle):\n",
    "    depths = _load_tsv(depthspath)\n",
    "    tnf = _load_tsv(tnfpath)\n",
    "    \n",
    "    dataset = TensorDataset(data_tensor, tnf_tensor)\n",
    "   \n",
    "    return _torchdata.DataLoader(dataset=dataset, batch_size=batchsize, shuffle=shuffle,\n",
    "                          num_workers=1, pin_memory=use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(_torch.nn.Module):\n",
    "    \"\"\"Variational autoencoder\"\"\"\n",
    "    \n",
    "    def __init__(self, nsamples, nhiddens, nlatent):\n",
    "        if len(nhiddens) < 1:\n",
    "            raise ValueError('Must have at least one hidden layer')\n",
    "        \n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Initialize normal attributes\n",
    "        self.nsamples = nsamples\n",
    "        self.nfeatures = self.nsamples + 136\n",
    "        self.nhiddens = nhiddens\n",
    "        self.nlatent = nlatent\n",
    "        \n",
    "        # Lists for holding encoder and decoder hidden layers\n",
    "        self.encoder_hidden_linears = _torch.nn.ModuleList()\n",
    "        self.decoder_hidden_linears = _torch.nn.ModuleList()\n",
    "        \n",
    "        self.encoder_hidden_batchnorms = _torch.nn.ModuleList()\n",
    "        self.decoder_hidden_batchnorms = _torch.nn.ModuleList()\n",
    "        \n",
    "        # Add first encoding hidden layer\n",
    "        first_linear = _torch.nn.Linear(self.nfeatures, self.nhiddens[0])\n",
    "        first_batchnorm = _torch.nn.BatchNorm1d(self.nhiddens[0])\n",
    "        self.encoder_hidden_linears.append(first_linear)\n",
    "        self.encoder_hidden_batchnorms.append(first_batchnorm)\n",
    "        \n",
    "        # Add the next hidden layers, if any\n",
    "        for ninput, noutput in zip(self.nhiddens, self.nhiddens[1:]):\n",
    "            linear = _torch.nn.Linear(ninput, noutput)\n",
    "            batchnorm = _torch.nn.BatchNorm1d(noutput)\n",
    "            \n",
    "            self.encoder_hidden_linears.append(linear)\n",
    "            self.encoder_hidden_batchnorms.append(batchnorm)\n",
    "            \n",
    "        # Latent layers\n",
    "        self.mu = _torch.nn.Linear(self.nhiddens[-1], self.nlatent)\n",
    "        self.logsigma = _torch.nn.Linear(self.nhiddens[-1], self.nlatent)\n",
    "        \n",
    "        # Now add the decoding layers\n",
    "        first_linear = _torch.nn.Linear(self.nlatent, self.nhiddens[-1])\n",
    "        first_batchnorm = _torch.nn.BatchNorm1d(self.nhiddens[-1])\n",
    "        self.decoder_hidden_linears.append(first_linear)\n",
    "        self.decoder_hidden_batchnorms.append(first_batchnorm)\n",
    "        \n",
    "        for ninput, noutput in zip(self.nhiddens[-1, -1, -1], self.nhiddens[-2, -1, -1]):\n",
    "            linear = _torch.nn.Linear(ninput, noutput)\n",
    "            batchnorm = _torch.nn.BatchNorm1d(noutput)\n",
    "            \n",
    "            self.decoder_hidden_linears.append(linear)\n",
    "            self.decoder_hidden_batchnorms.append(batchnorm)\n",
    "            \n",
    "        # Final output layer\n",
    "        self.reconstructed = _torch.nn.Linear(self.nhidden[0], self.nfeatures)\n",
    "\n",
    "        # Activator functions (these are layers)\n",
    "        self.relu = _torch.nn.LeakyReLU()\n",
    "        self.softplus = _torch.nn.Softplus()\n",
    "        \n",
    "    def encode(self, data):\n",
    "        for linear, batchnorm in zip(self.encoder_hidden_linears, self.encoder_hidden_batchnorms):\n",
    "            data = linear(data)\n",
    "            data = self.relu(data)\n",
    "            data = batchnorm(data)\n",
    "            \n",
    "        mu = self.mu(data)\n",
    "        logsigma = self.logsigma(data)\n",
    "        logsigma = self.softplus(logsigma)\n",
    "        \n",
    "        return mu, logsigma\n",
    "    \n",
    "    def reparameterize(self, mu, logsigma, usecuda):\n",
    "        epsilon = _torch.randn(*mu.size)\n",
    "        \n",
    "        if usecuda:\n",
    "            epsilon = epsilon.cuda()\n",
    "        \n",
    "        epsilon = _torch.autograd.Variable(epsilon)\n",
    "        \n",
    "        latent = mu + epsilon * _torch.exp(logsigma / 2)\n",
    "        \n",
    "        return latent\n",
    "    \n",
    "    def decode(self, latent):\n",
    "        for linear, batchnorm in zip(self.decoder_hidden_linears, self.decoder_hidden_batchnorms):\n",
    "            data = linear(data)\n",
    "            data = self.relu(data)\n",
    "            data = batchnorm(data)\n",
    "            \n",
    "        latent_both = self.reconstructed(data)\n",
    "        \n",
    "        reconstructed_depths = _torch.nn.functional.softmax(latent_both.narrow(1, 0, self.nsamples), dim=1)\n",
    "        reconstructed_tnf = latent_both.narrow(1, self.nsamples, 136)\n",
    "    \n",
    "        return reconstructed_depths, reconstructed_tnf\n",
    "    \n",
    "    def forward(self, depths, tnf):\n",
    "        data = _torch.cat((depths, tnf), 1)\n",
    "        mu, logsigma = self.encode(data)\n",
    "        latent = self.reparameterize(mu, logsigma)\n",
    "        reconstructed_depths, reconstructed_tnf = self.decode(latent)\n",
    "        \n",
    "        return reconstructed_depths, reconstructed_tnf, mu, logsigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'Autograd'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5c317a59e603>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutograd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'Autograd'"
     ]
    }
   ],
   "source": [
    "torch.Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(depths, depths_rec, tnf, tnf_rec, mu, logsigma, tnf_factor):\n",
    "    MSE = _torch.nn.MSELoss() \n",
    "    mse = _torch.nn.MSELoss(tnf_rec, tnf)\n",
    "    bce = _torch.nn.functional.binary_cross_entropy(depths_rec, depths, size_average=False)\n",
    "    kld = -0.5 * _torch.mean(1 + logsigma - mu.pow(2) - logsigma.exp())\n",
    "    loss = bce + kld + tnf_factor * mse\n",
    "    \n",
    "    return loss, bce, mse, kld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, epoch, dataloader, optimizer, tnf_factor, usecuda):\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_bce = 0\n",
    "    epoch_mse = 0\n",
    "    epoch_kld = 0\n",
    "    \n",
    "    # Get mini batches\n",
    "    for nbatch, (depths, tnf) in enumerate(dataloader):\n",
    "        # Get the data\n",
    "        depths = _torch.autograd.Variable(depths)\n",
    "        tnf = _torch.autograd.Variable(tnf)\n",
    "        \n",
    "        if usecuda:\n",
    "            depths = depths.cuda()\n",
    "            tnf = tnf.cuda()\n",
    "        \n",
    "        # Do not carry over from previous mini batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Do a forward pass\n",
    "        depths_rec, tnf_rec, mu, logsigma = model(depths, tnf)\n",
    "        loss, bce, mse, kld = calculate_loss(depths, depths_rec, tnf, tnf_rec, mu, logsigma, tnf_factor)\n",
    "        \n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Keep track of per-epoch statistics\n",
    "        epoch_loss += loss.data[0]\n",
    "        epoch_bce += bce.data[0]\n",
    "        epoch_mse += mse.data[0]\n",
    "        epoch_kld += kld.data[0]\n",
    "        \n",
    "        # Print per-100 minibatches statistics\n",
    "        if nbatch % 100 == 0:\n",
    "            message = '\\t{:{width}} / {}:\\tLoss: {:.4f}\\tBCE: {:.4f}\\tMSE: {:.4f}\\tKLD: {:.4f}'.format(\n",
    "                       nbatch * len(depths),\n",
    "                       len(dataloader.dataset),\n",
    "                       loss.data[0] / len(depths),\n",
    "                       bce.data[0] / len(depths),\n",
    "                       mse.data[0] / len(depths),\n",
    "                       kld.data[0] / len(depths),\n",
    "                       width=len(str(len(dataloader.dataset))))\n",
    "            \n",
    "            print(message)\n",
    "            \n",
    "    # Print epoch statistics\n",
    "    message = '\\nEpoch {}:\\tLoss: {:.4f}\\tBCE: {:.4f}\\tMSE: {:.4f}\\tKLD: {:.4f}'.format(\n",
    "           epoch,\n",
    "           loss.data[0] / len(depths),\n",
    "           bce.data[0] / len(depths),\n",
    "           mse.data[0] / len(depths),\n",
    "           kld.data[0] / len(depths))\n",
    "    \n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "   '''Main program'''\n",
    "   \n",
    "   # get number of features\n",
    "   n_features = []\n",
    "   n_features.append(get_features(args.i[0]))\n",
    "   n_features.append(get_features(args.tnf[0]))\n",
    "   \n",
    "   # load data for training\n",
    "   if args.to_memory:\n",
    "      data_loader = load_data_mem(args)\n",
    "   else:\n",
    "      data_loader = load_data(args)\n",
    "   \n",
    "   # define model and optimizer\n",
    "   model = VAE(n_features, args.dp_rate)\n",
    "   if args.cuda:\n",
    "      model.cuda()\n",
    "   optimizer = optim.Adam(model.parameters(), lr=args.lrate)\n",
    "   \n",
    "   # set beta\n",
    "   if args.warmup == 0:\n",
    "      beta = 1\n",
    "      to_add = 0\n",
    "   else:\n",
    "      to_add = 1/args.warmup\n",
    "      beta = 0\n",
    "   \n",
    "   # train\n",
    "   for epoch in range(1, args.nepochs + 1):\n",
    "      train(epoch, model, data_loader, optimizer, beta, args.mse)\n",
    "      if beta >= 1:\n",
    "         beta = 1\n",
    "      else:\n",
    "         beta = beta + to_add\n",
    "   \n",
    "   # evaluate\n",
    "   # currently data does not need to be reloaded because input-data is already shuffled #\n",
    "   # load data for eval\n",
    "   #if args.to_memory:\n",
    "   #   data_loader_eval = load_data_mem(args, training=False)\n",
    "   #else:\n",
    "   #   data_loader_eval = load_data(args, training=False)\n",
    "   #evaluate(model, data_loader_eval, beta, args.o)\n",
    "   evaluate(model, data_loader, beta, args.mse, args.o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   5'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 5\n",
    "'{:4}'.format(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, data_loader, optimizer, beta, mse):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    KLD_loss = 0\n",
    "    BCE_loss = 0\n",
    "    MSE_loss = 0\n",
    "    for batch_idx, (data,tnf) in enumerate(data_loader):\n",
    "        data = Variable(data)\n",
    "        tnf = Variable(tnf)\n",
    "        if args.cuda:\n",
    "            data = data.cuda()\n",
    "            tnf = tnf.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        y_data, y_tnf, mu, logvar = model(data, tnf)\n",
    "        BCE, KLD, loss, MSE = loss_function(y_data, data, y_tnf, tnf, mu, logvar, beta, mse)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.data[0]\n",
    "        KLD_loss += KLD.data[0]\n",
    "        BCE_loss += BCE.data[0]\n",
    "        MSE_loss += MSE.data[0]\n",
    "        if batch_idx % 50 == 0:\n",
    "         print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.4f}\\tBCE: {:.4f}\\tKLD: {:.4f}\\tBeta: {:.2f}'.format(\n",
    "            epoch, batch_idx * len(data), len(data_loader.dataset),\n",
    "            100. * batch_idx / len(data_loader),\n",
    "            loss.data[0] / len(data),\n",
    "            BCE.data[0] / len(data),\n",
    "            KLD.data[0] / len(data),\n",
    "            beta))\n",
    "\n",
    "    print('====> Epoch: {}\\tLoss: {:.4f}\\tBCE: {:.4f}\\tKLD: {:.4f}\\tMSE: {:.5f}\\tBeta: {:.2f}'.format(\n",
    "          epoch, train_loss / len(data_loader.dataset),\n",
    "          BCE_loss / len(data_loader.dataset),\n",
    "          KLD_loss / len(data_loader.dataset),\n",
    "          MSE_loss / len(data_loader.dataset),\n",
    "          beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch.nn.functional.binary_cross_entropy(input, target, weight=None, size_average=True)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_torch.nn.functional.binary_cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_tsv(path, skiprows=1, skipcols=1):\n",
    "    with _vambtools.Reader(path) as filehandle:\n",
    "        firstline = next(filehandle)\n",
    "    \n",
    "    ncolumns = len(firstline.split('\\t'))\n",
    "    \n",
    "    if skipcols >= ncolumns:\n",
    "        raise ValueError('Cannot skip {} of {} columns in {}'.format(\n",
    "            skipcols, ncolumns, path))\n",
    "        \n",
    "    columns = list(range(skipcols, ncolumns))\n",
    "    \n",
    "    dataframe = _pd.read_csv(path, delimiter='\\t', header=None,\n",
    "                        skiprows=skiprows, usecols=columns, dtype=_np.float32)\n",
    "    \n",
    "    return _torch.from_numpy(dataframe.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.4182  0.2403  0.1667  0.0971  0.0777\n",
       " 0.3748  0.2163  0.1910  0.1362  0.0816\n",
       " 0.3861  0.2911  0.1629  0.0910  0.0689\n",
       "                   ⋮                    \n",
       " 0.2300  0.2188  0.1409  0.1794  0.2309\n",
       " 0.1235  0.3253  0.2218  0.1845  0.1449\n",
       " 0.2790  0.2013  0.1419  0.0808  0.2970\n",
       "[torch.FloatTensor of size 275285x5]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_mem(args, training=True):\n",
    "   '''Data loader to memory'''\n",
    "   \n",
    "   data_tensor = torch.Tensor(np.asarray(pd.read_csv(args.i[0], delim_whitespace=True, header=None)))\n",
    "   tnf_tensor = torch.Tensor(np.asarray(pd.read_csv(args.tnf[0], delim_whitespace=True, header=None)))\n",
    "   dataset = TensorDataset(data_tensor, tnf_tensor)\n",
    "   \n",
    "   if args.cuda:\n",
    "      kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "   else:\n",
    "      kwargs = {'num_workers': 1, 'pin_memory': False}\n",
    "   \n",
    "   if training:\n",
    "      data_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=args.batch, shuffle=False, **kwargs)\n",
    "   else:\n",
    "      data_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=args.batch, shuffle=False, **kwargs)\n",
    "   return data_loader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
