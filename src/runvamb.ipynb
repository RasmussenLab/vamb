{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__doc__ = \"\"\"Run the entire Vamb pipiline.\n",
    "\n",
    "Usage:\n",
    "Given a path to contigs and a list/tuple of paths to bamfiles\n",
    ">>> run(outdir, contigpath, bampaths, **kwargs)\n",
    "\n",
    "Creates a new directory and runs each module of the Vamb pipeline in the\n",
    "new directory. Does not yet support resuming stopped runs - in order to do so,\n",
    "you must run each module independently from commandline or from Python.\n",
    "\n",
    "See the tutorial at https://github.com/jakobnissen/vamb\n",
    "\"\"\"\n",
    "\n",
    "__cmd_doc__ = \"\"\"Run the Vamb pipeline.\n",
    "\n",
    "Creates a new direcotry and runs each module of the Vamb pipeline in the\n",
    "new directory. Does not yet support resuming stopped runs - in order to do so,\n",
    "you must run each module independently from commandline or from Python\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all Vamb modules\n",
    "import sys as _sys\n",
    "import os as _os\n",
    "import numpy as _np\n",
    "import torch as _torch\n",
    "\n",
    "if __package__ is None or __package__ == '':\n",
    "    import vambtools as _vambtools\n",
    "    import parsecontigs as _parsecontigs\n",
    "    import parsebam as _parsebam\n",
    "    import encode as _encode\n",
    "    import cluster as _cluster\n",
    "    import threshold as _threshold\n",
    "    \n",
    "else:\n",
    "    import vamb.vambtools as _vambtools\n",
    "    import vamb.parsecontigs as _parsecontigs\n",
    "    import vamb.parsebam as _parsebam\n",
    "    import vamb.encode as _encode\n",
    "    import vamb.cluster as _cluster\n",
    "    import vamb.threshold as _threshold\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_PROCESSES = min(8, _os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _checkpaths(outdir, contigspath, bampaths):\n",
    "    # Check presence of outdir and its parent directory\n",
    "    if _os.path.exists(outdir):\n",
    "        raise FileExistsError(outdir)\n",
    "    \n",
    "    parentdir = _os.path.dirname(outdir)\n",
    "    if parentdir and not _os.path.isdir(parentdir):\n",
    "        raise NotADirectoryError(parentdir)\n",
    "        \n",
    "    # Check presence of contigs\n",
    "    if not _os.path.isfile(contigspath):\n",
    "        raise FileNotFoundError(contigspath)\n",
    "        \n",
    "    # Check presence of bamfiles\n",
    "    for bampath in bampaths:\n",
    "        if not _os.path.isfile(bampath):\n",
    "            raise FileNotFoundError(bampath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(outdir, contigspath, bampaths, minlength=100, minascore=50,\n",
    "        readprocesses=DEFAULT_PROCESSES, nhiddens=[325, 325], nlatent=40,\n",
    "       nepochs=300, batchsize=100, cuda=False, tnfweight=1.0, lrate=1e-4,\n",
    "       verbose=True, tandemcluster=False, max_steps=15, nsamples=1000,\n",
    "       min_clustersize=5):\n",
    "    \"placeholder docstring\"\n",
    "    \n",
    "    ############## NOTE #############################\n",
    "    #\n",
    "    # Things to add/change:\n",
    "    # - Needed to dump RPKM and reload it? What is mem requirement?\n",
    "    # - Maybe dump trained weights?\n",
    "    #\n",
    "    #################################################\n",
    "    \n",
    "    _checkpaths(outdir, contigspath, bampaths)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Creating output directory', end='\\n\\n')\n",
    "    _os.mkdir(outdir)\n",
    "    \n",
    "    # Get contignames, contiglengths and TNF\n",
    "    if verbose:\n",
    "        print('Computing TNF from contigs file', end='\\n\\n')\n",
    "        \n",
    "    with _vambtools.Reader(contigspath, 'rb') as contigsfile:\n",
    "        tnfs, contignames, lengths = _parsecontigs.read_contigs(contigsfile, minlength=minlength)\n",
    "    \n",
    "    del lengths\n",
    "    \n",
    "    # Get RPKMS\n",
    "    if verbose:\n",
    "        print('Computing depths from BAM files', end='\\n\\n')\n",
    "    \n",
    "    sample_rpkms, contignames2 = _parsebam.read_bamfiles(bampaths, minscore=minascore,\n",
    "                                                       minlength=minlength, processes=readprocesses)\n",
    "    \n",
    "    assert contignames == contignames2\n",
    "    del contignames2\n",
    "    contignames = _np.array(contignames)\n",
    "    \n",
    "    # Save RPKMs, delete and load into Numpy array to save memory (needed?)\n",
    "    npzpath = _os.path.join(outdir, 'sample_rpkms.npz')\n",
    "    \n",
    "    _parsebam.write_samplerpkm(npzpath, sample_rpkms)\n",
    "    del sample_rpkms\n",
    "    \n",
    "    rpkms = _parsebam.read_npz(npzpath, bampaths)\n",
    "    \n",
    "    # Construct VAE and train it\n",
    "    if verbose:\n",
    "        print('Making and training VAE on data.')\n",
    "    \n",
    "    vae, dataloader = _encode.trainvae(rpkms, tnfs, nhiddens=nhiddens, nlatent=nlatent,\n",
    "                                      nepochs=nepochs, batchsize=batchsize, cuda=cuda,\n",
    "                                      tnfweight=tnfweight, lrate=lrate, verbose=verbose)\n",
    "    \n",
    "    # Feed dataloader to VAE\n",
    "    if verbose:\n",
    "        print('\\nEncoding data to latent representation', end='\\n\\n')\n",
    "    \n",
    "    latent = vae.encode(dataloader)\n",
    "    \n",
    "    del tnfs, rpkms\n",
    "    \n",
    "    _vambtools.zscore(latent, axis=1, inplace=True)\n",
    "    \n",
    "    # Find threshold\n",
    "    _ = _threshold.getthreshold(latent, samples=nsamples, normalized=True)\n",
    "    threshold, support, separated = _\n",
    "    \n",
    "    if verbose:\n",
    "        print('Estimated clustering threshold', round(threshold, 3))\n",
    "    \n",
    "    if separated < 0.25:\n",
    "        wn = ('Warning: {} % of sampled contigs has '\n",
    "              'well-separated threshold').format(round(separated * 100, 1))\n",
    "        print(wn, file=_sys.stderr)\n",
    "        \n",
    "    if support < 0.50:\n",
    "        wn = ('Warning: {} % of sampled contigs has '\n",
    "              '*any* observable threshold').format(round(support * 100, 1))\n",
    "        print(wn, file=_sys.stderr)\n",
    "    \n",
    "    # Cluster and write\n",
    "    if verbose:\n",
    "        print('Clustering latent representation', end='\\n\\n')\n",
    "    \n",
    "    if tandemcluster:\n",
    "        clusters = _cluster.tandemcluster(latent, contignames, threshold,\n",
    "                                          max_steps=max_steps, normalized=True)\n",
    "        \n",
    "    else:\n",
    "        clusterit = _cluster.cluster(latent, contignames, threshold,\n",
    "                                          max_steps=max_steps, normalized=True)\n",
    "        clusters = dict()\n",
    "        \n",
    "        for medoid, cluster in clusterit:\n",
    "            clusters[medoid] = cluster\n",
    "    \n",
    "    clusterpath = _os.path.join(outdir, 'clusters.tsv')\n",
    "    \n",
    "    with open(clusterpath, 'w') as clusterfile:\n",
    "        _cluster.writeclusters(clusterfile, clusters, min_size=min_clustersize)\n",
    "        \n",
    "    if verbose:\n",
    "        print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.__doc__ = \"\"\"Run the entire Vamb pipeline.\n",
    "\n",
    "Inputs:\n",
    "    outdir: Directory to create and put outputs\n",
    "    contigspath: Path to contigs FASTA file\n",
    "    bampaths: Tuple or lists of paths to BAM files\n",
    "\n",
    "    minlength: Ignore sequences with length below this [100]\n",
    "    minascore: Ignore reads with alignment score below this [50]\n",
    "    readprocesses: N processes to spawn reading BAMs [{}]\n",
    "    nhiddens: List of number of neurons in hidden layers [325, 325]\n",
    "    nlatent: Number of neurons in latent layer [40]\n",
    "    nepochs: Number of training epochs [300]\n",
    "    batchsize: Sequences per mini-batch when training [100]\n",
    "    cuda: Use CUDA (GPU acceleration software) [False]\n",
    "    tnfweight: Relative weight of TNF error when computing loss [1.0]\n",
    "    lrate: Learning rate for the optimizer [1e-4]\n",
    "    verbose: Print progress to STDOUT [True]\n",
    "    tandemcluster: Use fast but inaccurate clustering [False]\n",
    "    max_steps: Stop searching for optimal medoid after N futile attempts [15]\n",
    "    nsamples: Number of samples to estimate clustering threshold [1000]\n",
    "    min_clustersize: Ignore all clusters with fewer sequences than this [5]\n",
    "\n",
    "Output: None, but creates directory with clusters.tsv file.\n",
    "\"\"\".format(DEFAULT_PROCESSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    ################ Create parser ############################\n",
    "    \n",
    "    usage = \"python runvamb.py OUTPATH FASTA BAMPATHS [OPTIONS ...]\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=__cmd_doc__,\n",
    "        formatter_class=argparse.RawDescriptionHelpFormatter,\n",
    "        usage=usage, add_help=False)\n",
    "    \n",
    "    # Help\n",
    "    helpos = parser.add_argument_group(title='Help', description=None)\n",
    "    helpos.add_argument('-h', '--help', help='print help and exit', action='help')\n",
    "    \n",
    "    # Positional arguments\n",
    "    reqos = parser.add_argument_group(title='Required arguments', description=None)\n",
    "    reqos.add_argument('outpath', help='output directory to create')\n",
    "    reqos.add_argument('fasta', help='path to fasta file')\n",
    "    reqos.add_argument('bamfiles', help='path to BAM files', nargs='+')\n",
    "    \n",
    "    # Optional arguments\n",
    "    inputos = parser.add_argument_group(title='IO options options', description=None)\n",
    "    \n",
    "    inputos.add_argument('-m', dest='minlength', metavar='', type=int, default=[100],\n",
    "                         help='ignore sequences shorter than this [100]')\n",
    "    inputos.add_argument('-a', dest='minascore', metavar='', type=int, default=[50],\n",
    "                         help='ignore reads with alignment score below this [50]')\n",
    "    inputos.add_argument('-p', dest='processes', metavar='', type=int, default=DEFAULT_PROCESSES,\n",
    "                         help=('reading processes to spawn '\n",
    "                              '[min(' + str(DEFAULT_PROCESSES) + ', nbamfiles)]'))\n",
    "    inputos.add_argument('--silent', help='Run silently [False]', action='store_false')\n",
    "    \n",
    "    vambos = parser.add_argument_group(title='Training options', description=None)\n",
    "    \n",
    "    vambos.add_argument('-n', dest='nhiddens', metavar='', type=int, nargs='+',\n",
    "                        default=[325, 325], help='hidden neurons [325 325]')\n",
    "    vambos.add_argument('-l', dest='nlatent', metavar='', type=int,\n",
    "                        default=40, help='latent neurons [40]')\n",
    "    vambos.add_argument('-e', dest='nepochs', metavar='', type=int,\n",
    "                        default=300, help='epochs [300]')\n",
    "    vambos.add_argument('-b', dest='batchsize', metavar='', type=int,\n",
    "                        default=100, help='batch size [100]')\n",
    "    vambos.add_argument('-t', dest='tnfweight',  metavar='',type=float,\n",
    "                        default=1.0, help='TNF weight [1.0]')\n",
    "    vambos.add_argument('-r', dest='lrate', metavar='', type=float,\n",
    "                        default=0.0001, help='learning rate [1e-4]')\n",
    "    vambos.add_argument('--cuda', help='use GPU [False]', action='store_true')\n",
    "    \n",
    "    clusto = parser.add_argument_group(title='Clustering options', description=None)\n",
    "    \n",
    "    clusto.add_argument('-x', dest='max_steps', metavar='', type=int,\n",
    "                        default=15, help='maximum clustering steps [15]')\n",
    "    clusto.add_argument('-s', dest='nsamples', metavar='', type=int, default=1000,\n",
    "                        help='n_samples to determine cluster threshold [1000]')\n",
    "    clusto.add_argument('--tandem', help='tandem cluster [False]', action='store_true')\n",
    "    \n",
    "    \n",
    "    # If no arguments, print help\n",
    "    if len(_sys.argv) == 1:\n",
    "        parser.print_help()\n",
    "        _sys.exit()\n",
    "        \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Check inputs\n",
    "    \n",
    "    _checkpaths(args.outpath, args.fasta, args.bamfiles)\n",
    "    \n",
    "    # Check IO options\n",
    "    \n",
    "    if args.minlength < 4:\n",
    "        raise ValueError('Minlength must be at least 4')\n",
    "    \n",
    "    if args.processes < 1:\n",
    "        raise argsparse.ArgumentTypeError('Zero or negative processes requested.')\n",
    "        \n",
    "    # Check training options\n",
    "    \n",
    "    if any(i < 1 for i in args.nhiddens):\n",
    "        raise ValueError('Minimum 1 neuron per layer, not {}'.format(min(args.hidden)))\n",
    "        \n",
    "    if args.nlatent < 1:\n",
    "        raise ValueError('Minimum 1 latent neuron, not {}'.format(args.latent))\n",
    "        \n",
    "    if args.nepochs < 1:\n",
    "        raise ValueError('Minimum 1 epoch, not {}'.format(args.nepochs))\n",
    "        \n",
    "    if args.batchsize < 1:\n",
    "        raise ValueError('Minimum batchsize of 1, not {}'.format(args.batchsize))\n",
    "        \n",
    "    if args.tnfweight < 0:\n",
    "        raise ValueError('TNF weight cannot be negative')\n",
    "        \n",
    "    if args.lrate < 0:\n",
    "        raise ValueError('Learning rate cannot be negative')\n",
    "        \n",
    "    if args.cuda and not _torch.cuda.is_available():\n",
    "        raise ModuleNotFoundError('Cuda is not available for PyTorch')\n",
    "        \n",
    "    # Check clustering options\n",
    "    if args.nsamples < 5:\n",
    "        raise ValueError('Must use at least 5 samples to determine threshold')\n",
    "    \n",
    "    if args.max_steps < 1:\n",
    "        raise ValueError('Max steps must be 1 or above.')\n",
    "        \n",
    "    # Run the program\n",
    "    run(args.outpath, args.fasta, args.bamfiles, minlength=args.minlength,\n",
    "        minascore=args.minascore,\n",
    "        readprocesses=args.processes,\n",
    "        nhiddens=args.nhiddens,\n",
    "        nlatent=args.nlatent,\n",
    "        nepochs=args.nepochs,\n",
    "        batchsize=args.batchsize,\n",
    "        cuda=args.cuda,\n",
    "        tnfweight=args.tnfweight,\n",
    "        lrate=args.lrate,\n",
    "        verbose=True,\n",
    "        tandemcluster=args.tandem,\n",
    "        max_steps=args.max_steps,\n",
    "        nsamples=args.nsamples,\n",
    "        min_clustersize=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
