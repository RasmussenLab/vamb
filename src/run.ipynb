{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../..')\n",
    "import vamb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_PROCESSES = min(os.cpu_count(), 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(outdir, fastapath, bampaths, mincontiglength, minalignscore, subprocesses,\n",
    "         nhiddens, nlatent, nepochs, batchsize, cuda, errorsum, mseratio,\n",
    "         minclustersize, maxclusters, logfile):\n",
    "    # Assume all files are as they should be\n",
    "    \n",
    "    # Print starting vamb version ...\n",
    "    print('Starting Vamb version ', '.'.join(map(str, vamb.__version__)), file=logfile)\n",
    "    print('\\tDate and time is', datetime.datetime.now(), file=logfile)\n",
    "    starttime = time.time()\n",
    "    \n",
    "    # Get TNFs, save as npz\n",
    "    print('\\nCalculating TNF', file=logfile)\n",
    "    \n",
    "    with open(fastapath, 'rb') as tnffile:\n",
    "        tnfs, contignames, contiglengths = vamb.parsecontigs.read_contigs(tnffile, minlength=mincontiglength)\n",
    "    del contiglengths\n",
    "    \n",
    "    vamb.vambtools.write_npz(os.path.join(outdir, 'tnf.npz'), tnfs)\n",
    "    tnfdonetime = time.time()\n",
    "    seconds = round(tnfdonetime - starttime, 2)\n",
    "    print('\\tCalculated TNF in {} seconds.'.format(seconds), file=logfile)\n",
    "    \n",
    "    # Parse BAMs, save as npz\n",
    "    print('\\nParsing {} BAM files with {} subprocesses.'.format(len(bampaths), subprocesses), file=logfile)\n",
    "    rpkms = vamb.parsebam.read_bamfiles(bampaths, minalignscore, mincontiglength, subprocesses, logfile=logfile)\n",
    "    \n",
    "    if len(rpkms) != len(contignames):\n",
    "        raise ValueError('Number of FASTA vs BAM file headers do not match. '\n",
    "                         'Are you sure the BAM files originate from same FASTA file '\n",
    "                         'and have headers?')\n",
    "        \n",
    "    vamb.vambtools.write_npz(os.path.join(outdir, 'rpkm.npz'), rpkms)\n",
    "    bamdonetime = time.time()\n",
    "    seconds = round(bamdonetime - tnfdonetime, 2)\n",
    "    print('\\tCalculated RPKM in {} seconds.'.format(seconds), file=logfile)\n",
    "    \n",
    "    # Train, save model\n",
    "    print('\\nTraining VAE', file=logfile)\n",
    "    modelpath = os.path.join(outdir, 'model')\n",
    "    vae, dataloader = vamb.encode.trainvae(rpkms, tnfs, nhiddens=nhiddens, nlatent=nlatent,\n",
    "                                          nepochs=nepochs, batchsize=batchsize, cuda=cuda,\n",
    "                                          errorsum=errorsum, mseratio=mseratio, verbose=True,\n",
    "                                          logfile=logfile, modelfile=modelpath)\n",
    "    \n",
    "    latent = vae.encode(dataloader)\n",
    "    vamb.vambtools.write_npz(os.path.join(outdir, 'latent.npz'), latent)\n",
    "    del tnfs, rpkms, dataloader\n",
    "    \n",
    "    encodedonetime = time.time()\n",
    "    seconds = round(encodedonetime - bamdonetime, 2)\n",
    "    print('\\tTrained VAE and encoded in {} seconds.'.format(seconds), file=logfile)\n",
    "    \n",
    "    # Cluster, save tsv file\n",
    "    print('\\nClustering', file=logfile)\n",
    "    clusteriterator = vamb.cluster.cluster(latent, labels=contignames, logfile=logfile)\n",
    "    \n",
    "    with open(os.path.join(outdir, 'clusters.tsv'), 'w') as clustersfile:\n",
    "        vamb.cluster.write_clusters(clustersfile, clusteriterator, max_clusters=maxclusters, min_size=minclustersize)\n",
    "    \n",
    "    clusterdonetime = time.time()\n",
    "    seconds = round(clusterdonetime - encodedonetime, 2)\n",
    "    print('\\tClustered contigs in {} seconds.'.format(seconds), file=logfile)\n",
    "    \n",
    "    seconds = round(clusterdonetime - starttime, 2)\n",
    "    print('\\nDone with Vamb in {} seconds.'.format(seconds), file=logfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__cmd_doc__ = \"\"\"Run the Vamb pipeline.\n",
    "\n",
    "For advanced use and extensions of Vamb, check documentation of the package\n",
    "at https://github.com/jakobnissen/vamb.\n",
    "\"\"\"\n",
    "usage = \"python runvamb.py OUTPATH FASTA BAMPATHS [OPTIONS ...]\"\n",
    "parser = argparse.ArgumentParser(\n",
    "    description=__cmd_doc__,\n",
    "    formatter_class=argparse.RawDescriptionHelpFormatter,\n",
    "    usage=usage, add_help=False)\n",
    "\n",
    "# Help\n",
    "helpos = parser.add_argument_group(title='Help', description=None)\n",
    "helpos.add_argument('-h', '--help', help='print help and exit', action='help')\n",
    "\n",
    "# Positional arguments\n",
    "reqos = parser.add_argument_group(title='Required arguments', description=None)\n",
    "reqos.add_argument('outdir', help='output directory to create')\n",
    "reqos.add_argument('fasta', help='path to fasta file')\n",
    "reqos.add_argument('bamfiles', help='path to BAM files', nargs='+')\n",
    "\n",
    "# Optional arguments\n",
    "inputos = parser.add_argument_group(title='IO options', description=None)\n",
    "\n",
    "inputos.add_argument('-m', dest='minlength', metavar='', type=int, default=100,\n",
    "                     help='ignore sequences shorter than this [100]')\n",
    "inputos.add_argument('-a', dest='minascore', metavar='', type=int, default=50,\n",
    "                     help='ignore reads with alignment score below this [50]')\n",
    "inputos.add_argument('-p', dest='subprocesses', metavar='', type=int, default=DEFAULT_PROCESSES,\n",
    "                     help=('reading subprocesses to spawn '\n",
    "                          '[min(' + str(DEFAULT_PROCESSES) + ', nbamfiles)]'))\n",
    "\n",
    "vambos = parser.add_argument_group(title='Training options', description=None)\n",
    "\n",
    "vambos.add_argument('-n', dest='nhiddens', metavar='', type=int, nargs='+',\n",
    "                    default=[325, 325], help='hidden neurons [325 325]')\n",
    "vambos.add_argument('-l', dest='nlatent', metavar='', type=int,\n",
    "                    default=40, help='latent neurons [40]')\n",
    "vambos.add_argument('-e', dest='nepochs', metavar='', type=int,\n",
    "                    default=300, help='epochs [300]')\n",
    "vambos.add_argument('-b', dest='batchsize', metavar='', type=int,\n",
    "                    default=100, help='batch size [100]')\n",
    "vambos.add_argument('-s', dest='errorsum',  metavar='',type=float,\n",
    "                    default=1000.0, help='Amount to learn [1000]')\n",
    "vambos.add_argument('-r', dest='mseratio',  metavar='',type=float,\n",
    "                    default=0.2, help='Weight of TNF versus depth [0.2]')\n",
    "vambos.add_argument('--cuda', help='use GPU [False]', action='store_true')\n",
    "\n",
    "clusto = parser.add_argument_group(title='Clustering options', description=None)\n",
    "clusto.add_argument('--tandem', help='use tandem clustering [False]', action='store_true')\n",
    "\n",
    "clusto.add_argument('-i', dest='minsize', metavar='', type=int,\n",
    "                    default=1, help='Minimum cluster size [1]')\n",
    "clusto.add_argument('-c', dest='maxclusters', metavar='', type=int,\n",
    "                    default=-1, help='Stop after c clusters [-1 = inf]')\n",
    "\n",
    "######################### PRINT HELP IF NO ARGUMENTS ###################\n",
    "if len(sys.argv) == 1:\n",
    "    parser.print_help()\n",
    "    sys.exit()\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "######################### CHECK INPUT/OUTPUT FILES #####################\n",
    "\n",
    "# Outdir does not exist\n",
    "if os.path.exists(args.outdir):\n",
    "    raise FileExistsError(args.outdir)\n",
    "\n",
    "# Outdir is in an existing parent dir\n",
    "parentdir = os.path.dirname(args.outdir)\n",
    "if parentdir and not os.path.isdir(parentdir):\n",
    "    raise NotADirectoryError(parentdir)\n",
    "\n",
    "# Contigs exists\n",
    "if not os.path.isfile(args.fasta):\n",
    "    raise FileNotFoundError(args.fasta)\n",
    "    \n",
    "# All bamfiles exists\n",
    "for bampath in args.bamfiles:\n",
    "    if not os.path.isfile(bampath):\n",
    "        raise FileNotFoundError(bampath)\n",
    "\n",
    "####################### CHECK ARGUMENTS FOR TNF AND BAMFILES ###########\n",
    "\n",
    "if args.minlength < 100:\n",
    "    raise argparse.ArgumentTypeError('Minimum contig length must be at least 100')\n",
    "\n",
    "if args.subprocesses < 1:\n",
    "    raise argparse.ArgumentTypeError('Zero or negative subprocesses requested.')\n",
    "    \n",
    "if args.minascore < 0:\n",
    "    raise argparse.ArgumentTypeError('Minimum alignment score cannot be negative')\n",
    "\n",
    "###################### CHECK TRAINING OPTIONS ####################\n",
    "\n",
    "if any(i < 1 for i in args.nhiddens):\n",
    "    raise argparse.ArgumentTypeError('Minimum 1 neuron per layer, not {}'.format(min(args.hidden)))\n",
    "\n",
    "if args.nlatent < 1:\n",
    "    raise argparse.ArgumentTypeError('Minimum 1 latent neuron, not {}'.format(args.latent))\n",
    "\n",
    "if args.nepochs < 1:\n",
    "    raise argparse.ArgumentTypeError('Minimum 1 epoch, not {}'.format(args.nepochs))\n",
    "\n",
    "if args.batchsize < 1:\n",
    "    raise argparse.ArgumentTypeError('Minimum batchsize of 1, not {}'.format(args.batchsize))\n",
    "\n",
    "if args.errorsum < 0:\n",
    "    raise argparse.ArgumentTypeError('Errorsum cannot be negative')\n",
    "    \n",
    "if args.mseratio <= 0 or args.mseratio >= 1:\n",
    "    raise argparse.ArgumentTypeError('MSE ratio must be above 0 and below 1')\n",
    "\n",
    "if args.cuda and not torch.cuda.is_available():\n",
    "    raise ModuleNotFoundError('Cuda is not available for PyTorch')\n",
    "    \n",
    "###################### CHECK CLUSTERING OPTIONS ####################\n",
    "\n",
    "if args.minsize < 1:\n",
    "    raise argparse.ArgumentTypeError('Minimum cluster size must be at least 0.')\n",
    "\n",
    "################### RUN PROGRAM #########################\n",
    "os.mkdir(args.outdir)\n",
    "logpath = os.path.join(args.outdir, 'log.txt')\n",
    "\n",
    "with open(logpath, 'w') as logfile:\n",
    "    main(args.outdir, args.fasta, args.bamfiles,\n",
    "         mincontiglength=args.minlength,\n",
    "         minalignscore=args.minascore,\n",
    "         subprocesses=args.subprocesses,\n",
    "         nhiddens=args.nhiddens,\n",
    "         nlatent=args.nlatent,\n",
    "         nepochs=args.nepochs,\n",
    "         batchsize=args.batchsize,\n",
    "         errorsum=args.errorsum,\n",
    "         mseratio=args.mseratio,\n",
    "         cuda=args.cuda,\n",
    "         minclustersize=args.minsize,\n",
    "         maxclusters=args.maxclusters,\n",
    "         logfile=logfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
