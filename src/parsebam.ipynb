{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metagenomic RPKM estimator\n",
    "\n",
    "# Author: Jakob Nybo Nissen, DTU Bioinformatics, jakni@bioinformatics.dtu.dk\n",
    "# Date: 2018-04-24\n",
    "    \n",
    "# This script calculates RPKM, when paired end reads are mapped to a contig\n",
    "# catalogue with BWA MEM. It will not be accurate with single end reads or\n",
    "# any other mapper than BWA MEM.\n",
    "\n",
    "# Theory:\n",
    "# We want a simple way to estimate abundance of redundant contig catalogues.\n",
    "# Earlier we used to run analysis on deduplicated gene catalogues, but since \n",
    "# both depth and kmer composition are only stable for longer contigs, we have\n",
    "# moved to contig catalogues. We have not found a way of deduplicating contigs.\n",
    "\n",
    "# For this we have until now used two methods:\n",
    "# 1) Only counting the primary hits. In this case the read will never be\n",
    "# assigned to any contig which differ by just 1 basepair. Even for\n",
    "# identical contigs, reads are assigned randomly which causes noise.\n",
    "\n",
    "# 2) Using MetaBAT's jgi_summarize_bam_contig_depths, a script which is not\n",
    "# documented and we cannot figure out how works. When testing with small\n",
    "# toy data, it produces absurd results.\n",
    "\n",
    "# This script is an attempt to take an approach as simple as possible while\n",
    "# still being sound technically. We simply count the number of reads in a\n",
    "# contig normalized by contig length and total number of reads.\n",
    "\n",
    "# We look at all hits, including secondary hits. We do not discount partial\n",
    "# alignments. Also, if a read maps to multiple contigs, we don't count each hit\n",
    "# as less than if it mapped to both. The reason for all these decisions is that\n",
    "# if the aligner believes it's a hit, we believe the contig is present.\n",
    "\n",
    "# We do not take varying insert sizes into account. It is unlikely that\n",
    "# any contig with enough reads to provide a reliable estimate of depth would,\n",
    "# by chance, only recruit read pairs with short or long insert size. So this\n",
    "# will average out over all contigs.\n",
    "\n",
    "# We DO filter the input file for duplicate lines, as BWA MEM erroneously\n",
    "# produces quite a few of them.\n",
    "\n",
    "# We count each read independently, because BWA MEM often assigns mating reads\n",
    "# to different contigs. If a read has their mate unmapped, we count it twice\n",
    "# to compensate (one single read corresponds to two paired reads)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "__doc__ = \"\"\"Estimate RPKM (depths) from BAM files of reads mapped to contigs.\n",
    "\n",
    "Usage:\n",
    ">>> bampaths = ['/path/to/bam1.bam', '/path/to/bam2.bam', '/path/to/bam3.bam']\n",
    ">>> rpkms = read_bamfiles(bampaths)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysam as _pysam\n",
    "\n",
    "import sys as _sys\n",
    "import os as _os\n",
    "import multiprocessing as _multiprocessing\n",
    "import numpy as _np\n",
    "import gzip as _gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SUBPROCESSES = min(8, _os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_all_references(alignedsegment):\n",
    "    \"\"\"Given a pysam aligned segment, returns a list with the names of all\n",
    "    references the read maps to, both primary and secondary hits.\n",
    "    \"\"\"\n",
    "    \n",
    "    references = [alignedsegment.reference_name]\n",
    "    \n",
    "    # Some reads don't have secondary hits\n",
    "    if not alignedsegment.has_tag('XA'):\n",
    "        return references\n",
    "    \n",
    "    # XA is a string contigname1,<other info>;contigname2,<other info>; ...\n",
    "    secondary_alignment_string = alignedsegment.get_tag('XA')\n",
    "    secondary_alignments = secondary_alignment_string.split(';')[:-1]\n",
    "    \n",
    "    for secondary_alignment in secondary_alignments:\n",
    "        references.append(secondary_alignment.partition(',')[0])\n",
    "        \n",
    "    return references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _filter_segments(segmentiterator, minscore):\n",
    "    \"\"\"Returns an iterator of AlignedSegment filtered for reads with low\n",
    "    alignment score, and for any segments identical to the previous segment.\n",
    "    This is necessary as BWA MEM produces dopplegangers.\n",
    "    \"\"\"\n",
    "    \n",
    "    # First get the first segment, so we in the loop can compare to the previous\n",
    "    alignedsegment = next(segmentiterator)\n",
    "    \n",
    "    yield alignedsegment\n",
    "        \n",
    "    lastname = alignedsegment.query_name\n",
    "    lastwasforward = alignedsegment.flag & 64 == 64\n",
    "        \n",
    "    for alignedsegment in segmentiterator:\n",
    "        if alignedsegment.get_tag('AS') < minscore:\n",
    "            continue\n",
    "        \n",
    "        # Depressingly, BWA somtimes outputs the same read multiple times.\n",
    "        # We identify them by having same name and directionality as previous. \n",
    "        thisname = alignedsegment.query_name\n",
    "        thisisforward = alignedsegment.flag & 64 == 64\n",
    "        \n",
    "        if thisisforward is not lastwasforward or thisname != lastname:\n",
    "            yield alignedsegment\n",
    "            \n",
    "            lastname = thisname\n",
    "            lastwasforward = thisisforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_contig_rpkms(path, minscore=50, minlength=2000):\n",
    "    \"\"\"Returns  RPKM (reads per kilobase per million mapped reads)\n",
    "    for all contigs present in BAM header.\n",
    "    \n",
    "    Inputs:\n",
    "        path: Path to BAM file\n",
    "        minscore [50]: Minimum alignment score (AS field) to consider\n",
    "        minlength [2000]: Discard any references shorter than N bases \n",
    "        \n",
    "    Outputs:\n",
    "        path: Same as input path\n",
    "        rpkms A float32-array with RPKM for each contig in BAM header\n",
    "    \"\"\"\n",
    "    \n",
    "    bamfile = _pysam.AlignmentFile(path, \"rb\")\n",
    "\n",
    "    # We can only get secondary alignment reference names, not indices. So we must\n",
    "    # make an idof dict to look up the indices.\n",
    "    idof = {contig: i for i, contig in enumerate(bamfile.references)}\n",
    "    contiglengths = bamfile.lengths\n",
    "    halfreads = _np.zeros(len(contiglengths), dtype=_np.int32)\n",
    "    \n",
    "    nhalfreads = 0\n",
    "    for segment in _filter_segments(bamfile, minscore):\n",
    "        nhalfreads += 1\n",
    "        \n",
    "        # Read w. unmapped mates count twice as they represent a whole read\n",
    "        value = 2 if segment.mate_is_unmapped else 1\n",
    "        \n",
    "        for reference in _get_all_references(segment):\n",
    "            id = idof[reference]\n",
    "            halfreads[id] += value\n",
    "            \n",
    "    bamfile.close()\n",
    "    del idof\n",
    "    \n",
    "    rpkms = _np.zeros(len(contiglengths), dtype=_np.float32)\n",
    "    \n",
    "    millionmappedreads = nhalfreads / 1e6\n",
    "    \n",
    "    for i, (contiglength, nhalfreads) in enumerate(zip(contiglengths, halfreads)):\n",
    "        kilobases = contiglength / 1000\n",
    "        rpkms[i] = nhalfreads / (kilobases * millionmappedreads)\n",
    "        \n",
    "    # Now filter for small contigs\n",
    "    lengthmask = _np.array(contiglengths, dtype=_np.int32) >= minlength\n",
    "    rpkms = rpkms[lengthmask]\n",
    "    \n",
    "    return path, rpkms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bamfiles(paths, minscore=50, minlength=100,\n",
    "                  subprocesses=DEFAULT_SUBPROCESSES, logfile=None):\n",
    "    \"Placeholder docstring - replaced after this func definition\"\n",
    "    \n",
    "    if logfile is not None:\n",
    "        def _callback(result):\n",
    "            path, rpkms = result\n",
    "            print('\\tProcessed ', path, file=logfile)\n",
    "\n",
    "        def _error_callback(result):\n",
    "            path, rpkms = result\n",
    "            print('\\tERROR WHEN PROCESSING ', path, file=logfile)\n",
    "    else:\n",
    "        def _callback(result):\n",
    "            pass\n",
    "\n",
    "        def _error_callback(result):\n",
    "            pass\n",
    "    \n",
    "    # Get references and lengths from first BAM file.\n",
    "    # We need these to print them in the output.\n",
    "    # Might as well do it before spawning all those processes.\n",
    "    firstfile = _pysam.AlignmentFile(paths[0], \"rb\")\n",
    "    ncontigs = sum(1 for length in firstfile.lengths if length >= minlength)\n",
    "    firstfile.close()\n",
    "    del firstfile\n",
    "    \n",
    "    if ncontigs == 0:\n",
    "        raise ValueError('No headers in first bam file after filtering')\n",
    "    \n",
    "    # Spawn independent processed to calculate RPKM for each of the BAM files\n",
    "    processresults = list()\n",
    "\n",
    "    # Queue all the processes\n",
    "    with _multiprocessing.Pool(processes=subprocesses) as pool:\n",
    "        for path in paths:\n",
    "            arguments = (path, minscore, minlength)\n",
    "            processresults.append(pool.apply_async(_get_contig_rpkms, arguments,\n",
    "                                                   callback=_callback, error_callback=_error_callback))\n",
    "        \n",
    "        # For some reason, this is needed.\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "            \n",
    "    columnof = {p:i for i, p in enumerate(paths)}\n",
    "    rpkms = _np.zeros((ncontigs, len(paths)), dtype=_np.float32)\n",
    "    \n",
    "    for processresult in processresults:\n",
    "        if processresult.successful():\n",
    "            path, rpkm = processresult.get()\n",
    "            \n",
    "            if len(rpkm) != ncontigs:\n",
    "                raise ValueError('Expected {} headers in {}, got {}.'.format(\n",
    "                                 ncontigs, path, len(rpkm)))\n",
    "            rpkms[:, columnof[path]] = rpkm\n",
    "            \n",
    "        else:\n",
    "            return processresult\n",
    "            raise _multiprocessing.ProcessError\n",
    "            \n",
    "    return rpkms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_bamfiles.__doc__ = \"\"\"Spawns processes to parse BAM files and get contig rpkms.\n",
    "    \n",
    "Input:\n",
    "    path: Path to BAM file\n",
    "    minscore [50]: Minimum alignment score (AS field) to consider\n",
    "    minlength [100]: Ignore any references shorter than N bases \n",
    "    subprocesses [{}]: Number of subprocesses to spawn\n",
    "    logfile: [None] File to print progress to\n",
    "\n",
    "Output: A (n_contigs x n_samples) Numpy array with RPKM\n",
    "\"\"\".format(DEFAULT_SUBPROCESSES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
