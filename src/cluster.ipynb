{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "__doc__ = \"\"\"Iterative medoid clustering.\n",
    "\n",
    "Usage:\n",
    ">>> cluster_iterator = cluster(rpkms, tnfs, labels=contignames)\n",
    ">>> clusters = dict(cluster_iterator)\n",
    "\n",
    "Implements two core functions: cluster and tandemcluster, along with the helper\n",
    "functions writeclusters and readclusters.\n",
    "For all functions in this module, a collection of clusters are represented as\n",
    "a {clustername, set(elements)} dict.\n",
    "\n",
    "cluster algorithm:\n",
    "(1): Pick random seed observation S\n",
    "(2): Define inner_obs(S) = all observations with Pearson distance from S < INNER\n",
    "(3): Sample MOVES observations I from inner_obs\n",
    "(4): If any mean(inner_obs(i)) < mean(inner_obs(S)) for i in I: Let S be i, go to (2)\n",
    "     Else: Outer_obs(S) = all observations with Pearson distance from S < OUTER\n",
    "(5): Output outer_obs(S) as cluster, remove inner_obs(S) from observations\n",
    "(6): If no more observations or MAX_CLUSTERS have been reached: Stop\n",
    "     Else: Go to (1)\n",
    "\n",
    "tandemcluster agorithm:\n",
    "(1): Pick random observation S\n",
    "(2): Define inner_obs(S) = 10,000 closest obs to S\n",
    "(3): Define outer_obs(S) = 20,000 closest obs to S\n",
    "(4): Remove inner_obs(S) from dataset\n",
    "(5): Cluster outer_obs(S) with algorithm above, add result to clusters.\n",
    "(6): If more than 20,000 observations remain: Go to (1)\n",
    "(7): Assign each obs to the largest cluster it features in\n",
    "\"\"\"\n",
    "\n",
    "import sys as _sys\n",
    "import os as _os\n",
    "import numpy as _np\n",
    "from collections import defaultdict as _defaultdict\n",
    "import vamb.vambtools as _vambtools\n",
    "import vamb.threshold as _threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pearson_distances(matrix, index):\n",
    "    \"\"\"Calculates the Pearson distances from row `index` to all rows\n",
    "    in the matrix, including itself. Returns numpy array of distances\"\"\"\n",
    "    \n",
    "    # Distance D = (P - 1) / -2, where P is Pearson correlation coefficient.\n",
    "    # For two vectors x and y with numbers xi and yi,\n",
    "    # P = sum((xi-x_mean)*(yi-y_mean)) / (std(y) * std(x) * len(x)).\n",
    "    # If we normalize matrix so x_mean = y_mean = 0 and std(x) = std(y) = 1,\n",
    "    # this reduces to sum(xi*yi) / len(x) = x @ y.T / len(x) =>\n",
    "    # D = ((x @ y.T) / len(x)) - 1) / -2 =>\n",
    "    # D = (x @ y.T - len(x)) * (-1 / 2len(x))\n",
    "    \n",
    "    # Matrix should have already been zscore normalized by axis 1 (subtract mean, div by std)\n",
    "    # Also make sure that no rows are [0, 0, 0 ... ]\n",
    "    vectorlength = matrix.shape[1]\n",
    "    result = _np.dot(matrix, matrix[index].T)\n",
    "    result -= vectorlength\n",
    "    result *= -1 / (2 * vectorlength)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _getinner(matrix, point, inner_threshold):\n",
    "    \"\"\"Gets the distance vector, array of inner points and average distance\n",
    "    to inner points from a starting point\"\"\"\n",
    "    \n",
    "    distances = _pearson_distances(matrix, point)\n",
    "    inner_points = _np.where(distances < inner_threshold)[0]\n",
    "    \n",
    "    # This happens if std(matrix[points]) == 0, then all pearson distances\n",
    "    # become 0.5, even the distance to itself.\n",
    "    if len(inner_points) == 0:\n",
    "        inner_points = _np.array([point])\n",
    "        average_distance = 0   \n",
    "    \n",
    "    elif len(inner_points) == 1:\n",
    "        average_distance = 0\n",
    "    \n",
    "    else:\n",
    "        average_distance = _np.sum(distances[inner_points]) / (len(inner_points) - 1)\n",
    "\n",
    "    return distances, inner_points, average_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sample_clusters(matrix, point, max_attempts, inner_threshold, outer_threshold, randomstate):\n",
    "    \"\"\"Keeps sampling new points within the inner points until it has sampled\n",
    "    max_attempts without getting a new set of inner points with lower average\n",
    "    distance\"\"\"\n",
    "    \n",
    "    futile_attempts = 0\n",
    "    \n",
    "    # Keep track of tried points to avoid sampling the same more than once\n",
    "    tried = {point}\n",
    "    \n",
    "    distances, inner_points, average_distance = _getinner(matrix, point, inner_threshold)\n",
    "    \n",
    "    while len(inner_points) - len(tried) > 0 and futile_attempts < max_attempts:\n",
    "        sample = randomstate.choice(inner_points)\n",
    "        while sample in tried: # Not sure there is a faster way to prevent resampling\n",
    "            sample = randomstate.choice(inner_points)\n",
    "            \n",
    "        tried.add(sample)\n",
    "        \n",
    "        inner = _getinner(matrix, sample, inner_threshold)\n",
    "        sample_dist, sample_inner, sample_average =  inner\n",
    "        \n",
    "        if sample_average < average_distance:\n",
    "            point = sample\n",
    "            inner_points = sample_inner\n",
    "            average_distance = sample_average\n",
    "            distances = sample_dist\n",
    "            futile_attempts = 0\n",
    "            tried = {point}\n",
    "            \n",
    "        else:\n",
    "            futile_attempts += 1\n",
    "            \n",
    "    if inner_threshold == outer_threshold:\n",
    "        outer_points = inner_points\n",
    "    else:\n",
    "        outer_points = _np.where(distances < outer_threshold)[0]\n",
    "    \n",
    "    return point, inner_points, outer_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cluster(matrix, labels, inner_threshold, outer_threshold, max_steps):\n",
    "    \"\"\"Yields (medoid, points) pairs from a (obs x features) matrix\"\"\"\n",
    "\n",
    "    randomstate = _np.random.RandomState(324645)\n",
    "\n",
    "    # The seed keeps track of which point we initialize clusters from.\n",
    "    # It's necessary since we don't remove points after every cluster.\n",
    "    # We don't do that since removing points is expensive. If there's only\n",
    "    # one point in the outer_points, by definition, the point to be removed\n",
    "    # will never be present in any other cluster anyway.\n",
    "    seed = 0\n",
    "    keepmask = _np.ones(len(matrix), dtype=_np.bool)\n",
    "    \n",
    "    while len(matrix) > 0:           \n",
    "        # Find medoid using iterative sampling function above\n",
    "        sampling = _sample_clusters(matrix, seed, max_steps, inner_threshold, outer_threshold, randomstate)\n",
    "        medoid, inner_points, outer_points = sampling\n",
    "        seed += 1\n",
    "        \n",
    "        # Write data to output\n",
    "        yield labels[medoid], set(labels[outer_points])\n",
    "        \n",
    "        for point in inner_points:\n",
    "            keepmask[point] = False\n",
    "\n",
    "        # Only remove points if we have more than 1 point in cluster\n",
    "        if len(outer_points) > 1 or seed == len(matrix):\n",
    "            matrix = matrix[keepmask]\n",
    "            labels = labels[keepmask]\n",
    "            \n",
    "            # This is quicker than changing existing mask to True\n",
    "            keepmask = _np.ones(len(matrix), dtype=_np.bool)\n",
    "            \n",
    "            seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _precluster(matrix, labels, randomstate, nremove=10000, nextract=20000):\n",
    "    \"\"\"Does rough preclustering, splits matrix and labels into multiple,\n",
    "    overlapping matrixes/labels. Uses a version of Canopy clustering:\n",
    "    \n",
    "    1) Pick random seed observation, calculate distances P to all other obs\n",
    "    2) Inner threshold is `nremove`th smallest dist, other is `nextract`th\n",
    "    3) Create new matrix, labels pair for all obs within outer threshold\n",
    "    4) Remove all obs within inner dist from set\n",
    "    5) Continue from 1) until max `nextract` observations are left\n",
    "    \"\"\"\n",
    "    \n",
    "    if nextract < nremove:\n",
    "        raise ValueError('nextract must exceed or be equal to nremove')\n",
    "    \n",
    "    while len(matrix) > nextract:\n",
    "        seed = randomstate.randint(len(matrix))\n",
    "        distances = _pearson_distances(matrix, seed)\n",
    "        \n",
    "        sorted_distances = _np.sort(distances)\n",
    "        innerdistance = sorted_distances[nremove]\n",
    "        outerdistance = sorted_distances[nextract]\n",
    "        del sorted_distances\n",
    "        \n",
    "        innerindices = _np.where(distances <= innerdistance)[0]\n",
    "        outerindices = _np.where(distances <= outerdistance)[0]\n",
    "        \n",
    "        yield matrix[outerindices], labels[outerindices]\n",
    "        \n",
    "        matrix = _np.delete(matrix, innerindices, 0)\n",
    "        labels = _np.delete(labels, innerindices, 0)\n",
    "    \n",
    "    yield matrix, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _collapse(clustername, cluster, contigsof, clusterof):\n",
    "    \"\"\"When a new cluster is created among other clusters which might share\n",
    "    its points, assigns the points to the largest cluster.\n",
    "    \n",
    "    Inputs:\n",
    "        clustername: some hashable identifier of cluster\n",
    "        cluster: Set of points\n",
    "        contigsof: {identifier: set(points) dict for all clusters}\n",
    "        clusterof: {point: identifier} for all points\n",
    "        \n",
    "    Output: None\n",
    "    \"\"\"\n",
    "\n",
    "    # Get list of contigs sorted by length of set they're in\n",
    "    # We sort to minimize contig reassignment\n",
    "    membership = list()\n",
    "    for contig in cluster:\n",
    "        if contig in clusterof:\n",
    "            length = len(contigsof[clusterof[contig]])\n",
    "\n",
    "        else:\n",
    "            length = 0\n",
    "\n",
    "        membership.append((contig, length))\n",
    "\n",
    "    membership.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # From contig in largest set to that in smallest:\n",
    "    for contig, length in membership:\n",
    "        # If it's already in a larger set, remove it from this proposed set\n",
    "        if length > len(cluster):\n",
    "            cluster.remove(contig)\n",
    "\n",
    "        # Else, since they're sorted, we're done here\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # All remaining contigs now truly belong to this proposed set\n",
    "    for contig in cluster:\n",
    "        # Remove it from other clusters if they are in another cluster\n",
    "        # and delete the cluster all its contigs are now gone\n",
    "        if contig in clusterof:\n",
    "            othername = clusterof[contig]\n",
    "            othercluster = contigsof[othername]\n",
    "            othercluster.remove(contig)\n",
    "\n",
    "            if len(othercluster) == 0:\n",
    "                contigsof.pop(othername)\n",
    "\n",
    "        # Finally, add this new proposed cluster to set of clusters\n",
    "        clusterof[contig] = clustername\n",
    "\n",
    "    if len(cluster) > 0:\n",
    "        contigsof[clustername] = cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_inputs(max_steps, inner, outer):\n",
    "    \"\"\"Checks whether max_steps, inner and outer are okay.\n",
    "    Can be run before loading matrix into memory.\"\"\"\n",
    "    \n",
    "    if max_steps < 1:\n",
    "        raise ValueError('maxsteps must be a positive integer')\n",
    "    \n",
    "    if inner is None:\n",
    "        if outer is not None:\n",
    "            raise ValueError('If inner is None, outer must be None')\n",
    "        \n",
    "    elif outer is None:\n",
    "        outer = inner\n",
    "        \n",
    "    elif outer < inner:\n",
    "        raise ValueError('outer must exceed or be equal to inner')\n",
    "        \n",
    "    return inner, outer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_params(matrix, inner, outer, labels, nsamples, maxsize, logfile):\n",
    "    \"\"\"Checks matrix, labels, nsamples, maxsize and estimates inner if necessary.\"\"\"\n",
    "    \n",
    "    if logfile is None:\n",
    "        warningfile = _sys.stderr\n",
    "    else:\n",
    "        warningfile = logfile\n",
    "    \n",
    "    if len(matrix) < 1:\n",
    "        raise ValueError('Matrix must have at least 1 observation.')\n",
    "    \n",
    "    if labels is None:\n",
    "        labels = _np.arange(len(matrix)) + 1\n",
    "        \n",
    "    elif type(labels) != _np.ndarray or len(labels) != len(matrix):\n",
    "        raise ValueError('labels must be a 1D Numpy array with same length as matrix')\n",
    "        \n",
    "    if len(set(labels)) != len(matrix):\n",
    "        raise ValueError('Labels must be unique')\n",
    "    \n",
    "    if inner is None:\n",
    "        if len(matrix) < 1000 and inner is None:\n",
    "            raise ValueError('Cannot estimate from less than 1000 contigs')\n",
    "\n",
    "        if len(matrix) < nsamples:\n",
    "            raise ValueError('Specified more samples than available contigs')\n",
    "\n",
    "        if maxsize < 1:\n",
    "            raise ValueError('maxsize must be positive number')\n",
    "        \n",
    "        try:\n",
    "            if logfile is not None:\n",
    "                print('\\tEstimating threshold with {} samples'.format(nsamples), file=logfile)\n",
    "            \n",
    "            _gt = _threshold.getthreshold(matrix, _pearson_distances, nsamples, maxsize)\n",
    "            inner, support, separation = _gt\n",
    "            outer = inner\n",
    "            \n",
    "            if logfile is not None:\n",
    "                print('\\tClustering threshold:', inner, file=logfile)\n",
    "                print('\\tThreshold support:', support, file=logfile)\n",
    "                print('\\tThreshold separation:', separation, file=logfile)\n",
    "            \n",
    "            if separation < 0.25:\n",
    "                sep = round(separation * 100, 1)\n",
    "                wn = '\\tWarning: Only {}% of contigs has well-separated threshold'\n",
    "                print(wn.format(sep), file=warningfile)\n",
    "\n",
    "            if support < 0.50:\n",
    "                sup = round(support * 100, 1)\n",
    "                wn = '\\tWarning: Only {}% of contigs has *any* observable threshold'\n",
    "                print(wn.format(sup), file=warningfile)\n",
    "                \n",
    "        except _threshold.TooLittleData as error:\n",
    "            wn = '\\tWarning: Too little data: {}. Setting threshold to 0.08'\n",
    "            print(wn.format(error.args[0]), file=warningfile)\n",
    "            inner = outer = 0.08\n",
    "        \n",
    "    return labels, inner, outer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(matrix, labels=None, inner=None, outer=None, max_steps=25,\n",
    "            normalized=False, nsamples=2000, maxsize=2500, logfile=None):\n",
    "    \"\"\"Iterative medoid cluster generator. Yields (medoid), set(labels) pairs.\n",
    "    \n",
    "    Inputs:\n",
    "        matrix: A (obs x features) Numpy matrix of values\n",
    "        labels: None or Numpy array with labels for matrix rows [None = ints]\n",
    "        inner: Optimal medoid search within this distance from medoid [None = auto]\n",
    "        outer: Radius of clusters extracted from medoid. [None = inner]\n",
    "        max_steps: Stop searching for optimal medoid after N futile attempts [25]\n",
    "        normalized: Matrix is already zscore-normalized [False]\n",
    "        nsamples: Estimate threshold from N samples [1000]\n",
    "        maxsize: Discard sample if more than N contigs are within threshold [2500]\n",
    "        logfile: Print threshold estimates and certainty to file [None]\n",
    "    \n",
    "    Output: Generator of (medoid, set(labels_in_cluster)) tuples.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not normalized is True:\n",
    "        matrix = _vambtools.zscore(matrix, axis=1)\n",
    "        \n",
    "    inner, outer = _check_inputs(max_steps, inner, outer)\n",
    "    labels, inner, outer = _check_params(matrix, inner, outer, labels, nsamples, maxsize, logfile)\n",
    "    \n",
    "    return _cluster(matrix, labels, inner, outer, max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tandemcluster(matrix, labels=None, inner=None, outer=None, max_steps=25,\n",
    "            normalized=False, nsamples=2000, maxsize=2500, logfile=None):\n",
    "    \"\"\"Splits the datasets, then clusters each partition before merging\n",
    "    the resulting clusters. This is faster, especially on larger datasets, but\n",
    "    less accurate than normal clustering.\n",
    "    \n",
    "    Inputs:\n",
    "        matrix: A (obs x features) Numpy matrix of values\n",
    "        labels: None or Numpy array with labels for matrix rows [None = ints]\n",
    "        inner: Optimal medoid search within this distance from medoid [None = auto]\n",
    "        outer: Radius of clusters extracted from medoid. [None = inner]\n",
    "        max_steps: Stop searching for optimal medoid after N futile attempts [25]\n",
    "        normalized: Matrix is already zscore-normalized [False]\n",
    "        nsamples: Estimate threshold from N samples [1000]\n",
    "        maxsize: Discard sample if more than N contigs are within threshold [2500]\n",
    "        logfile: Print threshold estimates and certainty to file [None]\n",
    "    \n",
    "    Output: {(partition, medoid): set(labels_in_cluster) dictionary}\n",
    "    \"\"\"\n",
    "    \n",
    "    if not normalized:\n",
    "        matrix = _vambtools.zscore(matrix, axis=1)\n",
    "        \n",
    "    inner, outer = _check_inputs(max_steps, inner, outer)\n",
    "    labels, inner, outer = _check_params(matrix, inner, outer, labels, nsamples, maxsize, logfile)\n",
    "    \n",
    "    randomstate = _np.random.RandomState(324645) \n",
    "    \n",
    "    contigsof = dict()\n",
    "    clusterof = dict()\n",
    "    \n",
    "    partitions = _precluster(matrix, labels, randomstate, nremove=10000, nextract=30000)\n",
    "    \n",
    "    for partition, (submatrix, sublabels) in enumerate(partitions):\n",
    "        for medoid, cluster in _cluster(submatrix, sublabels, inner, outer, max_steps):\n",
    "            _collapse((partition, medoid), cluster, contigsof, clusterof)\n",
    "        \n",
    "        matrix = None\n",
    "            \n",
    "    return contigsof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_clusters(filehandle, clusters, max_clusters=None, min_size=1,\n",
    "                 header=None):\n",
    "    \"\"\"Writes clusters to an open filehandle.\n",
    "    \n",
    "    Inputs:\n",
    "        filehandle: An open filehandle that can be written to\n",
    "        clusters: An iterator generated by function `clusters` or dict\n",
    "        max_clusters: Stop printing after this many clusters [None]\n",
    "        min_size: Don't output clusters smaller than N contigs\n",
    "        header: Commented one-line header to add\n",
    "        \n",
    "    Outputs:\n",
    "        clusternumber: Number of clusters written\n",
    "        ncontigs: Number of contigs written\n",
    "    \"\"\"\n",
    "    \n",
    "    if not hasattr(filehandle, 'writable') or not filehandle.writable():\n",
    "        raise ValueError('Filehandle must be a writable file')\n",
    "        \n",
    "    if iter(clusters) is not clusters:\n",
    "        clusters = clusters.items()\n",
    "    \n",
    "    if header is not None and len(header) > 0:\n",
    "        if '\\n' in header:\n",
    "            raise ValueError('Header cannot contain newline')\n",
    "        \n",
    "        if header[0] != '#':\n",
    "            header = '# ' + header\n",
    "        \n",
    "        print(header, file=filehandle)\n",
    "    \n",
    "    clusternumber = 0\n",
    "    ncontigs = 0\n",
    "    \n",
    "    for clustername, contigs in clusters:\n",
    "        if clusternumber == max_clusters:\n",
    "            break\n",
    "        \n",
    "        if len(contigs) < min_size:\n",
    "            continue\n",
    "        \n",
    "        clustername = 'cluster_' + str(clusternumber + 1)\n",
    "        \n",
    "        for contig in contigs:\n",
    "            print(clustername, contig, sep='\\t', file=filehandle)\n",
    "            \n",
    "        clusternumber += 1\n",
    "        ncontigs += len(contigs)\n",
    "        \n",
    "    return clusternumber, ncontigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_clusters(filehandle, min_size=1):\n",
    "    \"\"\"Read clusters from a file as created by function `writeclusters`.\n",
    "    \n",
    "    Inputs:\n",
    "        filehandle: An open filehandle that can be read from\n",
    "        min_size: Minimum number of contigs in cluster to be kept\n",
    "    \n",
    "    Output: A {clustername: set(contigs)} dict\"\"\"\n",
    "    \n",
    "    contigsof = _defaultdict(set)\n",
    "    \n",
    "    for line in filehandle:\n",
    "        stripped = line.strip()\n",
    "        \n",
    "        if stripped[0] == '#':\n",
    "            continue\n",
    "            \n",
    "        clustername, contigname = stripped.split('\\t')\n",
    "        \n",
    "        contigsof[clustername].add(contigname)\n",
    "        \n",
    "    contigsof = {cl: co for cl, co in contigsof.items() if len(co) >= min_size}\n",
    "        \n",
    "    return contigsof"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
