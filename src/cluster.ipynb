{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "__doc__ = \"\"\"Iterative medoid clustering of Numpy arrays.\n",
    "\n",
    "Implements two core functions: cluster and tandemcluster, along with the helper\n",
    "functions writeclusters and readclusters.\n",
    "For all functions in this module, a collection of clusters are represented as\n",
    "a {clustername, set(elements)} dict.\n",
    "\n",
    "cluster algorithm:\n",
    "(1): Pick random seed observation S\n",
    "(2): Define inner_obs(S) = all observations with Pearson distance from S < INNER\n",
    "(3): Sample MOVES observations I from inner_obs\n",
    "(4): If any inner_obs(i) > inner_obs(S) for i in I: Let S be i, go to (2)\n",
    "     Else: Outer_obs(S) = all observations with Pearson distance from S < OUTER\n",
    "(5): Output outer_obs(S) as cluster, remove inner_obs(S) from observations\n",
    "(6): If no more observations or MAX_CLUSTERS have been reached: Stop\n",
    "     Else: Go to (1)\n",
    "\n",
    "tandemcluster agorithm:\n",
    "(1): Pick random observation S\n",
    "(2): Define inner_obs(S) = 10,000 closest obs to S\n",
    "(3): Define outer_obs(S) = 20,000 closest obs to S\n",
    "(4): Remove inner_obs(S) from dataset\n",
    "(5): Cluster outer_obs(S) with algorithm above, add result to clusters.\n",
    "(6): If more than 20,000 observations remain: Go to (1)\n",
    "(7): Assign each obs to the largest cluster it features in\n",
    "\"\"\"\n",
    "\n",
    "cmd_doc = \"\"\"Iterative medoid clustering.\n",
    "\n",
    "    Input: An observations x features matrix in text format.\n",
    "    Output: Tab-sep lines with clustername, observation(1-indexed).\n",
    "\"\"\"\n",
    "\n",
    "import sys as _sys\n",
    "import os as _os\n",
    "import numpy as _np\n",
    "from collections import defaultdict as _defaultdict\n",
    "\n",
    "if __package__ is None or __package__ == '':\n",
    "    import vambtools as _vambtools\n",
    "    \n",
    "else:\n",
    "    import vamb.vambtools as _vambtools\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _checklabels(labels, length):\n",
    "    \"\"\"If labels are None, returns an array of integers 1 to length.\n",
    "    Else checks that labels are already of correct length and unique\"\"\"\n",
    "    \n",
    "    if labels is None:\n",
    "        labels = _np.arange(length) + 1\n",
    "        \n",
    "    elif type(labels) != _np.ndarray or len(labels) != length:\n",
    "        raise ValueError('labels must be a 1D Numpy array with same length as matrix')\n",
    "        \n",
    "    if len(set(labels)) != length:\n",
    "        raise ValueError('Labels must be unique')\n",
    "        \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _distances_to_vector(matrix, index):\n",
    "    \"\"\"Calculates the Pearson distances from row `index` to all rows\n",
    "    in the matrix, including itself. Returns numpy array of distances\"\"\"\n",
    "    \n",
    "    # Distance D = (P - 1) / -2, where P is Pearson correlation coefficient.\n",
    "    # For two vectors x and y with numbers xi and yi,\n",
    "    # P = sum((xi-x_mean)*(yi-y_mean)) / (std(y) * std(x) * len(x)).\n",
    "    # If we normalize matrix so x_mean = y_mean = 0 and std(x) = std(y) = 1,\n",
    "    # this reduces to sum(xi*yi) / len(x) = x @ y.T / len(x) =>\n",
    "    # D = ((x @ y.T) / len(x)) - 1) / -2 =>\n",
    "    # D = (x @ y.T - len(x)) * (-1 / 2len(x))\n",
    "    \n",
    "    # Matrix should have already been zscore normalized by axis 1 (subtract mean, div by std)\n",
    "    vectorlength = matrix.shape[1]\n",
    "    result = _np.dot(matrix, matrix[index].T)\n",
    "    result -= vectorlength\n",
    "    result *= -1 / (2 * vectorlength)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _getinner(matrix, point, inner_threshold):\n",
    "    \"\"\"Gets the distance vector, array of inner points and average distance\n",
    "    to inner points from a starting point\"\"\"\n",
    "    \n",
    "    distances = _distances_to_vector(matrix, point)\n",
    "    inner_points = _np.where(distances < inner_threshold)[0]\n",
    "    \n",
    "    if len(inner_points) == 1:\n",
    "        average_distance = 0\n",
    "    else:\n",
    "        average_distance = _np.sum(distances[inner_points]) / (len(inner_points) - 1)\n",
    "\n",
    "    return distances, inner_points, average_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sample_clusters(matrix, point, max_attempts, inner_threshold, outer_threshold, randomstate):\n",
    "    \"\"\"Keeps sampling new points within the inner points until it has sampled\n",
    "    max_attempts without getting a new set of inner points with lower average\n",
    "    distance\"\"\"\n",
    "    \n",
    "    futile_attempts = 0\n",
    "    \n",
    "    # Keep track of tried points to avoid sampling the same more than once\n",
    "    tried = {point}\n",
    "    \n",
    "    distances, inner_points, average_distance = _getinner(matrix, point, inner_threshold)\n",
    "    \n",
    "    while len(inner_points) - len(tried) > 0 and futile_attempts < max_attempts:\n",
    "        sample = randomstate.choice(inner_points)\n",
    "        while sample in tried: # Not sure there is a faster way to prevent resampling\n",
    "            sample = randomstate.choice(inner_points)\n",
    "            \n",
    "        tried.add(sample)\n",
    "        \n",
    "        inner = _getinner(matrix, sample, inner_threshold)\n",
    "        sample_dist, sample_inner, sample_average =  inner\n",
    "        \n",
    "        if sample_average < average_distance:\n",
    "            point = sample\n",
    "            inner_points = sample_inner\n",
    "            average_distance = sample_average\n",
    "            distances = sample_dist\n",
    "            futile_attempts = 0\n",
    "            tried = {point}\n",
    "            \n",
    "        else:\n",
    "            futile_attempts += 1\n",
    "            \n",
    "    outer_points = _np.where(distances < outer_threshold)[0]\n",
    "    \n",
    "    return point, inner_points, outer_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cluster(matrix, labels, inner_threshold, outer_threshold, max_steps):\n",
    "    \"\"\"Yields (medoid, points) pairs from a (obs x features) matrix\"\"\"\n",
    "\n",
    "    randomstate = _np.random.RandomState(324645)\n",
    "    \n",
    "    # This list keeps track of points to remove because they compose the inner circle\n",
    "    # of clusters. We only remove points when we create a cluster with more than one\n",
    "    # point, since one-point-clusters don't interfere with other clusters and the\n",
    "    # point removal operations are expensive.\n",
    "    toremove = list()\n",
    "    \n",
    "    # This index keeps track of which point we initialize clusters from.\n",
    "    # It's necessary since we don't remove points after every cluster.\n",
    "    seed_index = 0\n",
    "    \n",
    "    # We initialize clusters from most extreme to less extreme. This is\n",
    "    # arbitrary and just to have some kind of reproducability.\n",
    "    # Note to Simon: Sorting by means makes no sense with normalized rows.\n",
    "    extremes = _np.max(matrix, axis=1)\n",
    "    argextremes = _np.argsort(extremes)\n",
    "    \n",
    "    while len(matrix) > 0:           \n",
    "        # Most extreme point (without picking same point twice)\n",
    "        seed = argextremes[-seed_index -1]\n",
    "        \n",
    "        # Find medoid using iterative sampling function above\n",
    "        sampling = _sample_clusters(matrix, seed, max_steps, inner_threshold, outer_threshold, randomstate)\n",
    "        medoid, inner_points, outer_points = sampling\n",
    "        \n",
    "        # Write data to output\n",
    "        yield labels[medoid], set(labels[outer_points])\n",
    "            \n",
    "        seed_index += 1\n",
    "        \n",
    "        for point in inner_points:\n",
    "            toremove.append(point)\n",
    "\n",
    "        # Only remove points if we have more than 1 point in cluster\n",
    "        # Note that these operations are really expensive.\n",
    "        if len(inner_points) > 1 or len(argextremes) == seed_index:\n",
    "            matrix = _np.delete(matrix, toremove, 0)\n",
    "            labels = _np.delete(labels, toremove, 0)\n",
    "            extremes = _np.delete(extremes, toremove, 0)\n",
    "            argextremes = _np.argsort(extremes)\n",
    "            seed_index = 0\n",
    "            toremove.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _precluster(matrix, labels, randomstate, nremove=10000, nextract=20000):\n",
    "    \"\"\"Does rough preclustering, splits matrix and labels into multiple,\n",
    "    overlapping matrixes/labels. Uses a version of Canopy clustering:\n",
    "    \n",
    "    1) Pick random seed observation, calculate distances P to all other obs\n",
    "    2) Inner threshold is `nremove`th smallest dist, other is `nextract`th\n",
    "    3) Create new matrix, labels pair for all obs within outer threshold\n",
    "    4) Remove all obs within inner dist from set\n",
    "    5) Continue from 1) until max `nextract` observations are left\n",
    "    \"\"\"\n",
    "    \n",
    "    if nextract < nremove:\n",
    "        raise ValueError('nextract must exceed or be equal to nremove')\n",
    "    \n",
    "    while len(matrix) > nextract:\n",
    "        seed = randomstate.randint(len(matrix))\n",
    "        distances = _distances_to_vector(matrix, seed)\n",
    "        \n",
    "        sorted_distances = _np.sort(distances)\n",
    "        innerdistance = sorted_distances[nremove]\n",
    "        outerdistance = sorted_distances[nextract]\n",
    "        del sorted_distances\n",
    "        \n",
    "        innerindices = _np.where(distances <= innerdistance)[0]\n",
    "        outerindices = _np.where(distances <= outerdistance)[0]\n",
    "        \n",
    "        yield matrix[outerindices], labels[outerindices]\n",
    "        \n",
    "        matrix = _np.delete(matrix, innerindices, 0)\n",
    "        labels = _np.delete(labels, innerindices, 0)\n",
    "    \n",
    "    yield matrix, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _collapse(clustername, cluster, contigsof, clusterof):\n",
    "    \"\"\"When a new cluster is created among other clusters which might share\n",
    "    its points, assigns the points to the largest cluster.\n",
    "    \n",
    "    Inputs:\n",
    "        clustername: some hashable identifier of cluster\n",
    "        cluster: Set of points\n",
    "        contigsof: {identifier: set(points) dict for all clusters}\n",
    "        clusterof: {point: identifier} for all points\n",
    "        \n",
    "    Output: None\n",
    "    \"\"\"\n",
    "\n",
    "    # Get list of contigs sorted by length of set they're in\n",
    "    # We sort to minimize contig reassignment\n",
    "    \n",
    "    membership = list()\n",
    "    for contig in cluster:\n",
    "        if contig in clusterof:\n",
    "            length = len(contigsof[clusterof[contig]])\n",
    "\n",
    "        else:\n",
    "            length = 0\n",
    "\n",
    "        membership.append((contig, length))\n",
    "\n",
    "    membership.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # From contig in largest set to that in smallest:\n",
    "    for contig, length in membership:\n",
    "        # If it's already in a larger set, remove it from this proposed set\n",
    "        if length > len(cluster):\n",
    "            cluster.remove(contig)\n",
    "\n",
    "        # Else, since they're sorted, we're done here\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # All remaining contigs now truly belong to this proposed set\n",
    "    for contig in cluster:\n",
    "        # Remove it from other clusters if they are in another cluster\n",
    "        # and delete the cluster all its contigs are now gone\n",
    "        if contig in clusterof:\n",
    "            othername = clusterof[contig]\n",
    "            othercluster = contigsof[othername]\n",
    "            othercluster.remove(contig)\n",
    "\n",
    "            if len(othercluster) == 0:\n",
    "                contigsof.pop(othername)\n",
    "\n",
    "        # Finally, add this new proposed cluster to set of clusters\n",
    "        clusterof[contig] = clustername\n",
    "\n",
    "    if len(cluster) > 0:\n",
    "        contigsof[clustername] = cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tandemcluster(matrix, labels, inner, outer=None, max_steps=15):\n",
    "    \"\"\"Splits the datasets, then clusters each partition before merging\n",
    "    the resulting clusters. This is faster, especially on larger datasets, but\n",
    "    less accurate than normal clustering.\n",
    "    \n",
    "    Inputs:\n",
    "        matrix: A (obs x features) Numpy matrix of values\n",
    "        labels: Numpy array with labels for matrix rows. None or 1-D array\n",
    "        inner: Optimal medoid search within this distance from medoid\n",
    "        outer: Radius of clusters extracted from medoid. If None, same as inner\n",
    "        max_steps: Stop searching for optimal medoid after N futile attempts\n",
    "    \n",
    "    Output: {(partition, medoid): set(labels_in_cluster) dictionary}\n",
    "    \"\"\"\n",
    "    \n",
    "    if outer is None:\n",
    "        outer = inner\n",
    "        \n",
    "    elif outer < inner:\n",
    "        raise ValueError('outer must exceed or be equal to inner')\n",
    "    \n",
    "    \n",
    "    labels = _checklabels(labels, len(matrix))\n",
    "    matrix = _vambtools.zscore(matrix, axis=1)\n",
    "    \n",
    "    randomstate = _np.random.RandomState(324645) \n",
    "    \n",
    "    contigsof = dict()\n",
    "    clusterof = dict()\n",
    "    \n",
    "    partitions = _precluster(matrix, labels, randomstate, nremove=10000, nextract=20000)\n",
    "    \n",
    "    for partition, (submatrix, sublabels) in enumerate(partitions):\n",
    "        for medoid, cluster in _cluster(submatrix, sublabels, inner, outer, max_steps):\n",
    "            _collapse((partition, medoid), cluster, contigsof, clusterof)\n",
    "        \n",
    "        # Allow the garbage collector to delete the normalized matrix\n",
    "        matrix = None\n",
    "            \n",
    "    return contigsof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(matrix, labels, inner, outer=None, max_steps=15):\n",
    "    \"\"\"Iterative medoid cluster generator. Yields (medoid), set(labels) pairs.\n",
    "    \n",
    "    Inputs:\n",
    "        matrix: A (obs x features) Numpy matrix of values\n",
    "        labels: Numpy array with labels for matrix rows or None.\n",
    "        inner: Optimal medoid search within this distance from medoid\n",
    "        outer: Radius of clusters extracted from medoid. If None, same as inner\n",
    "        max_steps: Stop searching for optimal medoid after N futile attempts\n",
    "    \n",
    "    Output: Generator of (medoid, set(labels_in_cluster)) tuples.\n",
    "    \"\"\"\n",
    "\n",
    "    if outer is None:\n",
    "        outer = inner\n",
    "        \n",
    "    elif outer < inner:\n",
    "        raise ValueError('outer must exceed or be equal to inner')\n",
    "    \n",
    "    labels = _checklabels(labels, len(matrix))\n",
    "    matrix = _vambtools.zscore(matrix, axis=1)\n",
    "    \n",
    "    return _cluster(matrix, labels, inner, outer, max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeclusters(filehandle, clusters, max_clusters=None, min_size=1,\n",
    "                 header=None):\n",
    "    \"\"\"Writes clusters to an open filehandle.\n",
    "    \n",
    "    Inputs:\n",
    "        filehandle: An open filehandle that can be written to\n",
    "        clusters: An iterator generated by function `clusters` or dict\n",
    "        max_clusters: Stop printing after this many clusters [None]\n",
    "        min_size: Don't output clusters smaller than N contigs\n",
    "        header: Commented one-line header to add\n",
    "        \n",
    "    Output: None\n",
    "    \"\"\"\n",
    "    \n",
    "    if not hasattr(filehandle, 'writable') or not filehandle.writable():\n",
    "        raise ValueError('Filehandle must be a writable file')\n",
    "        \n",
    "    if iter(clusters) is not clusters:\n",
    "        clusters = clusters.items()\n",
    "    \n",
    "    if header is not None and len(header) > 0:\n",
    "        if '\\n' in header:\n",
    "            raise ValueError('Header cannot contain newline')\n",
    "        \n",
    "        if header[0] != '#':\n",
    "            header = '# ' + header\n",
    "        \n",
    "        print(header, file=filehandle)\n",
    "    \n",
    "    clusternumber = 0\n",
    "    for clustername, contigs in clusters:\n",
    "        if clusternumber == max_clusters:\n",
    "            break\n",
    "        \n",
    "        if len(contigs) < min_size:\n",
    "            continue\n",
    "        \n",
    "        clustername = 'cluster_' + str(clusternumber + 1)\n",
    "        \n",
    "        for contig in contigs:\n",
    "            print(clustername, contig, sep='\\t', file=filehandle)\n",
    "            \n",
    "        clusternumber += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readclusters(filehandle, min_size=1):\n",
    "    \"\"\"Read clusters from a file as created by function `writeclusters`.\n",
    "    \n",
    "    Inputs:\n",
    "        filehandle: An open filehandle that can be read from\n",
    "        min_size: Minimum number of contigs in cluster to be kept\n",
    "    \n",
    "    Output: A {clustername: set(contigs)} dict\"\"\"\n",
    "    \n",
    "    contigsof = _defaultdict(set)\n",
    "    \n",
    "    for line in filehandle:\n",
    "        stripped = line.strip()\n",
    "        \n",
    "        if stripped[0] == '#':\n",
    "            continue\n",
    "            \n",
    "        clustername, contigname = stripped.split('\\t')\n",
    "        \n",
    "        contigsof[clustername].add(contigname)\n",
    "        \n",
    "    contigsof = {cl: co for cl, co in contigsof.items() if len(co) >= min_size}\n",
    "        \n",
    "    return contigsof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'argparse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-d6fc68721b30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0musage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"python cluster.py [OPTIONS ...] INPUT OUTPUT\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     parser = argparse.ArgumentParser(\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmd_doc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mformatter_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRawDescriptionHelpFormatter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'argparse' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    usage = \"python cluster.py [OPTIONS ...] INPUT OUTPUT\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=cmd_doc,\n",
    "        formatter_class=argparse.RawDescriptionHelpFormatter,\n",
    "        usage=usage)\n",
    "   \n",
    "    # create the parser\n",
    "    parser.add_argument('input', help='input dataset')\n",
    "    parser.add_argument('output', help='output clusters')\n",
    "    parser.add_argument('inner', help='inner distance threshold', type=float)\n",
    "    parser.add_argument('outer', help='outer distance threshold', type=float)\n",
    "    \n",
    "    \n",
    "    parser.add_argument('-c', dest='max_clusters',\n",
    "                        help='stop after creating N clusters [None, i.e. infinite]', type=int)\n",
    "    parser.add_argument('-m', dest='max_steps',\n",
    "                        help='stop searchin for optimal medoid after N attempts [15]',\n",
    "                        default=15, type=int)\n",
    "    parser.add_argument('-s', dest='min_size',\n",
    "                        help='minimum cluster size to output [1]',default=1, type=int)\n",
    "    parser.add_argument('--precluster', help='precluster first [False]', action='store_true')\n",
    "    \n",
    "    # Print help if no arguments are given\n",
    "    if len(_sys.argv) == 1:\n",
    "        parser.print_help()\n",
    "        _sys.exit()\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    if args.outer is None:\n",
    "        args.outer = args.inner\n",
    "    \n",
    "    elif args.outer < args.inner:\n",
    "        raise ValueError('outer threshold must exceed or be equal to inner threshold')\n",
    "        \n",
    "    if args.inner <= 0:\n",
    "        raise ValueError('inner threshold must be larger than 1.')\n",
    "        \n",
    "    if args.min_size < 1:\n",
    "        raise ValueError('Minimum size must be 1 or above.')\n",
    "        \n",
    "    if args.max_steps < 1:\n",
    "        raise ValueError('Max steps must be 1 or above.')\n",
    "        \n",
    "    if args.max_clusters is not None and args.precluster:\n",
    "        raise ValueError('Conflicting arguments: precluster and max_clusters')\n",
    "    \n",
    "    if not _os.path.isfile(args.input):\n",
    "        raise FileNotFoundError(args.input)\n",
    "    \n",
    "    if _os.path.isfile(args.output):\n",
    "        raise FileExistsError(args.output)\n",
    "        \n",
    "    directory = _os.path.dirname(args.output)\n",
    "    if directory and not _os.path.isdir(directory):\n",
    "        raise NotADirectoryError(directory)\n",
    "   \n",
    "    matrix = _np.loadtxt(args.input, delimiter='\\t', dtype=_np.float32)\n",
    "    \n",
    "    if args.precluster:\n",
    "        contigsof = tandemcluster(matrix, None, args.inner, outer=None,\n",
    "                               max_steps=args.max_steps)\n",
    "        \n",
    "    else:\n",
    "        contigsof = cluster(matrix, None, args.inner, outer=None,\n",
    "                                max_steps=args.max_steps)\n",
    "    \n",
    "    # Allow the garbage collector to clean the original matrix\n",
    "    del matrix\n",
    "    \n",
    "    header = \"clustername\\tcontigindex(1-indexed)\"\n",
    "    \n",
    "    with open(args.output, 'w') as filehandle:\n",
    "        writeclusters(filehandle, contigsof, args.max_clusters, args.min_size,\n",
    "                     header=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
