{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "\n",
    "First step is to get Vamb on your computer.\n",
    "\n",
    "__If you have `git` installed__:\n",
    "\n",
    "    [jakni@nissen:Downloads]$ # Clone Vamb from GitHub into Downloads/vamb\n",
    "\n",
    "    [jakni@nissen:Downloads]$ git clone https://github.com/jakobnissen/vamb vamb\n",
    "    \n",
    "__If you don't__\n",
    "\n",
    "    [jakni@nissen:Downloads]$ # You then presumably have access to a Vamb directory\n",
    "\n",
    "    [jakni@nissen:Downloads]$ cp -r /path/to/vamb/directory vamb\n",
    "    \n",
    "---\n",
    "Now you have Vamb on your computer. Time to get it imported\n",
    "    \n",
    "    [jakni@nissen:~]$ python\n",
    "    >>> import vamb\n",
    "    Traceback (most recent call last):\n",
    "      File \"<stdin>\", line 1, in <module>\n",
    "    ModuleNotFoundError: No module named 'vamb'\n",
    "    >>> # We're not in the directory containing the vamb directory.\n",
    "    >>> # That means the directory containing the vamb dir is not in out sys.path.\n",
    "    >>> # Either move the vamb directory to one of you sys.path dirs \n",
    "    >>> # or add the vamb directory to sys.path. We'll do the latter.\n",
    "    >>> import sys\n",
    "    >>> sys.path.append('/home/jakni/Downloads')\n",
    "    >>> import vamb\n",
    "    >>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting help\n",
    "\n",
    "You'll almost certianly need help when using Vamb (we wish it was so easy you didn't, but making user friendly software is hard!).\n",
    "\n",
    "Luckily, there's the built-in `help` function in Python.\n",
    "\n",
    "---\n",
    "\n",
    "`>>> help(vamb)`\n",
    "    \n",
    "    Help on package vamb:\n",
    "\n",
    "    NAME\n",
    "        vamb - Variational Autoencoder for Metagenomic Binning\n",
    "\n",
    "    DESCRIPTION\n",
    "        Vamb does what it says on the tin - bins metagenomes using a variational autoencoder.\n",
    "        \n",
    "    [ lines elided ]\n",
    "    \n",
    "        General workflow:\n",
    "        1) Filter contigs by size using vamb.filtercontigs\n",
    "        2) Calculate TNF using vamb.parsecontigs\n",
    "        3) Create RPKM table using vamb.parsebam\n",
    "        4) Train autoencoder using vamb.encode\n",
    "        5) Cluster latent representation using vamb.cluster\n",
    "    \n",
    "    [ lines elided ]\n",
    "    \n",
    "---\n",
    "    \n",
    "The `PACKAGE CONTENTS` under `help(vamb)` is just a list of all importable files in the `vamb` directory - some of these really shouldn't be imported, so ignore that.\n",
    "\n",
    "---\n",
    "You can also get help for the modules:\n",
    "\n",
    "`>>> help(vamb.cluster)`\n",
    "\n",
    "    Help on module vamb.cluster in vamb:\n",
    "\n",
    "    NAME\n",
    "        vamb.cluster - Iterative medoid clustering of Numpy arrays.\n",
    "\n",
    "    DESCRIPTION\n",
    "        Implements two core functions: cluster and tandemcluster, along with the helper\n",
    "        functions writeclusters and readclusters.\n",
    "        For all functions in this module, a collection of clusters are represented as\n",
    "        a {clustername, set(elements)} dict.\n",
    "\n",
    "        Clustering algorithm:\n",
    "    \n",
    "    [ lines elided ]\n",
    "        \n",
    "---\n",
    "And for functions:\n",
    "\n",
    "`>>> help(vamb.cluster.tandemcluster)`\n",
    "\n",
    "    Help on function tandemcluster in module vamb.cluster:\n",
    "\n",
    "    tandemcluster(matrix, labels, inner, outer=None, max_steps=15, spearman=False)\n",
    "        Splits the datasets, then clusters each partition before merging\n",
    "        the resulting clusters. This is faster, especially on larger datasets, but\n",
    "        less accurate than normal clustering.\n",
    "\n",
    "        Inputs:\n",
    "            matrix: A (obs x features) Numpy matrix of values\n",
    "            labels: Numpy array with labels for matrix rows. None or 1-D array\n",
    "            inner: Optimal medoid search within this distance from medoid\n",
    "            outer: Radius of clusters extracted from medoid. If None, same as inner\n",
    "            max_steps: Stop searching for optimal medoid after N futile attempts\n",
    "            spearman: Use Spearman, not Pearson correlation\n",
    "\n",
    "        Output: {medoid: set(labels_in_cluster) dictionary}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple workflow example from within Python\n",
    "\n",
    "You begin with some some FASTQ files from, say, 6 samples. First you do the following steps:\n",
    "\n",
    "1) Preprocess the reads and check their quality\n",
    "\n",
    "2) Assemble each sample individually OR co-assemble and get the contigs out\n",
    "\n",
    "3) Concatenate the FASTA files together while making sure all contig headers stay unique\n",
    "\n",
    "Now, like other metagenomic binners of contigs, Vamb relies on two properties of the contigs: The abundance of the contigs in each sample and the kmer-composition of the contigs. The observed values for both of these measures become uncertain when the contig is too small, so you should filter the small contigs away:\n",
    "\n",
    "4) Remove all small contigs from the FASTA file (say, less than 2000 bp in length)\n",
    "\n",
    "To estimate the abundance of the contigs, you need to map the reads from each sample to the FASTA file. When using BWA, don't filter for unproperly paired reads or minimum alignment score.\n",
    "\n",
    "5) Map the reads to the FASTA file to obtain 6 .bam files\n",
    "\n",
    "Now, maybe you can't filter the FASTA file. Maybe you have already spent tonnes of time getting those BAM files and you're not going to remap if your life depended on it. Maybe your FASTA file contains genes, not contigs, and so removing all entries less than e.g. 2000 bps is a bit too much to ask.\n",
    "\n",
    "That's fair enough. You can ignore any contigs lower than a certain length with the `minlength` keyword arguments of the `read_contigs` and the `read_bamfiles` functions (shown later). This is not ideal, since the smaller, ignored contigs will still have recruited some reads during mapping which are then not mapped to the larger contigs.\n",
    "___\n",
    "\n",
    "This gives us the following results, here put in `/home/jakni/Downloads/example`:\n",
    "\n",
    "* `contigs.fna` - The filtered FASTA contigs which were mapped against, and\n",
    "* `bamfiles/*.bam` - The 6 .bam files from mapping the reads to the contigs above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jakni/Documents/scripts/')\n",
    "import vamb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, parse the FASTA file\n",
    "\n",
    "How was it I did that again? Oh, right, the `read_contigs` function in the `parsecontigs` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function read_contigs in module vamb.parsecontigs:\n",
      "\n",
      "read_contigs(byte_iterator, minlength=100)\n",
      "    Parses a FASTA file open in binary reading mode.\n",
      "    \n",
      "    Input:\n",
      "        byte_iterator: Iterator of binary lines of a FASTA file\n",
      "        minlength[100]: Ignore any references shorter than N bases \n",
      "    \n",
      "    Outputs:\n",
      "        tnfs: A (n_FASTA_entries x 136) matrix of tetranucleotide freq.\n",
      "        contignames: A list of contig headers\n",
      "        lengths: A list of contig lengths\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(vamb.parsecontigs.read_contigs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file in binary mode - you can use the vamb.vambtools.Reader to read\n",
    "# from normal or gzipped file seamlessly. Here I just use the open function\n",
    "with open('/home/jakni/Downloads/example/contigs.fna', 'rb') as filehandle:\n",
    "    tnfs, contignames, lengths = vamb.parsecontigs.read_contigs(filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of tnfs: <class 'numpy.ndarray'> of dtype float32\n",
      "Shape of tnfs: (39551, 136)\n",
      "\n",
      "Type of contignames: <class 'list'>\n",
      "Length of contignames: 39551\n",
      "\n",
      "First 10 elements of contignames:\n",
      "s30_NODE_1_length_245508_cov_18.4904\n",
      "s30_NODE_2_length_222690_cov_39.7685\n",
      "s30_NODE_3_length_222459_cov_20.3665\n",
      "s30_NODE_4_length_173155_cov_20.1181\n",
      "s30_NODE_5_length_161239_cov_20.1237\n",
      "s30_NODE_6_length_157102_cov_20.734\n",
      "s30_NODE_7_length_156768_cov_44.8078\n",
      "s30_NODE_8_length_152691_cov_19.6759\n",
      "s30_NODE_9_length_121154_cov_21.6491\n",
      "s30_NODE_10_length_119726_cov_136.834\n",
      "Type of lengths: <class 'list'>\n",
      "Length of lengths: 39551\n",
      "\n",
      "First 10 elements of lengths:\n",
      "245508\n",
      "222690\n",
      "222459\n",
      "173155\n",
      "161239\n",
      "157102\n",
      "156768\n",
      "152691\n",
      "121154\n",
      "119726\n"
     ]
    }
   ],
   "source": [
    "# Let's have a look at the resulting data\n",
    "\n",
    "print('Type of tnfs:', type(tnfs), 'of dtype', tnfs.dtype)\n",
    "print('Shape of tnfs:', tnfs.shape, end='\\n\\n')\n",
    "\n",
    "print('Type of contignames:', type(contignames))\n",
    "print('Length of contignames:', len(contignames), end='\\n\\n')\n",
    "\n",
    "print('First 10 elements of contignames:')\n",
    "for i in range(10):\n",
    "    print(contignames[i])\n",
    "    \n",
    "print('Type of lengths:', type(lengths))\n",
    "print('Length of lengths:', len(lengths), end='\\n\\n')\n",
    "\n",
    "print('First 10 elements of lengths:')\n",
    "for i in range(10):\n",
    "    print(lengths[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The tnfs is the tetranucleotide frequency - it's the frequency of the canonical kmer of each 4mer in the contig. The matrix is normalized across samples such that the frequency of e.g. 'AGGC' is measured relative to other contigs.\n",
    "\n",
    "Here, you should probably consider whether you can keep everything in memory. If not, all the relevant modules have reading and writing functions so you can dump the results to disk and delete them from memory. This is a small dataset, so there's no problem. With hundreds of samples and millions of contigs, this becomes a problem, even though Vamb is fairly memory-friendly.\n",
    "\n",
    "As a rule of thumb, the memory consumption for the most memory intensive step is approximately 8 \\* (n_samples + 136) \\* n_contigs bytes plus a little bit of overhead. If this fits snugly in your RAM with some room to spare, don't worry about the RAM.\n",
    "\n",
    "In my example, I have 6 samples and 39551 contigs for a total memory usage of ~45 MB.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the BAM files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function read_bamfiles in module vamb.parsebam:\n",
      "\n",
      "read_bamfiles(paths, minscore=50, minlength=2000, processors=4)\n",
      "    Spawns processes to parse BAM files and get contig rpkms.\n",
      "    \n",
      "    Input:\n",
      "        path: Path to BAM file\n",
      "        minscore [50]: Minimum alignment score (AS field) to consider\n",
      "        minlength [2000]: Discard any references shorter than N bases \n",
      "        processors [all]: Number of processes to spawn\n",
      "    \n",
      "    Outputs:\n",
      "        sample_rpkms: A {path: Numpy-32-float-RPKM} dictionary\n",
      "        contignames: A list of contignames from first BAM header\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(vamb.parsebam.read_bamfiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We can see (in the default value for the `processors` argument) that the function detects 4 CPUs on my laptop. That means we can read 4 BAM files at a time. Each BAM file being read takes up some memory, but unless you have a machine with tonnes of CPUs and little RAM, that's not going to be an issue. In either case, it's probably going to be IO bound when we give it 8+ cores, so that's unlikely to become an issue.\n",
    "\n",
    "This step is not super optimized, but the largest file is just 1.8 GB, so it takes a few minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bamfiles = !ls /home/jakni/Downloads/example/bamfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jakni/Downloads/example/bamfiles/e101.filtered.bam',\n",
       " '/home/jakni/Downloads/example/bamfiles/e178.filtered.bam',\n",
       " '/home/jakni/Downloads/example/bamfiles/e179.filtered.bam',\n",
       " '/home/jakni/Downloads/example/bamfiles/e196.filtered.bam',\n",
       " '/home/jakni/Downloads/example/bamfiles/e198.filtered.bam',\n",
       " '/home/jakni/Downloads/example/bamfiles/e30.filtered.bam']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bamfiles = ['/home/jakni/Downloads/example/bamfiles/' + p for p in bamfiles]\n",
    "bamfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That looks right.\n",
    "\n",
    "# We already have the contignames, so who cares about saving that\n",
    "sample_rpkms, _ = vamb.parsebam.read_bamfiles(bamfiles)\n",
    "del _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of sample_rpkms: <class 'dict'>\n",
      "Content of the dict:\n",
      "First key: /home/jakni/Downloads/example/bamfiles/e101.filtered.bam\n",
      "First value: [0.11535063 0.17579393 0.58409214 ... 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print('Type of sample_rpkms:', type(sample_rpkms))\n",
    "\n",
    "print('Content of the dict:')\n",
    "print('First key:', next(iter(sample_rpkms.keys())))\n",
    "print('First value:', next(iter(sample_rpkms.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "This is the depths (RPKM) table, here in the form of a dict with one item per column of the dict.\n",
    "\n",
    "We need to convert this to a Numpy array to feed it to the VAE.\n",
    "\n",
    "This is where RAM might be an issue, as you'll have two copies of the depths table in memory. To get around that, you can use the `write_rpkms` and `array_fromnpz` functions to write to disk, then read it in in a single matrix.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function array_from_sample_rpkms in module vamb.parsebam:\n",
      "\n",
      "array_from_sample_rpkms(sample_rpkms, columns)\n",
      "    Creates a (n-contigs x n-bamfiles) array from a sample_rpkms\n",
      "    (expected to be a {path: Numpy-32-float-RPKM} dictionary)\n",
      "    \n",
      "    Inputs: \n",
      "        sample_rpkms: A {path: Numpy-32-float-RPKM} dictionary\n",
      "        columns: Names of BAM file paths in sample_rpkms object in correct order\n",
      "        \n",
      "    Output: A (n-contigs x n-bamfiles) array with each column being the corre-\n",
      "    sponding array from sample_rpkms, normalized so each row sums to 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(vamb.parsebam.array_from_sample_rpkms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpkms = vamb.parsebam.array_from_sample_rpkms(sample_rpkms, bamfiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now, I tend to be a bit ~~paranoid~~<sup>careful</sup>, so if I loaded in 500 GB of BAM files, I'd want to save the work I have now in case something goes wrong - and we're about to fire up the VAE so lots of things can go wrong.\n",
    "\n",
    "What importants objects do I have in memory?\n",
    "\n",
    "* contignames: A list of contignames\n",
    "* lengths: A list of contig lengths\n",
    "* rpkms: A numpy array of rpkms\n",
    "* tnfs: A numpy array of tnfs\n",
    "\n",
    "I'm going to use `pickle` to save the Python lists and `vamb.vambtools.write_npz` to save the Numpy arrays (the latter is just a wrapper for `numpy.savez_compressed`). Of course, I could have used pickle for it all.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('/home/jakni/Downloads/example/contignames.pickle', 'wb') as file:\n",
    "    pickle.dump(contignames, file, protocol=4)\n",
    "\n",
    "with open('/home/jakni/Downloads/example/lengths.pickle', 'wb') as file:\n",
    "    pickle.dump(lengths, file, protocol=4)\n",
    "\n",
    "with open('/home/jakni/Downloads/example/tnfs.npz', 'wb') as file:\n",
    "    vamb.vambtools.write_npz(file, tnfs)\n",
    "    \n",
    "with open('/home/jakni/Downloads/example/rpkms.npz', 'wb') as file:\n",
    "    vamb.vambtools.write_npz(file, rpkms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we need to train the autoencoder\n",
    "\n",
    "Again, you can use `help` to see how to use the module\n",
    "\n",
    "`help(vamb.encode)`\n",
    "\n",
    "    Help on module vamb.encode in vamb:\n",
    "\n",
    "    NAME\n",
    "        vamb.encode - Encode a depths matrix and a tnf matrix to latent representation.\n",
    "\n",
    "    DESCRIPTION\n",
    "        Creates a variational autoencoder in PyTorch and tries to represent the depths\n",
    "        and tnf in the latent space under gaussian noise.\n",
    "\n",
    "        usage:\n",
    "        >>> vae, dataloader = trainvae(depths, tnf) # Make & train VAE on Numpy arrays\n",
    "        >>> latent = vae.encode(dataloader) # Encode to latent representation\n",
    "        >>> latent.shape\n",
    "        (183882, 40)\n",
    "        \n",
    "    [ lines elided ]\n",
    "    \n",
    "---\n",
    "Aha, so we need to use the `trainvae` function first, then the `VAE.encode` method. You can call the `help` functions on those, but I'm not showing that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tLoss: 2.4766\tBCE: 2.4632\tMSE: 0.00394\tKLD: 0.0094\n",
      "Epoch: 2\tLoss: 2.2839\tBCE: 2.2691\tMSE: 0.00599\tKLD: 0.0088\n",
      "Epoch: 3\tLoss: 2.1273\tBCE: 2.1102\tMSE: 0.00837\tKLD: 0.0087\n",
      "Epoch: 4\tLoss: 1.9752\tBCE: 1.9554\tMSE: 0.01141\tKLD: 0.0083\n",
      "Epoch: 5\tLoss: 1.8483\tBCE: 1.8260\tMSE: 0.01442\tKLD: 0.0079\n"
     ]
    }
   ],
   "source": [
    "# I'm training just 5 epochs for this demonstration.\n",
    "# When actually using the VAE, 200-300 epochs are suitable\n",
    "vae, dataloader = vamb.encode.trainvae(rpkms, tnfs, nepochs=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We can see the Mean Squared Error (which is the TNF-related loss) is rising these first 5 epochs, presumably as it sacrifices an efficient representation of the TNF in order to learn the depths (whose loss is BCE) better. This is quite expected, and we have the MSE loss typically 2-3 orders of magnitude less than BCE exactly so it will make this choice.\n",
    "\n",
    "Okay, so now we have the trained `vae` and the `dataloader`. Let's feed the dataloader to the VAE in order to get the latent representation:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39551, 40)\n"
     ]
    }
   ],
   "source": [
    "latent = vae.encode(dataloader)\n",
    "\n",
    "print(latent.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "That's 39551 contigs each represented by the value of 40 latent neurons.\n",
    "\n",
    "Now we need to cluster this. But first, we must determine a proper clustering threshold.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining the clustering threshold\n",
    "\n",
    "# Also add this part when it's stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering the latent representation\n",
    "\n",
    "There's two clustering algorithms: A `vamb.cluster.cluster`, an accurate one which scales badly (quadratically) with large datasets (up to one or two million contigs is alright depending on your patience), and `vamb.cluster.tandemcluster`, a less accurate one which scales better.\n",
    "\n",
    "The heavy lifting here is done in Numpy, so it might be worth making sure the BLAS library your Numpy is using is fast. You can check it with `numpy.__config__.show()` and if it says something with `mkl` or `openblas`, you're golden.\n",
    "\n",
    "I have a measly 40k contigs, so I'm obviously going for the slow but accurate function.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function cluster in module vamb.cluster:\n",
      "\n",
      "cluster(matrix, labels, inner, outer=None, max_steps=15, spearman=False)\n",
      "    Iterative medoid cluster generator. Yields (medoid), set(labels) pairs.\n",
      "    \n",
      "    Inputs:\n",
      "        matrix: A (obs x features) Numpy matrix of values\n",
      "        labels: Numpy array with labels for matrix rows. None or 1-D array\n",
      "        inner: Optimal medoid search within this distance from medoid\n",
      "        outer: Radius of clusters extracted from medoid. If None, same as inner\n",
      "        max_steps: Stop searching for optimal medoid after N futile attempts\n",
      "        spearman: Use Spearman, not Pearson correlation\n",
      "    \n",
      "    Output: Generator of (medoid, set(labels_in_cluster)) tuples.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(vamb.cluster.cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(contignames)\n",
    "cluster_iterator = vamb.cluster.cluster(latent, labels, threshold)\n",
    "\n",
    "clusters = dict()\n",
    "for medoid, contigs in cluster_iterator:\n",
    "    clusters[medoid] = contigs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing the clusters\n",
    "\n",
    "This is not automatic, because how to do it really depends on what you're looking for in your data.\n",
    "\n",
    "One of the greatest weaknesses of Vamb is that the bins tend to be highly fragmented. You'll have lots of tiny bins, some of which are legitimate (viruses, plasmids), but most are parts of larger genomes that didn't get binned properly.\n",
    "\n",
    "Here, let's say we're only interested in bacteria. So we throw away all bins with less than 250,000 basepairs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's make a contignames: length dict\n",
    "lengthof = dict(zip(contignames, lengths))\n",
    "\n",
    "# Now filter away the small bins\n",
    "filtered_bins = dict()\n",
    "\n",
    "for medoid, contigs in clusters.items():\n",
    "    binsize = sum(lengthof[contig] for contig in contigs)\n",
    "    \n",
    "    if binsize >= 250000:\n",
    "        filtered_bins[medoid] = contigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bins before filtering: 6641\n",
      "Number of bins after filtering: 113\n"
     ]
    }
   ],
   "source": [
    "print('Number of bins before filtering:', len(clusters))\n",
    "print('Number of bins after filtering:', len(filtered_bins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now, let's print them. For this we will use two writer functions:\n",
    "\n",
    "1) `vamb.cluster.writeclusters`, which writes which clusters contains which contigs to a simple tab-separated file, and\n",
    "\n",
    "2) `vamb.vambtools.writebins`, which writes FASTA files corresponding to each of the bins to a directory.\n",
    "\n",
    "We will need to load all the contigs belonging to any bin into memory to use `vamb.vambtools.writebins`. If your bins don't fit in memory, you gotta find another way to make those FASTA bins\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/jakni/Downloads/example/bins.tsv', 'w') as file:\n",
    "    vamb.cluster.writeclusters(file, filtered_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep contigs in any filtered bin in memory\n",
    "allcontigs = set.union(*filtered_bins.values())\n",
    "\n",
    "with open('/home/jakni/Downloads/example/contigs.fna', 'rb') as file:\n",
    "    fastadict = vamb.vambtools.loadfasta(file, keep=allcontigs)\n",
    "    \n",
    "vamb.vambtools.writebins('/home/jakni/Downloads/example/bins/', filtered_bins, fastadict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (If you have a reference: Benchmark the output)\n",
    "\n",
    "For this to make any sense, you need to have a *reference*, that is, a list of bins that are deemed true and complete.\n",
    "\n",
    "The reference could be a {clustername: set(contigs)} dict along with a {contigname: length} dict, just like the `clusters` and `lengthof` we made. It could also be a tab-separated file with clustername, contigname, length-rows, one row per contig.\n",
    "\n",
    "Now, I have no reference for this dataset, so I created a reference file completely randomly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# binname contigname length\r\n",
      "0\ts198_NODE_2960_length_5085_cov_5.30505\t5085\r\n",
      "0\ts30_NODE_9489_length_2530_cov_2.23365\t2530\r\n",
      "0\ts179_NODE_2638_length_5642_cov_2.52661\t5642\r\n",
      "0\ts30_NODE_160_length_42890_cov_12.914\t42890\r\n",
      "0\ts198_NODE_4819_length_3620_cov_4.35672\t3620\r\n",
      "0\ts178_NODE_2065_length_4779_cov_4.00513\t4779\r\n",
      "0\ts198_NODE_1167_length_8851_cov_6.04376\t8851\r\n",
      "0\ts198_NODE_7205_length_2698_cov_5.10081\t2698\r\n",
      "0\ts198_NODE_5233_length_3401_cov_5.00303\t3401\r\n"
     ]
    }
   ],
   "source": [
    "!head /home/jakni/Downloads/example/reference.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We of course expect the benchmark to show we have at most a handful of very incomplete bins, since the reference is random.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_path = '/home/jakni/Downloads/example/reference.tsv'\n",
    "\n",
    "with open(reference_path) as filehandle:b\n",
    "    reference = vamb.benchmark.Reference.fromfile(filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We also need to instantiate the Observed bins (which we created above!), and a BenchMarkResult\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could also do this from a file, but here we have the dictionary at hand\n",
    "observed = vamb.benchmark.Observed(filtered_bins, reference)\n",
    "\n",
    "# Keyword-only arguments to make sure you don't accidentally swap them around.\n",
    "# It'll raise an error if you use non-keyword arguments.\n",
    "result = vamb.benchmark.BenchMarkResult(reference=reference, observed=observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = vamb.benchmark.BenchMarkResult(reference=reference, observed=observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tRecall\n",
      "Prec.\t0.3\t0.4\t0.5\t0.6\t0.7\t0.8\t0.9\t0.95\n",
      "0.7\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "0.8\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "0.9\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "0.95\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "0.99\t0\t0\t0\t0\t0\t0\t0\t0\n"
     ]
    }
   ],
   "source": [
    "# Okay, how did we do?\n",
    "result.printmatrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "As expected (because the reference was randomly generated), the results are terrible - in fact, they couldn't be worse.\n",
    "\n",
    "To check what else the BenchMarkResult measures, check `help(vamb.benchmark.BenchMarkResult)`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now what?\n",
    "\n",
    "Here's some ideas:\n",
    "\n",
    "__Quality control your bins with CheckM__\n",
    "\n",
    "CheckM tries to asses the contimation and completeness of the given bins. It also tries to classify them... with more limited success\n",
    "\n",
    "__Improve the bins with Stranglerfig or RefineM__\n",
    "\n",
    "These tools inspect your bins and refines them by reassigning contigs between bins."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
