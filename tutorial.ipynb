{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO:\n",
    "\n",
    "Rewrite this notebook:\n",
    "\n",
    "1) Installation - elide the importing to Python part\n",
    "\n",
    "2) How to use from command line\n",
    "\n",
    "3) How to use from Python w. examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "\n",
    "Since Python is interpreted and cross-platform you just have to get the files on your computer:\n",
    "\n",
    "__If you have `git` installed__:\n",
    "\n",
    "    [jakni@nissen:scripts]$ git clone https://github.com/jakobnissen/vamb vamb\n",
    "    \n",
    "__If you don't__\n",
    "\n",
    "You then presumably have access to the vamb directory with this notebook, so just put it wherever:\n",
    "\n",
    "    [jakni@nissen:scripts]$ cp -r /path/to/vamb/directory vamb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart\n",
    "\n",
    "Take a brief look on the options with:\n",
    "\n",
    "    [jakni@nissen:scripts]$ python path/to/vamb/runvamb.py --help\n",
    "\n",
    "Do the defaults look alright? They probably do, but you might want to check number of processes to launch, GPU acceleration and whether you want the faster `tandemclustering` option enabled.\n",
    "\n",
    "Then just do:\n",
    "\n",
    "    [jakni@nissen:scripts]$ python path/to/vamb/runvamb.py outdir contig.fna path/to/bamfiles/*.bam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "Like other metagenomic binners, Vamb relies on two properties of the DNA sequences to be binned:\n",
    "\n",
    "* The kmer-composition of the sequence (here tetranucleotide frequency, *TNF*).\n",
    "* The abundance of the contigs in each sample (the *depth* or the *RPKM*), and\n",
    "\n",
    "So before you can run Vamb, you need to have files from which Vamb can calculate these values.\n",
    "\n",
    "* TNF is calculated from a regular fasta file of DNA sequences.\n",
    "* Depth is calculated from BAM-files of mapping reads to that same fasta file.\n",
    "\n",
    "The observed values for both of these measures become uncertain when the sequences is too short due to the law of large numbers. Therefore, Vamb works poorly on short sequences.\n",
    "\n",
    "With fewer samples (up to 100), we recommend using contigs from an assembly with a minimum contig length cutoff of ~2000-ish basepairs. With many samples, the number of contigs become overwhelming. The better approach is to split the dataset up into smaller chuncks and bin them independently.\n",
    "\n",
    "If needed, Vamb *can* also work on shorter sequences such as genes, which are more easily homology reduced and thus can support hundreds of samples.\n",
    "\n",
    "There are situations where you can't just filter the fasta file, maybe because you have already spent tonnes of time getting those BAM files and you're not going to remap if your life depended on it, or because your fasta file contains genes and so removing all entries less than e.g. 2000 bps is a bit too much to ask.\n",
    "\n",
    "In those situations, you can still pass the argument `minlength` if you want to have Vamb ignore the smaller contigs. This is not ideal, since the smaller, contigs will still have recruited some reads during mapping which are then not mapped to the larger contigs, but it can work alright.\n",
    "\n",
    "\n",
    "### Recommended preparation\n",
    "\n",
    "__1) Preprocess the reads and check their quality__\n",
    "\n",
    "We recommend AdapterRemoval combined with FastQC for this.\n",
    "\n",
    "__2) Assemble each sample individually OR co-assemble and get the contigs out__\n",
    "\n",
    "We recommend using metaSPAdes on each sample individually.\n",
    "\n",
    "__3) Concatenate the FASTA files together while making sure all contig headers stay unique__\n",
    "\n",
    "We recommend prepending the sample name to each contig header from that sample.\n",
    "\n",
    "__4) Remove all small contigs from the FASTA file__\n",
    "\n",
    "There's a tradeoff here between a too low cutoff, retaining hard-to-bin contigs which adversely affects the binning of all contigs, and throwing out good data. We recommend choosing a length cutoff of ~2000 bp.\n",
    "\n",
    "__5) Map the reads to the FASTA file to obtain 6 .bam files__\n",
    "\n",
    "We have used BWA MEM for mapping, fully aware that it is not suited for this task. In theory, any mapper that produces a BAM file with an alignment score tagged 'AS:i' and multiple secondary hits tagged 'XA:Z' can work.\n",
    "\n",
    "___\n",
    "\n",
    "In this tutorial, we have the two relevant prerequisite files in the directory `/home/jakni/Downloads/example`:\n",
    "\n",
    "* `contigs.fna` - The filtered FASTA contigs which were mapped against, and\n",
    "* `bamfiles/*.bam` - The 6 .bam files from mapping the reads to the contigs above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running from command line\n",
    "\n",
    "You can run either the entire pipeline from commandline, or each module independently.\n",
    "\n",
    "---\n",
    "\n",
    "__For the entire pipeline__, you need to use the `runvamb.py` script:\n",
    "\n",
    "    [jakni@nissen:~]$ python Documents/scripts/vamb/runvamb.py --help\n",
    "    usage: python runvamb.py OUTPATH FASTA BAMPATHS [OPTIONS ...]\n",
    "\n",
    "    Run the Vamb pipeline.\n",
    "\n",
    "    Creates a new direcotry and runs each module of the Vamb pipeline in the\n",
    "    new directory. Does not yet support resuming stopped runs - in order to do so,\n",
    "    \n",
    "    [ lines elided ]\n",
    "\n",
    "You use it like this:\n",
    "\n",
    "    [jakni@nissen:~] python path/to/vamb/runvamb.py output_directory contig.fna path/to/bamfiles/*.bam\n",
    "    \n",
    "__For each module__, you find the relevant script:\n",
    "\n",
    "    [jakni@nissen:~]$ python Documents/scripts/vamb/parsecontigs.py --help\n",
    "    usage: parsecontigs.py contigs.fna(.gz) tnfout lengthsout\n",
    "\n",
    "    Calculate z-normalized tetranucleotide frequency from a FASTA file.\n",
    "    \n",
    "    [ lines elided ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detailed walkthrough and running from the Python interpreter\n",
    "\n",
    "The Vamb pipeline consist of a handful of tasks each which have a dedicated module:\n",
    "\n",
    "---\n",
    "1) Parse fasta file and get TNF of each sequence, as well as sequence length and names\n",
    "\n",
    "2) Parse the BAM files and get depth estimate for each sequence in the fasta file\n",
    "\n",
    "3) Train a VAE with the depths and TNF matrices\n",
    "\n",
    "4) Encode the depths and TNF matrices using the VAE\n",
    "\n",
    "5) Cluster the encoded inputs to metgenomic bins\n",
    "\n",
    "---\n",
    "In this walkthrough, we will go through each step in more detail from within the Python interpreter. We will explain what each step does. With this knowledge, you should be able to extend Vamb relatively easily.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step zero: Importing Vamb and getting help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to get Vamb imported\n",
    "    \n",
    "    [jakni@nissen:~]$ python\n",
    "    >>> import vamb\n",
    "    Traceback (most recent call last):\n",
    "      File \"<stdin>\", line 1, in <module>\n",
    "    ModuleNotFoundError: No module named 'vamb'\n",
    "    >>> # We're not in the directory containing the vamb directory.\n",
    "    >>> # That means the directory containing the vamb dir is not in out sys.path.\n",
    "    >>> # Either move the vamb directory to one of your sys.path dirs \n",
    "    >>> # or add the vamb directory to sys.path. We'll do the latter.\n",
    "    >>> import sys\n",
    "    >>> sys.path.append('/home/jakni/Documents/scripts/')\n",
    "    >>> import vamb\n",
    "    >>> # No error message - success!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll almost certianly need help when using Vamb (we wish it was so easy you didn't, but making user friendly software is *hard!*).\n",
    "\n",
    "Luckily, there's the built-in `help` function in Python.\n",
    "\n",
    "---\n",
    "\n",
    "`>>> help(vamb)`\n",
    "    \n",
    "    Help on package vamb:\n",
    "\n",
    "    NAME\n",
    "        vamb - Variational Autoencoder for Metagenomic Binning\n",
    "\n",
    "    DESCRIPTION\n",
    "        Vamb does what it says on the tin - bins metagenomes using a variational autoencoder.\n",
    "        \n",
    "    [ lines elided ]\n",
    "    \n",
    "        General workflow:\n",
    "        1) Filter contigs by size using vamb.filtercontigs\n",
    "        2) Map reads to contigs to obtain BAM file\n",
    "        3) Calculate TNF of contigs using vamb.parsecontigs\n",
    "        4) Create RPKM table using vamb.parsebam\n",
    "        5) Train autoencoder using vamb.encode\n",
    "        6) Cluster latent representation using vamb.cluster\n",
    "    \n",
    "    [ lines elided ]\n",
    "    \n",
    "---\n",
    "    \n",
    "The `PACKAGE CONTENTS` under `help(vamb)` is just a list of all importable files in the `vamb` directory - some of these really shouldn't be imported, so ignore that.\n",
    "\n",
    "---\n",
    "You can also get help for the modules:\n",
    "\n",
    "`>>> help(vamb.cluster)`\n",
    "\n",
    "    Help on module vamb.cluster in vamb:\n",
    "\n",
    "    NAME\n",
    "        vamb.cluster - Iterative medoid clustering of Numpy arrays.\n",
    "\n",
    "    DESCRIPTION\n",
    "        Implements two core functions: cluster and tandemcluster, along with the helper\n",
    "        functions writeclusters and readclusters.\n",
    "        For all functions in this module, a collection of clusters are represented as\n",
    "        a {clustername, set(elements)} dict.\n",
    "\n",
    "        Clustering algorithm:\n",
    "    \n",
    "    [ lines elided ]\n",
    "        \n",
    "---\n",
    "And for functions:\n",
    "\n",
    "`>>> help(vamb.cluster.tandemcluster)`\n",
    "\n",
    "    Help on function tandemcluster in module vamb.cluster:\n",
    "\n",
    "    tandemcluster(matrix, labels, inner, outer=None, max_steps=15, spearman=False)\n",
    "        Splits the datasets, then clusters each partition before merging\n",
    "        the resulting clusters. This is faster, especially on larger datasets, but\n",
    "        less accurate than normal clustering.\n",
    "\n",
    "        Inputs:\n",
    "            matrix: A (obs x features) Numpy matrix of values\n",
    "            labels: Numpy array with labels for matrix rows. None or 1-D array\n",
    "            inner: Optimal medoid search within this distance from medoid\n",
    "            outer: Radius of clusters extracted from medoid. If None, same as inner\n",
    "            max_steps: Stop searching for optimal medoid after N futile attempts\n",
    "            spearman: Use Spearman, not Pearson correlation\n",
    "\n",
    "        Output: {medoid: set(labels_in_cluster) dictionary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jakni/Documents/scripts/')\n",
    "import vamb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step one: Parse the FASTA file\n",
    "\n",
    "If you forget what to do at each step, remember that `help(vamb)` said:\n",
    "\n",
    "    General workflow:\n",
    "    1) Filter contigs by size using vamb.filtercontigs\n",
    "    2) Map reads to contigs to obtain BAM file\n",
    "    3) Calculate TNF of contigs using vamb.parsecontigs\n",
    "    \n",
    "    [ lines elided ]\n",
    "\n",
    "Okay, we already have filtered contigs, and we have mapped reads to them and gotten BAM files, so we begin with `vamb.parsecontigs`. How do you use that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module vamb.parsecontigs in vamb:\n",
      "\n",
      "NAME\n",
      "    vamb.parsecontigs - Calculate z-normalized tetranucleotide frequency from a FASTA file.\n",
      "\n",
      "DESCRIPTION\n",
      "    Usage:\n",
      "    >>> with open('/path/to/contigs.fna', 'rb') as filehandle\n",
      "    ...     tnfs, contignames, lengths = read_contigs(filehandle)\n",
      "\n",
      "FUNCTIONS\n",
      "    read_contigs(byte_iterator, minlength=100)\n",
      "        Parses a FASTA file open in binary reading mode.\n",
      "        \n",
      "        Input:\n",
      "            byte_iterator: Iterator of binary lines of a FASTA file\n",
      "            minlength[100]: Ignore any references shorter than N bases \n",
      "        \n",
      "        Outputs:\n",
      "            tnfs: A (n_FASTA_entries x 136) matrix of tetranucleotide freq.\n",
      "            contignames: A list of contig headers\n",
      "            lengths: A list of contig lengths\n",
      "\n",
      "DATA\n",
      "    TNF_HEADER = '#contigheader\\tAAAA/TTTT\\tAAAC/GTTT\\tAAAG/CTTT\\tAAAT...A...\n",
      "    __cmd_doc__ = 'Calculate z-normalized tetranucleotide frequency...eoti...\n",
      "\n",
      "FILE\n",
      "    /home/jakni/Documents/scripts/vamb/parsecontigs.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(vamb.parsecontigs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "I use `vamb.parsecontigs.read_contigs` with the inputs and outputs as written:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file in binary mode - you can use the vamb.vambtools.Reader to read\n",
    "# from normal or gzipped file seamlessly. Here I just use the open function\n",
    "with open('/home/jakni/Downloads/example/contigs.fna', 'rb') as filehandle:\n",
    "    tnfs, contignames, lengths = vamb.parsecontigs.read_contigs(filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of tnfs: <class 'numpy.ndarray'> of dtype float32\n",
      "Shape of tnfs: (39551, 136)\n",
      "\n",
      "Type of contignames: <class 'list'>\n",
      "Length of contignames: 39551\n",
      "\n",
      "First 10 elements of contignames:\n",
      "s30_NODE_1_length_245508_cov_18.4904\n",
      "s30_NODE_2_length_222690_cov_39.7685\n",
      "s30_NODE_3_length_222459_cov_20.3665\n",
      "s30_NODE_4_length_173155_cov_20.1181\n",
      "s30_NODE_5_length_161239_cov_20.1237\n",
      "s30_NODE_6_length_157102_cov_20.734\n",
      "s30_NODE_7_length_156768_cov_44.8078\n",
      "s30_NODE_8_length_152691_cov_19.6759\n",
      "s30_NODE_9_length_121154_cov_21.6491\n",
      "s30_NODE_10_length_119726_cov_136.834\n",
      "\n",
      "\n",
      "Type of lengths: <class 'numpy.ndarray'>\n",
      "Length of lengths: 39551\n",
      "\n",
      "First 10 elements of lengths:\n",
      "245508\n",
      "222690\n",
      "222459\n",
      "173155\n",
      "161239\n",
      "157102\n",
      "156768\n",
      "152691\n",
      "121154\n",
      "119726\n"
     ]
    }
   ],
   "source": [
    "# Let's have a look at the resulting data\n",
    "\n",
    "print('Type of tnfs:', type(tnfs), 'of dtype', tnfs.dtype)\n",
    "print('Shape of tnfs:', tnfs.shape, end='\\n\\n')\n",
    "\n",
    "print('Type of contignames:', type(contignames))\n",
    "print('Length of contignames:', len(contignames), end='\\n\\n')\n",
    "\n",
    "print('First 10 elements of contignames:')\n",
    "for i in range(10):\n",
    "    print(contignames[i])\n",
    "    \n",
    "print('\\n')\n",
    "    \n",
    "print('Type of lengths:', type(lengths))\n",
    "print('Length of lengths:', len(lengths), end='\\n\\n')\n",
    "\n",
    "print('First 10 elements of lengths:')\n",
    "for i in range(10):\n",
    "    print(lengths[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "It turns out that related organisms tend to share a similar kmer-distribution across most of their genome. The reason for that is not understood, even though it's believed that common functional motifs, GC-content and presence/absence of endonucleases explains some of the observed similary.\n",
    "\n",
    "The `tnfs` is the tetranucleotide frequency - it's the frequency of the canonical kmer of each 4mer in the contig. The matrix is z-score normalized across contigs for each sample such that the frequency of e.g. 'AGGC' is measured relative to other contigs in that sample - this increases the signal-to-noise ratio.\n",
    "\n",
    "We use 4-mers because there are 136 canonical 4-mer, which is an appropriate number of features to cluster - not so few that there's no signal and not so many it becomes unwieldy and the estimates of the frequencies become uncertain.\n",
    "\n",
    "At this points, you should probably consider whether you can keep everything in memory. If not, all the relevant modules have reading and writing functions so you can dump the results to disk and delete them from memory. This is a small dataset, so there's no problem. With hundreds of samples and millions of contigs however, this becomes a problem, even though Vamb is fairly memory-friendly.\n",
    "\n",
    "As a rule of thumb, the memory consumption for the most memory intensive step is approximately 8 × (n_samples + 136) × n_contigs bytes plus a little bit of overhead. If this is much lower than your RAM, don't worry about it. If it's within a factor 5 of your available RAM, you'll need to delete objects you don't need anymore.\n",
    "\n",
    "In my example, I have 6 samples and 39551 contigs for a total memory usage of ~45 MB.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step two: Parsing the BAM files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function read_bamfiles in module vamb.parsebam:\n",
      "\n",
      "read_bamfiles(paths, minscore=50, minlength=100, processors=4)\n",
      "    Spawns processes to parse BAM files and get contig rpkms.\n",
      "    \n",
      "    Input:\n",
      "        path: Path to BAM file\n",
      "        minscore [50]: Minimum alignment score (AS field) to consider\n",
      "        minlength [100]: Ignore any references shorter than N bases \n",
      "        processors [all]: Number of processes to spawn\n",
      "    \n",
      "    Outputs:\n",
      "        sample_rpkms: A {path: Numpy-32-float-RPKM} dictionary\n",
      "        contignames: A list of contignames from first BAM header\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Again, we can use the help function to see what we need to do\n",
    "help(vamb.parsebam.read_bamfiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We can see (in the default value for the `processes` argument) that the function detects 4 cores on my laptop. It will then spawn 4 parallel processes to read the BAM files. It's capped at 8 processes, because at that level, it almost certainly becomes I/O bound.\n",
    "\n",
    "As with the `vamb.parsecontigs.read_contigs` function, I don't care about the `minlength` argument, since our fasta file is already filtered.\n",
    "\n",
    "Lastly, the function ignores all alignments with alignment score less than 50 (as determined by the optional `AS:i` field in the BAM file). That seems reasonable here.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jakni/Downloads/example/bamfiles/e101.filtered.bam',\n",
       " '/home/jakni/Downloads/example/bamfiles/e178.filtered.bam',\n",
       " '/home/jakni/Downloads/example/bamfiles/e179.filtered.bam',\n",
       " '/home/jakni/Downloads/example/bamfiles/e196.filtered.bam',\n",
       " '/home/jakni/Downloads/example/bamfiles/e198.filtered.bam',\n",
       " '/home/jakni/Downloads/example/bamfiles/e30.filtered.bam']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bamfiles = !ls /home/jakni/Downloads/example/bamfiles\n",
    "bamfiles = ['/home/jakni/Downloads/example/bamfiles/' + p for p in bamfiles]\n",
    "bamfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That looks right.\n",
    "\n",
    "# We already have the contignames, so who cares about saving that\n",
    "sample_rpkms, _ = vamb.parsebam.read_bamfiles(bamfiles)\n",
    "del _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of sample_rpkms: <class 'dict'>\n",
      "Content of the dict:\n",
      "First key: /home/jakni/Downloads/example/bamfiles/e101.filtered.bam\n",
      "First value: [0.11535063 0.17579393 0.58409214 ... 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print('Type of sample_rpkms:', type(sample_rpkms))\n",
    "\n",
    "print('Content of the dict:')\n",
    "print('First key:', next(iter(sample_rpkms.keys())))\n",
    "print('First value:', next(iter(sample_rpkms.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The idea here is that two contigs from the same genome will always be physically present together, and so they should have a similar abundance across all samples. Some contigs represent repeats like duplicated segments - these contigs should have a fixed ratio of abundance to other contigs. Thus, even when considering repeated contigs, there should be a tight Pearson correlation between abundances of contigs from the same genome.\n",
    "\n",
    "The `vamb.parsebam` module takes a rather crude approach to estimating abundance, namely by simply counting the number of mapped reads to each contig, divided by total number of reads and the contig's length. This measure is in trancriptomics often called RPKM, *reads per kilobase per million mapped reads*. Other metagenomic binners like Metabat and Canopy uses an average of per-nucleotide depth of coverage instead. We do not believe there is any theoretical or practical advantage of using depth over RPKM. We will use the terms *depth* and *rpkm* interchangably.\n",
    "\n",
    "The object we just created, `sample_rpkms` is the depth table in the form of a dictionary with one column (representing one BAM file) per entry. Just like the TNF, this needs to be converted to a (n_contigs x n_features) Numpy array in order for it to be used in the variational autoencoder.\n",
    "\n",
    "We can use the function `vamb.parsebam.array_from_sample_rpkms` to do this. This will create a new array in memory while retaining `sample_rpkms`. Each object requires approximately 4 x n_contigs x n_samples bytes. If having two of these objects in memory will consume all your RAM, you can dump the `sample_rpkms` to disk, delete it, and reload it in a matrix using the functions `write_rpkms` and `array_fromnpz` in the `vamb.parsebam` module.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpkms = vamb.parsebam.array_from_sample_rpkms(sample_rpkms, bamfiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now, I tend to be a bit ~~paranoid~~<sup>careful</sup>, so if I loaded in 500 GB of BAM files, I'd want to save the work I have now in case something goes wrong - and we're about to fire up the VAE so lots of things can go wrong.\n",
    "\n",
    "What importants objects do I have in memory right now?\n",
    "\n",
    "* contignames: A list of contignames\n",
    "* lengths: A list of contig lengths\n",
    "* rpkms: A numpy array of rpkms\n",
    "* tnfs: A numpy array of tnfs\n",
    "\n",
    "I'm going to use `pickle` to save the Python list and `vamb.vambtools.write_npz` to save the Numpy arrays (the latter is just a wrapper for `numpy.savez_compressed`). Of course, I could have used pickle for it all.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('/home/jakni/Downloads/example/contignames.pickle', 'wb') as file:\n",
    "    pickle.dump(contignames, file, protocol=4)\n",
    "\n",
    "with open('/home/jakni/Downloads/example/lengths.npz', 'wb') as file:\n",
    "    vamb.vambtools.write_npz(file, lengths)\n",
    "\n",
    "with open('/home/jakni/Downloads/example/tnfs.npz', 'wb') as file:\n",
    "    vamb.vambtools.write_npz(file, tnfs)\n",
    "    \n",
    "with open('/home/jakni/Downloads/example/rpkms.npz', 'wb') as file:\n",
    "    vamb.vambtools.write_npz(file, rpkms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step three: Train the autoencoder and encode imput data\n",
    "\n",
    "Again, you can use `help` to see how to use the module\n",
    "\n",
    "`help(vamb.encode)`\n",
    "\n",
    "    Help on module vamb.encode in vamb:\n",
    "\n",
    "    NAME\n",
    "        vamb.encode - Encode a depths matrix and a tnf matrix to latent representation.\n",
    "\n",
    "    DESCRIPTION\n",
    "        Creates a variational autoencoder in PyTorch and tries to represent the depths\n",
    "        and tnf in the latent space under gaussian noise.\n",
    "\n",
    "        usage:\n",
    "        >>> vae, dataloader = trainvae(depths, tnf) # Make & train VAE on Numpy arrays\n",
    "        >>> latent = vae.encode(dataloader) # Encode to latent representation\n",
    "        >>> latent.shape\n",
    "        (183882, 40)\n",
    "        \n",
    "    [ lines elided ]\n",
    "    \n",
    "---\n",
    "Aha, so we need to use the `trainvae` function first, then the `VAE.encode` method. You can call the `help` functions on those, but I'm not showing that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tLoss: 2.4766\tBCE: 2.4632\tMSE: 0.00394\tKLD: 0.0094\n",
      "Epoch: 2\tLoss: 2.2839\tBCE: 2.2691\tMSE: 0.00599\tKLD: 0.0088\n",
      "Epoch: 3\tLoss: 2.1273\tBCE: 2.1102\tMSE: 0.00837\tKLD: 0.0087\n",
      "Epoch: 4\tLoss: 1.9752\tBCE: 1.9554\tMSE: 0.01141\tKLD: 0.0083\n",
      "Epoch: 5\tLoss: 1.8483\tBCE: 1.8260\tMSE: 0.01442\tKLD: 0.0079\n"
     ]
    }
   ],
   "source": [
    "# I'm training just 5 epochs for this demonstration.\n",
    "# When actually using the VAE, 200-300 epochs are suitable\n",
    "vae, dataloader = vamb.encode.trainvae(rpkms, tnfs, nepochs=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The VAE encodes the high-dimensional (n_samples + 136 features) input data in a lower dimensional space (nlatent features). When training, it learns both the encoding scheme and attempts to reconstruct the input data given the latent representation influenced by gaussian noise.\n",
    "\n",
    "The theory here is that the latent representation should be a more efficient encoding of the input data. If the input data for the contigs indeed do fall into bins, an efficient encoding should be to simply encode the bin they belong to, then use the \"bin identity\" to reconstruct the data. We add noise to prevent it from learning a huge number of slightly different bins, in the most extreme, each bin contains only one contig.\n",
    "\n",
    "The loss of the VAE is the sum of three measures:\n",
    "\n",
    "* Binary cross entropy (BCE) measures the dissimilarity of the reconstructed abundances to observed abundances\n",
    "* Mean squared error (MSE) measures the dissimilary of reconstructed versus observed TNF\n",
    "* Kullback-Leibler divergence (KLD) measures the dissimilarity between the standard normal distribution and the distribution of values sampled from the latent layer with the gaussian noise\n",
    "\n",
    "At least in principle, the latter term incudes the VAE to not crazily overfit by imposing some sensible prior on the kind of encodings it can choose.\n",
    "\n",
    "We can see the Mean Squared Error (which is the TNF-related loss) is rising these first 5 epochs, presumably as it sacrifices an efficient representation of the TNF in order to learn the depths (whose loss is BCE) better. This happens sometimes, and it's alright - after all, co-abundance usually contain more information that TNF, and so we have chosen the BCE to be several orders of magnitude over MSE in order for the VAE to be able to make this choice.\n",
    "\n",
    "Okay, so now we have the trained `vae` and the `dataloader`. Let's feed the dataloader to the VAE in order to get the latent representation:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39551, 40)\n"
     ]
    }
   ],
   "source": [
    "latent = vae.encode(dataloader)\n",
    "\n",
    "print(latent.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "That's 39551 contigs each represented by the (non-noisy) value of 40 latent neurons.\n",
    "\n",
    "Now we need to cluster this. But first, we must determine a proper clustering threshold.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step four: Determining the clustering threshold\n",
    "\n",
    "__To be added when we've got a stable API for determining this__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step five: Clustering (binning) the latent representation\n",
    "\n",
    "Fundamentally, the process of binning is just clustering sequences based on some of their properties. The purpose of encoding the contigs to a lossy latent representation is to ease the process of clustering because contigs with similar properties are placed close together in latent space, and the latent space is smaller than the input feature space.\n",
    "\n",
    "With the latent representation conveniently represented by an (n_contigs x n_features) matrix, you could use any clustering algorithm to cluster them (such as the ones in `sklearn.cluster`). In practice though, you have likely a few million contigs and prior constrains on the diameter, shape and size of the clusters.\n",
    "\n",
    "The module `vamb.cluster` implements a simple and fast iterative medoid clustering algorithm. It is well suited for spherical clusters with a maximum size and for many samples. It is similar to the clustering algorithm used in the metagenomic binner Canopy.\n",
    "\n",
    "    Clustering algorithm:\n",
    "    (1): Pick random seed observation S\n",
    "    (2): Define inner_obs(S) = all observations with Pearson distance from S < INNER\n",
    "    (3): Sample MOVES observations I from inner_obs\n",
    "    (4): If any inner_obs(i) > inner_obs(S) for i in I: Let S be i, go to (2)\n",
    "         Else: Outer_obs(S) = all observations with Pearson distance from S < OUTER\n",
    "    (5): Output outer_obs(S) as cluster, remove inner_obs(S) from observations\n",
    "    (6): If no more observations or MAX_CLUSTERS have been reached: Stop\n",
    "         Else: Go to (1)\n",
    "\n",
    "We have implemented the algorithm in two functions: \n",
    "\n",
    "* `vamb.cluster.cluster`, simply clusters a matrix, and so scales approximately O(n<sup>2</sup>).\n",
    "\n",
    "* `vamb.cluster.tandemcluster` does some very rough preclustering and then clusters each precluster using `vamb.cluster.cluster`. Each observation is then assigned uniquely to the largest cluster it's a member of. This scales better with number of contigs, but accuracy is lost in the preclustering step.\n",
    "\n",
    "You can use the slow-but-accurate with up to one or two million contigs depending on your patience or ~10 million contigs if you're alright with running it for days.\n",
    "\n",
    "The heavy lifting here is done in Numpy, so it might be worth making sure the BLAS library your Numpy is using is fast. You can check it with `numpy.__config__.show()` and if it says anything other than `NOT AVAILABLE` under the `mkl` or `openblas` entries, you're golden.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function cluster in module vamb.cluster:\n",
      "\n",
      "cluster(matrix, labels, inner, outer=None, max_steps=15, spearman=False)\n",
      "    Iterative medoid cluster generator. Yields (medoid), set(labels) pairs.\n",
      "    \n",
      "    Inputs:\n",
      "        matrix: A (obs x features) Numpy matrix of values\n",
      "        labels: Numpy array with labels for matrix rows. None or 1-D array\n",
      "        inner: Optimal medoid search within this distance from medoid\n",
      "        outer: Radius of clusters extracted from medoid. If None, same as inner\n",
      "        max_steps: Stop searching for optimal medoid after N futile attempts\n",
      "        spearman: Use Spearman, not Pearson correlation\n",
      "    \n",
      "    Output: Generator of (medoid, set(labels_in_cluster)) tuples.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(vamb.cluster.cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(contignames)\n",
    "\n",
    "# Unlike tandemcluster, which outputs the dictionary directly,\n",
    "# the output of cluster is a generator\n",
    "cluster_iterator = vamb.cluster.cluster(latent, labels, threshold)\n",
    "\n",
    "clusters = dict()\n",
    "for medoid, contigs in cluster_iterator:\n",
    "    clusters[medoid] = contigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'medoid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ecaafe016729>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmedoid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Type of values:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontigs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'medoid' is not defined"
     ]
    }
   ],
   "source": [
    "print(medoid)\n",
    "print('Type of values:', type(contigs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step six: Postprocessing the clusters\n",
    "\n",
    "We haven't written any postprocessing modules because how to postprocess really depends on what you're looking for in your data.\n",
    "\n",
    "One of the greatest weaknesses of Vamb - probably of metagenomic binners in general - is that the bins tend to be highly fragmented. You'll have lots of tiny bins, some of which are legitimate (viruses, plasmids), but most are parts of larger genomes that didn't get binned properly.\n",
    "\n",
    "Here, let's say we're only interested in bacteria. So we throw away all bins with less than 250,000 basepairs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's make a contignames: length dict\n",
    "lengthof = dict(zip(contignames, lengths))\n",
    "\n",
    "# Now filter away the small bins\n",
    "filtered_bins = dict()\n",
    "\n",
    "for medoid, contigs in clusters.items():\n",
    "    binsize = sum(lengthof[contig] for contig in contigs)\n",
    "    \n",
    "    if binsize >= 250000:\n",
    "        filtered_bins[medoid] = contigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bins before filtering: 6641\n",
      "Number of bins after filtering: 113\n"
     ]
    }
   ],
   "source": [
    "print('Number of bins before filtering:', len(clusters))\n",
    "print('Number of bins after filtering:', len(filtered_bins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now, let's print them. For this we will use two writer functions:\n",
    "\n",
    "1) `vamb.cluster.writeclusters`, that writes which clusters contains which contigs to a simple tab-separated file, and\n",
    "\n",
    "2) `vamb.vambtools.writebins`, that writes FASTA files corresponding to each of the bins to a directory.\n",
    "\n",
    "We will need to load all the contigs belonging to any bin into memory to use `vamb.vambtools.writebins`. If your bins don't fit in memory, sorry, you gotta find another way to make those FASTA bins.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/jakni/Downloads/example/bins.tsv', 'w') as file:\n",
    "    vamb.cluster.writeclusters(file, filtered_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep contigs in any filtered bin in memory\n",
    "allcontigs = set.union(*filtered_bins.values())\n",
    "\n",
    "with open('/home/jakni/Downloads/example/contigs.fna', 'rb') as file:\n",
    "    fastadict = vamb.vambtools.loadfasta(file, keep=allcontigs)\n",
    "    \n",
    "vamb.vambtools.writebins('/home/jakni/Downloads/example/bins/', filtered_bins, fastadict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (If you have a reference: Benchmark the output)\n",
    "\n",
    "For this to make any sense, you need to have a *reference*, that is, a list of bins that are deemed true and complete.\n",
    "\n",
    "The reference could be a {clustername: set(contigs)} dict along with a {contigname: length} dict, just like the `clusters` and `lengthof` we made. It could also be a tab-separated file with (clustername, contigname, length)-rows, one row per contig.\n",
    "\n",
    "Now, I have no reference for this dataset, so I created a reference file completely randomly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# binname contigname length\r\n",
      "0\ts198_NODE_2960_length_5085_cov_5.30505\t5085\r\n",
      "0\ts30_NODE_9489_length_2530_cov_2.23365\t2530\r\n",
      "0\ts179_NODE_2638_length_5642_cov_2.52661\t5642\r\n",
      "0\ts30_NODE_160_length_42890_cov_12.914\t42890\r\n",
      "0\ts198_NODE_4819_length_3620_cov_4.35672\t3620\r\n",
      "0\ts178_NODE_2065_length_4779_cov_4.00513\t4779\r\n",
      "0\ts198_NODE_1167_length_8851_cov_6.04376\t8851\r\n",
      "0\ts198_NODE_7205_length_2698_cov_5.10081\t2698\r\n",
      "0\ts198_NODE_5233_length_3401_cov_5.00303\t3401\r\n"
     ]
    }
   ],
   "source": [
    "!head /home/jakni/Downloads/example/reference.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We of course expect the benchmark to show we have at most a handful of very incomplete bins, since the reference is random.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_path = '/home/jakni/Downloads/example/reference.tsv'\n",
    "\n",
    "with open(reference_path) as filehandle:\n",
    "    reference = vamb.benchmark.Reference.fromfile(filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We also need to instantiate the Observed bins (which we created above!), and a BenchMarkResult\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could also do this from the bins.tsv we created in the previous section,\n",
    "# but here we have the dictionary with bins in memory already.\n",
    "observed = vamb.benchmark.Observed(filtered_bins, reference)\n",
    "\n",
    "# Keyword-only arguments to make sure you don't accidentally swap them around.\n",
    "# It'll raise an error if you use non-keyword arguments.\n",
    "result = vamb.benchmark.BenchMarkResult(reference=reference, observed=observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = vamb.benchmark.BenchMarkResult(reference=reference, observed=observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tRecall\n",
      "Prec.\t0.3\t0.4\t0.5\t0.6\t0.7\t0.8\t0.9\t0.95\n",
      "0.7\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "0.8\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "0.9\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "0.95\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "0.99\t0\t0\t0\t0\t0\t0\t0\t0\n"
     ]
    }
   ],
   "source": [
    "# Okay, how did we do?\n",
    "result.printmatrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "This matrix shows the number of reference bins where, for each value of recall and precision there is at least one observed bin that passes those criteria.\n",
    "\n",
    "As expected (because the reference was randomly generated), the results are terrible - in fact, they couldn't be worse.\n",
    "\n",
    "To check what else the BenchMarkResult measures, check `help(vamb.benchmark.BenchMarkResult)`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now what?\n",
    "\n",
    "Here's some ideas:\n",
    "\n",
    "__Quality control your bins with CheckM__\n",
    "\n",
    "CheckM tries to asses the contimation and completeness of the given bins and works pretty well. It also tries to classify them taxonomically... with more limited success.\n",
    "\n",
    "__Improve the bins with Stranglerfig or RefineM__\n",
    "\n",
    "These tools inspect your bins and refines them by reassigning contigs between bins."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
