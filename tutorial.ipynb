{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "\n",
    "First step is to get Vamb on your computer.\n",
    "\n",
    "__If you have `git` installed__:\n",
    "\n",
    "    [jakni@nissen:Downloads]$ # Clone Vamb from GitHub into Downloads/vamb\n",
    "[jakni@nissen:Downloads]$ git clone https://github.com/jakobnissen/vamb vamb\n",
    "    \n",
    "__If you don't__\n",
    "\n",
    "    [jakni@nissen:Downloads]$ # You then presumably have access to a Vamb directory\n",
    "[jakni@nissen:Downloads]$ cp -r /path/to/vamb/directory vamb\n",
    "    \n",
    "---\n",
    "Now you have Vamb on your computer. Time to get it imported\n",
    "    \n",
    "    [jakni@nissen:~]$ python\n",
    "    >>> import vamb\n",
    "    Traceback (most recent call last):\n",
    "      File \"<stdin>\", line 1, in <module>\n",
    "    ModuleNotFoundError: No module named 'vamb'\n",
    "    >>> # We're not in the directory containing the vamb directory.\n",
    "    >>> # That means the directory containing the vamb dir is not in out sys.path.\n",
    "    >>> # Either move the vamb directory to one of you sys.path dirs \n",
    "    >>> # or add the vamb directory to sys.path. We'll do the latter.\n",
    "    >>> import sys\n",
    "    >>> sys.path.append('/home/jakni/Downloads')\n",
    "    >>> import vamb\n",
    "    >>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting help\n",
    "\n",
    "You'll almost certianly need help when using Vamb (we wish it was so easy you didn't, but making user friendly software is hard!).\n",
    "\n",
    "Luckily, there's the built-in `help` function in Python.\n",
    "\n",
    "---\n",
    "\n",
    "`>>> help(vamb)`\n",
    "    \n",
    "    Help on package vamb:\n",
    "\n",
    "    NAME\n",
    "        vamb - Vamb - Variational Autoencoder for Metagenomic Binning\n",
    "\n",
    "    DESCRIPTION\n",
    "        Vamb does what it says on the tin - bins metagenomes using a variational autoencoder.\n",
    "        \n",
    "    [ ... ]\n",
    "    \n",
    "---\n",
    "    \n",
    "The `PACKAGE CONTENTS` is just a list of all importable files in the `vamb` directory - some of these really shouldn't be imported, so ignore that.\n",
    "\n",
    "---\n",
    "You can also get help for the modules:\n",
    "\n",
    "`>>> help(vamb.cluster)`\n",
    "\n",
    "    Help on module vamb.cluster in vamb:\n",
    "\n",
    "    NAME\n",
    "        vamb.cluster - Iterative medoid clustering of Numpy arrays.\n",
    "\n",
    "    DESCRIPTION\n",
    "        Implements two core functions: cluster and tandemcluster, along with the helper\n",
    "        functions writeclusters and readclusters.\n",
    "        For all functions in this module, a collection of clusters are represented as\n",
    "        a {clustername, set(elements)} dict.\n",
    "\n",
    "        Clustering algorithm:\n",
    "        [...]\n",
    "        \n",
    "---\n",
    "And for functions:\n",
    "\n",
    "`>>> help(vamb.cluster.tandemcluster)`\n",
    "\n",
    "    Help on function tandemcluster in module vamb.cluster:\n",
    "\n",
    "    tandemcluster(matrix, labels, inner, outer=None, max_steps=15, spearman=False)\n",
    "        Splits the datasets, then clusters each partition before merging\n",
    "        the resulting clusters. This is faster, especially on larger datasets, but\n",
    "        less accurate than normal clustering.\n",
    "\n",
    "        Inputs:\n",
    "            matrix: A (obs x features) Numpy matrix of values\n",
    "            labels: Numpy array with labels for matrix rows. None or 1-D array\n",
    "            inner: Optimal medoid search within this distance from medoid\n",
    "            outer: Radius of clusters extracted from medoid. If None, same as inner\n",
    "            max_steps: Stop searching for optimal medoid after N futile attempts\n",
    "            spearman: Use Spearman, not Pearson correlation\n",
    "\n",
    "        Output: {medoid: set(labels_in_cluster) dictionary}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple workflow example\n",
    "\n",
    "You begin with some some FASTQ files from, say, 6 samples. First you do the following steps:\n",
    "\n",
    "1) Preprocess the reads and check their quality\n",
    "\n",
    "2) Assemble each sample individually OR co-assemble and get the contigs out\n",
    "\n",
    "3) Concatenate the FASTA files together while making sure all contig headers stay unique\n",
    "\n",
    "Now, like other metagenomic binners of contigs, Vamb relies on two properties of the contigs: The abundance of the contigs in each sample and the kmer-composition of the contigs. The observed values for both of these measures become uncertain when the contig is too small, so you need to filter the small contigs away:\n",
    "\n",
    "4) Remove all small contigs from the FASTA file (say, less than 2000 bp in length)\n",
    "\n",
    "To estimate the abundance of the contigs, you need to map the reads from each sample to the FASTA file. When using BWA, don't filter for unproperly paired reads or minimum alignment score.\n",
    "\n",
    "5) Map the reads to the FASTA file to obtain 6 .bam files\n",
    "___\n",
    "\n",
    "This gives us the following results, here put in `/home/jakni/Downloads/example`:\n",
    "\n",
    "* `contigs.fna` - The filtered FASTA contigs which were mapped against, and\n",
    "* `bamfiles/*.bam` - The 6 .bam files from mapping the reads to the contigs above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jakni/Documents/scripts/')\n",
    "import vamb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, parse the FASTA file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function read_contigs in module vamb.parsecontigs:\n",
      "\n",
      "read_contigs(contigpath, minlength=2000)\n",
      "    Parses a FASTA file and produces a list of headers and a matrix of TNF.\n",
      "    \n",
      "    Input:\n",
      "        contigpath: Path to a FASTA file with contigs\n",
      "        min_length[2000]: Minimum length of contigs\n",
      "    \n",
      "    Outputs:\n",
      "        contignames: A list of contig headers\n",
      "        tnfs: A (n_FASTA_entries x 136) matrix of tetranucleotide freq.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(vamb.parsecontigs.read_contigs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "contigpath = '/home/jakni/Downloads/example/contigs.fna'\n",
    "\n",
    "# Open the file in binary mode - you can use the vamb.vambtools.Reader to read\n",
    "# from normal or gzipped file seamlessly\n",
    "with open(contigpath, 'rb') as filehandle:\n",
    "    tnfs, contignames, lengths = vamb.parsecontigs.read_contigs(filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of tnfs: <class 'numpy.ndarray'> of dtype float32\n",
      "Shape of tnfs: (39551, 136)\n",
      "\n",
      "Type of contignames: <class 'list'>\n",
      "Length of contignames: 39551\n",
      "\n",
      "First 10 elements of contignames:\n",
      "s30_NODE_1_length_245508_cov_18.4904\n",
      "s30_NODE_2_length_222690_cov_39.7685\n",
      "s30_NODE_3_length_222459_cov_20.3665\n",
      "s30_NODE_4_length_173155_cov_20.1181\n",
      "s30_NODE_5_length_161239_cov_20.1237\n",
      "s30_NODE_6_length_157102_cov_20.734\n",
      "s30_NODE_7_length_156768_cov_44.8078\n",
      "s30_NODE_8_length_152691_cov_19.6759\n",
      "s30_NODE_9_length_121154_cov_21.6491\n",
      "s30_NODE_10_length_119726_cov_136.834\n"
     ]
    }
   ],
   "source": [
    "print('Type of tnfs:', type(tnfs), 'of dtype', tnfs.dtype)\n",
    "print('Shape of tnfs:', tnfs.shape, end='\\n\\n')\n",
    "\n",
    "print('Type of contignames:', type(contignames))\n",
    "print('Length of contignames:', len(contignames), end='\\n\\n')\n",
    "\n",
    "print('First 10 elements of contignames:')\n",
    "for i in range(10):\n",
    "    print(contignames[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The tnfs is the tetranucleotide frequency - it's the frequency of the canonical kmer of each 4mer in the contig. The matrix is normalized across samples such that the frequency of e.g. 'AGGC' is measured relative to other contigs.\n",
    "\n",
    "Here, you should probably consider whether you can keep everything in memory. If not, most module have reading and writing methods so you can dump the results and delete them from memory. This is a small dataset, so there's no problem. With hundreds of samples and millions of contigs, this becomes a problem, even though Vamb is fairly memory-friendly\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the BAM files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function read_bamfiles in module vamb.parsebam:\n",
      "\n",
      "read_bamfiles(paths, minscore=50, minlength=2000, processors=4)\n",
      "    Spawns processes to parse BAM files and get contig rpkms.\n",
      "    \n",
      "    Input:\n",
      "        path: Path to BAM file\n",
      "        minscore [50]: Minimum alignment score (AS field) to consider\n",
      "        minlength [2000]: Discard any references shorter than N bases \n",
      "        processors [all]: Number of processes to spawn\n",
      "    \n",
      "    Outputs:\n",
      "        sample_rpkms: A {path: Numpy-32-float-RPKM} dictionary\n",
      "        contignames: A list of contignames from first BAM header\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(vamb.parsebam.read_bamfiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We can see that the function detects 4 CPUs on my laptop. That means we can read 4 BAM files at a time. Each BAM file being read takes up some memory, but unless you have a machine with tonnes of CPUs and little RAM, that's not going to be an issue. In either case, it's probably going to be IO bound when we give it 8+ cores, so whatever.\n",
    "\n",
    "This step is not super optimized, but the largest file is just 1.8 GB, so it takes a few minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "bamfiles = !ls /home/jakni/Downloads/example/bamfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jakni/Downloads/example/bamfiles/e101.filtered.bam',\n",
       " '/home/jakni/Downloads/example/bamfiles/e178.filtered.bam',\n",
       " '/home/jakni/Downloads/example/bamfiles/e179.filtered.bam',\n",
       " '/home/jakni/Downloads/example/bamfiles/e196.filtered.bam',\n",
       " '/home/jakni/Downloads/example/bamfiles/e198.filtered.bam',\n",
       " '/home/jakni/Downloads/example/bamfiles/e30.filtered.bam']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bamfiles = ['/home/jakni/Downloads/example/bamfiles/' + p for p in bamfiles]\n",
    "bamfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That looks right.\n",
    "\n",
    "# We already have the contignames, so who cares about saving that\n",
    "sample_rpkms, _ = vamb.parsebam.read_bamfiles(bamfiles)\n",
    "del _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of sample_rpkms: <class 'dict'>\n",
      "Content of the dict:\n",
      "First key: /home/jakni/Downloads/example/bamfiles/e101.filtered.bam\n",
      "First value: [0.11535063 0.17579393 0.58409214 ... 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print('Type of sample_rpkms:', type(sample_rpkms))\n",
    "\n",
    "print('Content of the dict:')\n",
    "print('First key:', next(iter(sample_rpkms.keys())))\n",
    "print('First value:', next(iter(sample_rpkms.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "This is essentially the depths (RPKMS) table, here in the form of a dict.\n",
    "\n",
    "We need to convert this to a Numpy array to feed it to the VAE\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function array_from_sample_rpkms in module vamb.parsebam:\n",
      "\n",
      "array_from_sample_rpkms(sample_rpkms, columns)\n",
      "    Creates a (n-contigs x n-bamfiles) array from a sample_rpkms\n",
      "    (expected to be a {path: Numpy-32-float-RPKM} dictionary)\n",
      "    \n",
      "    Inputs: \n",
      "        sample_rpkms: A {path: Numpy-32-float-RPKM} dictionary\n",
      "        columns: Names of BAM file paths in sample_rpkms object in correct order\n",
      "        \n",
      "    Output: A (n-contigs x n-bamfiles) array with each column being the corre-\n",
      "    sponding array from sample_rpkms, normalized so each row sums to 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(vamb.parsebam.array_from_sample_rpkms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpkms = vamb.parsebam.array_from_sample_rpkms(sample_rpkms, bamfiles)\n",
    "\n",
    "# Now we're keeping the depths (RPKMS) twice in memory - we could delete\n",
    "# the dict if we wanted, but again, this dataset it so small it hardly matters.\n",
    "\n",
    "# The RAM consumption of either is approximately 4 * n_samples * n_contigs bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we need to train the autoencoder\n",
    "\n",
    "# ADD THIS PART WHEN I'VE ADDED THE VAE TO THE API\n",
    "\n",
    "here I just load the latent from disk - this should be in-memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = pd.read_csv('/home/jakni/Downloads/binningexample/latent.tsv', delimiter='\\t', dtype=np.float32, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = latent.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining the clustering threshold\n",
    "\n",
    "# Also add this part when it's stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering the latent representation\n",
    "\n",
    "There's two clustering algorithms: A `vamb.cluster.cluster`, an accurate one which scales badly with large datasets (up to one or two million contigs is alright), and `vamb.cluster.tandemcluster` less accurate which scales better.\n",
    "\n",
    "The heavy lifting here is done in Numpy, so it might be worth making sure the BLAS library your Numpy is using is fast.\n",
    "\n",
    "I have 40k contigs, so I'm obviously going for the slow but accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function cluster in module vamb.cluster:\n",
      "\n",
      "cluster(matrix, labels, inner, outer=None, max_steps=15, spearman=False)\n",
      "    Iterative medoid cluster generator. Yields (medoid), set(labels) pairs.\n",
      "    \n",
      "    Inputs:\n",
      "        matrix: A (obs x features) Numpy matrix of values\n",
      "        labels: Numpy array with labels for matrix rows. None or 1-D array\n",
      "        inner: Optimal medoid search within this distance from medoid\n",
      "        outer: Radius of clusters extracted from medoid. If None, same as inner\n",
      "        max_steps: Stop searching for optimal medoid after N futile attempts\n",
      "        spearman: Use Spearman, not Pearson correlation\n",
      "    \n",
      "    Output: Generator of (medoid, set(labels_in_cluster)) tuples.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(vamb.cluster.cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(contignames)\n",
    "cluster_iterator = vamb.cluster.cluster(latent, labels, threshold)\n",
    "\n",
    "clusters = dict()\n",
    "for medoid, contigs in cluster_iterator:\n",
    "    clusters[medoid] = contigs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing the clusters\n",
    "\n",
    "This is not automatic, because it really depends on what you're looking for.\n",
    "\n",
    "One of the greatest weaknesses of Vamb is that the bins tend to be highly fragmented. You'll have lots of tiny bins, some of which are legitimate (viruses, plasmids), but most are parts of larger genomes that didn't get binned properly.\n",
    "\n",
    "Here, let's say we're only interested in bacteria. So we throw away all bins with less than 250,000 basepairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's make a contignames: length dict\n",
    "lengthof = {name:length for name, length in zip(contignames, lengths)}\n",
    "\n",
    "# Now filter away the small bins\n",
    "filtered_bins = dict()\n",
    "\n",
    "for medoid, contigs in clusters.items():\n",
    "    binsize = sum(lengthof[contig] for contig in contigs)\n",
    "    \n",
    "    if binsize >= 250000:\n",
    "        filtered_bins[medoid] = contigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bins before filtering: 1520\n",
      "Number of bins after filtering: 86\n"
     ]
    }
   ],
   "source": [
    "print('Number of bins before filtering:', len(clusters))\n",
    "print('Number of bins after filtering:', len(filtered_bins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (If you have a reference: Benchmark Vamb)\n",
    "\n",
    "For this to make any sense, you need to have a *reference*, that is, a list of bins that are deemed true and complete.\n",
    "\n",
    "The reference could be a {clustername: set(contigs)} dict along with a {contigname: length} dict, just like the `clusters` and `lengthof` we made. It could also be a tab-separated file with clustername, contigname, length rows, one row per contig.\n",
    "\n",
    "Now, I have no reference for this dataset, so I created a reference file completely randomly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# binname contigname length\r\n",
      "0\ts198_NODE_2960_length_5085_cov_5.30505\t5085\r\n",
      "0\ts30_NODE_9489_length_2530_cov_2.23365\t2530\r\n",
      "0\ts179_NODE_2638_length_5642_cov_2.52661\t5642\r\n",
      "0\ts30_NODE_160_length_42890_cov_12.914\t42890\r\n",
      "0\ts198_NODE_4819_length_3620_cov_4.35672\t3620\r\n",
      "0\ts178_NODE_2065_length_4779_cov_4.00513\t4779\r\n",
      "0\ts198_NODE_1167_length_8851_cov_6.04376\t8851\r\n",
      "0\ts198_NODE_7205_length_2698_cov_5.10081\t2698\r\n",
      "0\ts198_NODE_5233_length_3401_cov_5.00303\t3401\r\n"
     ]
    }
   ],
   "source": [
    "!head /home/jakni/Downloads/example/reference.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have no reference for this dataset, so I just made a completely random one\n",
    "# since any correct bins according to this reference would be entirely incidental\n",
    "# the benchmark will probably show zero good bins.\n",
    "reference_path = '/home/jakni/Downloads/example/reference.tsv'\n",
    "\n",
    "with open(reference_path) as filehandle:\n",
    "    reference = vamb.benchmark.Reference.fromfile(filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We also need to instantiate the Observed bins (which we created above!), and a BenchMarkResult\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed = vamb.benchmark.Observed(clusters, reference)\n",
    "result = vamb.benchmark.BenchMarkResult(reference=reference, observed=observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tRecall\n",
      "Prec.\t0.3\t0.4\t0.5\t0.6\t0.7\t0.8\t0.9\t0.95\n",
      "0.7\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "0.8\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "0.9\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "0.95\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "0.99\t0\t0\t0\t0\t0\t0\t0\t0\n"
     ]
    }
   ],
   "source": [
    "# Okay, how did we do?\n",
    "result.printmatrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "As expected (because the reference was randomly generated), the results are terrible - in fact, they couldn't be worse.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now what?\n",
    "\n",
    "Here's some ideas:\n",
    "\n",
    "__Quality control your bins with CheckM__\n",
    "\n",
    "CheckM tries to asses the contimation and completeness of the given bins. It also tries to classify them... with more limited success\n",
    "\n",
    "__Improve the bins with Stranglerfig or RefineM__\n",
    "\n",
    "These tools inspect your bins and refines them by reassigning contigs between bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
