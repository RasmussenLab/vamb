{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To do:\n",
    "\n",
    "Add a \"labels\" to clusters function\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Precluster using Wikipedia's Canopy to break into smaller pieces, then cluster.\n",
    "\n",
    "Save all unclustered points (i.e. also the ones in clusters lower than the minsize threshold)\n",
    "\n",
    "Maybe first check if any of sklearns clustering algos work better than the it. medoid in here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "__doc__ = \"\"\"Iterative medoid clustering.\n",
    "\n",
    "Input: An observations x features matrix in text format.\n",
    "\n",
    "Output: Tab-sep lines with clustername, observation(1-indexed).\n",
    "\n",
    "Algorithm:\n",
    "(1): Pick random seed observation S\n",
    "(2): Define inner_obs(S) = all observations with Pearson distance from S < INNER\n",
    "(3): Sample MOVES observations I from inner_obs\n",
    "(4): If any inner_obs(i) > inner_obs(S) for i in I: Let S be i, go to (2)\n",
    "     Else: Outer_obs(S) = all observations with Pearson distance from S < OUTER\n",
    "(5): Output outer_obs(S) as cluster, remove inner_obs(S) from observations\n",
    "(6): If no more observations or MAX_CLUSTERS have been reached: Stop\n",
    "     Else: Go to (1)\n",
    "\"\"\"\n",
    "\n",
    "import sys as _sys\n",
    "import os as _os\n",
    "import argparse as _argparse\n",
    "import numpy as _np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _distances_to_vector(matrix, index):\n",
    "    \n",
    "    # Distance D = (P - 1) / -2, where P is Pearson correlation coefficient.\n",
    "    # For two vectors x and y with numbers xi and yi,\n",
    "    # P = sum((xi-x_mean)*(yi-y_mean)) / (std(y) * std(x) * len(x)).\n",
    "    # If we normalize matrix so x_mean = y_mean = 0 and std(x) = std(y) = 1,\n",
    "    # this reduces to sum(xi*yi) / len(x) = x @ y.T / len(x) =>\n",
    "    # D = ((x @ y.T) / len(x)) - 1) / -2 =>\n",
    "    # D = (x @ y.T - len(x)) * (-1 / 2len(x))\n",
    "    \n",
    "    # Matrix should have already been zscore normalized by axis 1 (subtract mean, div by std)\n",
    "    vectorlength = matrix.shape[1]\n",
    "    result = _np.dot(matrix, matrix[index].T)\n",
    "    result -= vectorlength\n",
    "    result *= -1 / (2 * vectorlength)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _getinner(matrix, point, inner_threshold):\n",
    "    \"\"\"Gets the distance vector, array of inner points and average distance\n",
    "    to inner points from a starting point\"\"\"\n",
    "    \n",
    "    distances = _distances_to_vector(matrix, point)\n",
    "    inner_points = _np.where(distances < inner_threshold)[0]\n",
    "    \n",
    "    if len(inner_points) == 1:\n",
    "        average_distance = 0\n",
    "    else:\n",
    "        average_distance = _np.sum(distances[inner_points]) / (len(inner_points) - 1)\n",
    "\n",
    "    return distances, inner_points, average_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sample_clusters(matrix, point, max_attempts, inner_threshold, outer_threshold):\n",
    "    \"\"\"Keeps sampling new points within the inner points until it has sampled\n",
    "    max_attempts without getting a new set of inner points with lower average\n",
    "    distance\"\"\"\n",
    "    \n",
    "    futile_attempts = 0\n",
    "    \n",
    "    # Keep track of tried points to avoid sampling the same more than once\n",
    "    tried = {point}\n",
    "    \n",
    "    distances, inner_points, average_distance = _getinner(matrix, point, inner_threshold)\n",
    "    \n",
    "    while len(inner_points) - len(tried) > 0 and futile_attempts < max_attempts:\n",
    "        sample = _np.random.choice(inner_points)\n",
    "        while sample in tried: # Not sure there is a faster way to prevent resampling\n",
    "            sample = _np.random.choice(inner_points)\n",
    "            \n",
    "        tried.add(sample)\n",
    "        \n",
    "        inner = _getinner(matrix, sample, inner_threshold)\n",
    "        sample_dist, sample_inner, sample_average =  inner\n",
    "        \n",
    "        if sample_average < average_distance:\n",
    "            point = sample\n",
    "            inner_points = sample_inner\n",
    "            average_distance = sample_average\n",
    "            distances = sample_dist\n",
    "            futile_attempts = 0\n",
    "            tried = {point}\n",
    "            \n",
    "        else:\n",
    "            futile_attempts += 1\n",
    "            \n",
    "    outer_points = _np.where(distances < outer_threshold)[0]\n",
    "    \n",
    "    return point, inner_points, outer_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusters(matrix, inner_threshold, outer_threshold=None, labels=None,\n",
    "             max_steps=15, min_size=5):\n",
    "    \"\"\"Yields (medoid, points) pairs from a (obs x features) matrix\n",
    "    \n",
    "    Inputs:\n",
    "        matrix: A (obs x features) Numpy matrix of values\n",
    "        inner_threshold: Optimal medoid search within this distance from medoid\n",
    "        outer_threshold [inner_threshold]: Radius of clusters extracted from medoid\n",
    "        labels [np.arange(len(matrix))]: Numpy array with labels for matrix rows \n",
    "        max_steps [15]: Stop searching for optimal medoid after N futile attempts\n",
    "        min_size [5]: Don't output clusters with fewer than N elements\n",
    "        \n",
    "    Output: A generator yielding (medoid_0_indexed, points_0_indexed) pairs\n",
    "    \"\"\"\n",
    "    \n",
    "    if outer_threshold is None:\n",
    "        outer_threshold = inner_threshold\n",
    "        \n",
    "    # This is to keep track of the original order of the points, even when we\n",
    "    # remove points as the clustering proceeds.\n",
    "    if labels is None:\n",
    "        labels = _np.arange(len(matrix))\n",
    "    elif type(labels) != _np.ndarray or len(labels) != len(matrix):\n",
    "        raise ValueError('labels must be a 1D Numpy array with same length as matrix')\n",
    "    \n",
    "    _np.random.seed(324645) # Reproducability even when it's random.\n",
    "    \n",
    "    # This list keeps track of points to remove because they compose the inner circle\n",
    "    # of clusters. We only remove points when we create a cluster with more than one\n",
    "    # point, since one-point-clusters don't interfere with other clusters and the\n",
    "    # point removal operations are expensive.\n",
    "    toremove = list()\n",
    "    \n",
    "    # This index keeps track of which point we initialize clusters from.\n",
    "    # It's necessary since we don't remove points after every cluster.\n",
    "    seed_index = 0\n",
    "    \n",
    "    # Normalize - this simplifies calculating the Pearson distance\n",
    "    # and boosts speed tremendously\n",
    "    matrix = matrix - matrix.mean(axis=1).reshape((len(matrix), 1))\n",
    "    std = _np.std(matrix, axis=1).reshape((len(matrix), 1))\n",
    "    std[std == 0] = 1\n",
    "    matrix /= std\n",
    "    del std\n",
    "    \n",
    "    # We initialize clusters from most extreme to less extreme. This is\n",
    "    # arbitrary and just to have some kind of reproducability.\n",
    "    # Note to Simon: Sorting by means makes no sense with normalized rows.\n",
    "    extremes = _np.max(matrix, axis=1)\n",
    "    argextremes = _np.argsort(extremes)\n",
    "    \n",
    "    clusters_completed = 0\n",
    "    \n",
    "    while len(matrix) > 0:           \n",
    "        # Most extreme point (without picking same point twice)\n",
    "        seed = argextremes[-seed_index -1]\n",
    "        \n",
    "        # Find medoid using iterative sampling function above\n",
    "        sampling = _sample_clusters(matrix, seed, max_steps, inner_threshold, outer_threshold)\n",
    "        medoid, inner_points, outer_points = sampling\n",
    "        \n",
    "        # Write data to output if the cluster is not too small\n",
    "        if len(outer_points) >= min_size:\n",
    "            yield labels[medoid], labels[outer_points]\n",
    "            clusters_completed += 1\n",
    "            \n",
    "        seed_index += 1\n",
    "        \n",
    "        for point in inner_points:\n",
    "            toremove.append(point)\n",
    "\n",
    "        # Only remove points if we have more than 1 point in cluster\n",
    "        # Note that these operations are really expensive.\n",
    "        if len(inner_points) > 1 or len(argextremes) == seed_index:\n",
    "            matrix = _np.delete(matrix, toremove, 0)\n",
    "            labels = _np.delete(labels, toremove, 0)\n",
    "            extremes = _np.delete(extremes, toremove, 0)\n",
    "            argextremes = _np.argsort(extremes)\n",
    "            seed_index = 0\n",
    "            toremove.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeclusters(filehandle, clusters, contignames=None):\n",
    "    \"\"\"Writes clusters to an open filehandle.\n",
    "    \n",
    "    Inputs:\n",
    "        clusters: An iterator generated by function `clusters`\n",
    "        filehandle: An open filehandle that can be written to.\n",
    "        contignames: None or an indexable array of contignames.\n",
    "        \n",
    "    Output: None\n",
    "    \"\"\"\n",
    "    \n",
    "    if not hasattr(filehandle, 'writable') or not filehandle.writable():\n",
    "        raise ValueError('Filehandle must be a writable file')\n",
    "    \n",
    "    if contignames is None:\n",
    "        print('#clustername', 'contigindex(1-indexed)', sep='\\t', file=filehandle)\n",
    "    else:\n",
    "        print('#clustername', 'contigheader', sep='\\t', file=filehandle)\n",
    "        \n",
    "    for clusternumber, (medoid, cluster) in enumerate(clusters):\n",
    "        clustername = 'cluster_' + str(clusternumber + 1)\n",
    "\n",
    "        for contigindex in cluster:\n",
    "            if contignames is None:\n",
    "                contigname = contigindex + 1\n",
    "            else:\n",
    "                contigname = contignames[contigindex]\n",
    "            \n",
    "            print(clustername, contigname, sep='\\t', file=filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: medoid_clustering.py [options]\n",
      "medoid_clustering.py: error: the following arguments are required: output\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    usage = \"python cluster.py [OPTIONS ...] INPUT OUTPUT\"\n",
    "    parser = _argparse.ArgumentParser(\n",
    "        description=__doc__,\n",
    "        formatter_class=_argparse.RawDescriptionHelpFormatter,\n",
    "        usage=usage)\n",
    "   \n",
    "    # create the parser\n",
    "    parser.add_argument('input', help='input dataset')\n",
    "    parser.add_argument('output', help='output clusters')\n",
    "    parser.add_argument('-i', dest='inner', help='inner threshold [0.08]', default=0.08, type=float)\n",
    "    parser.add_argument('-o', dest='outer', help='outer threshold [0.10]', default=0.10, type=float)\n",
    "    parser.add_argument('-c', dest='max_clusters', help='max seeds before exiting [30000]', default=30000, type=int)\n",
    "    parser.add_argument('-m', dest='moves', help='Moves before cluster is stable [15]', default=15, type=int)\n",
    "    parser.add_argument('-s', dest='min_size', help='Minimum cluster size to output [1]', default=1, type=int)\n",
    "    \n",
    "    # Print help if no arguments are given\n",
    "    if len(_sys.argv) == 1:\n",
    "        parser.print_help()\n",
    "        _sys.exit()\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    if not _os.path.isfile(args.input):\n",
    "        raise FileNotFoundError(args.input)\n",
    "    \n",
    "    if _os.path.isfile(args.output):\n",
    "        raise FileExistsError(args.output)\n",
    "   \n",
    "    matrix = _np.loadtxt(args.input, delimiter='\\t', dtype=_np.float32)\n",
    "    \n",
    "    clusters = cluster(matrix, args.inner, args.outer, args.max_clusters, \n",
    "                       args.moves, args.min_size)\n",
    "    \n",
    "    with open(args.output, 'w') as filehandle:\n",
    "        writeclusters(filehandle, clusters, contignames=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
