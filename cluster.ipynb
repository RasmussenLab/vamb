{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To do:\n",
    "\n",
<<<<<<< HEAD
    "Add a \"labels\" to clusters function\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Precluster using Wikipedia's Canopy to break into smaller pieces, then cluster.\n",
    "\n",
    "Save all unclustered points (i.e. also the ones in clusters lower than the minsize threshold)\n",
    "\n",
    "Maybe first check if any of sklearns clustering algos work better than the it. medoid in here."
=======
    "changelog: Instead of having a clusters iterator, now has a subcluster function and cluster iterator\n",
    "now has its own random state instead of sharing state with general numpy\n",
    "now can return clusters with contigs by name"
>>>>>>> subcluster
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm:\n",
    "# (1): Pick random seed observation S\n",
    "# (2): Define inner_obs(S) = all observations with Pearson distance from S < INNER\n",
    "# (3): Sample MOVES observations I from inner_obs\n",
    "# (4): If any inner_obs(i) > inner_obs(S) for i in I: Let S be i, go to (2)\n",
    "#      Else: Outer_obs(S) = all observations with Pearson distance from S < OUTER\n",
    "# (5): Output outer_obs(S) as cluster, remove inner_obs(S) from observations\n",
    "# (6): If no more observations or MAX_CLUSTERS have been reached: Stop\n",
    "#      Else: Go to (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
>>>>>>> subcluster
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "__doc__ = \"\"\"Iterative medoid clustering.\n",
    "\n",
    "Input: An observations x features matrix in text format.\n",
    "\n",
    "Output: Tab-sep lines with clustername, observation(1-indexed).\n",
<<<<<<< HEAD
    "\n",
    "Algorithm:\n",
    "(1): Pick random seed observation S\n",
    "(2): Define inner_obs(S) = all observations with Pearson distance from S < INNER\n",
    "(3): Sample MOVES observations I from inner_obs\n",
    "(4): If any inner_obs(i) > inner_obs(S) for i in I: Let S be i, go to (2)\n",
    "     Else: Outer_obs(S) = all observations with Pearson distance from S < OUTER\n",
    "(5): Output outer_obs(S) as cluster, remove inner_obs(S) from observations\n",
    "(6): If no more observations or MAX_CLUSTERS have been reached: Stop\n",
    "     Else: Go to (1)\n",
=======
>>>>>>> subcluster
    "\"\"\"\n",
    "\n",
    "import sys as _sys\n",
    "import os as _os\n",
    "import argparse as _argparse\n",
<<<<<<< HEAD
    "import numpy as _np"
=======
    "import numpy as _np\n",
    "from collections import defaultdict as _defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _checklabels(labels, length):\n",
    "    \"\"\"If labels are None, returns an array of integers 1 to length.\n",
    "    Else checks that labels are already of correct length and unique\"\"\"\n",
    "    \n",
    "    if labels is None:\n",
    "        labels = _np.arange(length) + 1\n",
    "        \n",
    "    elif type(labels) != _np.ndarray or len(labels) != length:\n",
    "        raise ValueError('labels must be a 1D Numpy array with same length as matrix')\n",
    "        \n",
    "    if len(set(labels)) != length:\n",
    "        raise ValueError('Labels must be unique')\n",
    "        \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize(matrix):\n",
    "    \"\"\"Returns a new, z-score normalized matrix across axis 1\"\"\"\n",
    "    \n",
    "    normalized = matrix - matrix.mean(axis=1).reshape((len(matrix), 1))\n",
    "    std = _np.std(normalized, axis=1).reshape((len(normalized), 1))\n",
    "    std[std == 0] = 1\n",
    "    normalized /= std\n",
    "    \n",
    "    return normalized"
>>>>>>> subcluster
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 19,
>>>>>>> subcluster
   "metadata": {},
   "outputs": [],
   "source": [
    "def _distances_to_vector(matrix, index):\n",
<<<<<<< HEAD
=======
    "    \"\"\"Calculates the Pearson distances from row `index` to all rows\n",
    "    in the matrix, including itself. Returns numpy array of distances\"\"\"\n",
>>>>>>> subcluster
    "    \n",
    "    # Distance D = (P - 1) / -2, where P is Pearson correlation coefficient.\n",
    "    # For two vectors x and y with numbers xi and yi,\n",
    "    # P = sum((xi-x_mean)*(yi-y_mean)) / (std(y) * std(x) * len(x)).\n",
    "    # If we normalize matrix so x_mean = y_mean = 0 and std(x) = std(y) = 1,\n",
    "    # this reduces to sum(xi*yi) / len(x) = x @ y.T / len(x) =>\n",
    "    # D = ((x @ y.T) / len(x)) - 1) / -2 =>\n",
    "    # D = (x @ y.T - len(x)) * (-1 / 2len(x))\n",
    "    \n",
    "    # Matrix should have already been zscore normalized by axis 1 (subtract mean, div by std)\n",
    "    vectorlength = matrix.shape[1]\n",
    "    result = _np.dot(matrix, matrix[index].T)\n",
    "    result -= vectorlength\n",
    "    result *= -1 / (2 * vectorlength)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 20,
>>>>>>> subcluster
   "metadata": {},
   "outputs": [],
   "source": [
    "def _getinner(matrix, point, inner_threshold):\n",
    "    \"\"\"Gets the distance vector, array of inner points and average distance\n",
    "    to inner points from a starting point\"\"\"\n",
    "    \n",
    "    distances = _distances_to_vector(matrix, point)\n",
    "    inner_points = _np.where(distances < inner_threshold)[0]\n",
    "    \n",
    "    if len(inner_points) == 1:\n",
    "        average_distance = 0\n",
    "    else:\n",
    "        average_distance = _np.sum(distances[inner_points]) / (len(inner_points) - 1)\n",
    "\n",
    "    return distances, inner_points, average_distance"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sample_clusters(matrix, point, max_attempts, inner_threshold, outer_threshold):\n",
=======
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sample_clusters(matrix, point, max_attempts, inner_threshold, outer_threshold, randomstate):\n",
>>>>>>> subcluster
    "    \"\"\"Keeps sampling new points within the inner points until it has sampled\n",
    "    max_attempts without getting a new set of inner points with lower average\n",
    "    distance\"\"\"\n",
    "    \n",
    "    futile_attempts = 0\n",
    "    \n",
    "    # Keep track of tried points to avoid sampling the same more than once\n",
    "    tried = {point}\n",
    "    \n",
    "    distances, inner_points, average_distance = _getinner(matrix, point, inner_threshold)\n",
    "    \n",
    "    while len(inner_points) - len(tried) > 0 and futile_attempts < max_attempts:\n",
<<<<<<< HEAD
    "        sample = _np.random.choice(inner_points)\n",
    "        while sample in tried: # Not sure there is a faster way to prevent resampling\n",
    "            sample = _np.random.choice(inner_points)\n",
=======
    "        sample = randomstate.choice(inner_points)\n",
    "        while sample in tried: # Not sure there is a faster way to prevent resampling\n",
    "            sample = randomstate.choice(inner_points)\n",
>>>>>>> subcluster
    "            \n",
    "        tried.add(sample)\n",
    "        \n",
    "        inner = _getinner(matrix, sample, inner_threshold)\n",
    "        sample_dist, sample_inner, sample_average =  inner\n",
    "        \n",
    "        if sample_average < average_distance:\n",
    "            point = sample\n",
    "            inner_points = sample_inner\n",
    "            average_distance = sample_average\n",
    "            distances = sample_dist\n",
    "            futile_attempts = 0\n",
    "            tried = {point}\n",
    "            \n",
    "        else:\n",
    "            futile_attempts += 1\n",
    "            \n",
    "    outer_points = _np.where(distances < outer_threshold)[0]\n",
    "    \n",
    "    return point, inner_points, outer_points"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusters(matrix, inner_threshold, outer_threshold=None, labels=None,\n",
    "             max_steps=15, min_size=5):\n",
    "    \"\"\"Yields (medoid, points) pairs from a (obs x features) matrix\n",
    "    \n",
    "    Inputs:\n",
    "        matrix: A (obs x features) Numpy matrix of values\n",
    "        inner_threshold: Optimal medoid search within this distance from medoid\n",
    "        outer_threshold [inner_threshold]: Radius of clusters extracted from medoid\n",
    "        labels [np.arange(len(matrix))]: Numpy array with labels for matrix rows \n",
    "        max_steps [15]: Stop searching for optimal medoid after N futile attempts\n",
    "        min_size [5]: Don't output clusters with fewer than N elements\n",
    "        \n",
    "    Output: A generator yielding (medoid_0_indexed, points_0_indexed) pairs\n",
    "    \"\"\"\n",
    "    \n",
    "    if outer_threshold is None:\n",
    "        outer_threshold = inner_threshold\n",
    "        \n",
    "    # This is to keep track of the original order of the points, even when we\n",
    "    # remove points as the clustering proceeds.\n",
    "    if labels is None:\n",
    "        labels = _np.arange(len(matrix))\n",
    "    elif type(labels) != _np.ndarray or len(labels) != len(matrix):\n",
    "        raise ValueError('labels must be a 1D Numpy array with same length as matrix')\n",
    "    \n",
    "    _np.random.seed(324645) # Reproducability even when it's random.\n",
=======
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cluster(matrix, labels, inner_threshold, outer_threshold, max_steps):\n",
    "    \"\"\"Yields (medoid, points) pairs from a (obs x features) matrix\"\"\"\n",
    "\n",
    "    randomstate = _np.random.RandomState(324645)\n",
>>>>>>> subcluster
    "    \n",
    "    # This list keeps track of points to remove because they compose the inner circle\n",
    "    # of clusters. We only remove points when we create a cluster with more than one\n",
    "    # point, since one-point-clusters don't interfere with other clusters and the\n",
    "    # point removal operations are expensive.\n",
    "    toremove = list()\n",
    "    \n",
    "    # This index keeps track of which point we initialize clusters from.\n",
    "    # It's necessary since we don't remove points after every cluster.\n",
    "    seed_index = 0\n",
    "    \n",
<<<<<<< HEAD
    "    # Normalize - this simplifies calculating the Pearson distance\n",
    "    # and boosts speed tremendously\n",
    "    matrix = matrix - matrix.mean(axis=1).reshape((len(matrix), 1))\n",
    "    std = _np.std(matrix, axis=1).reshape((len(matrix), 1))\n",
    "    std[std == 0] = 1\n",
    "    matrix /= std\n",
    "    del std\n",
    "    \n",
=======
>>>>>>> subcluster
    "    # We initialize clusters from most extreme to less extreme. This is\n",
    "    # arbitrary and just to have some kind of reproducability.\n",
    "    # Note to Simon: Sorting by means makes no sense with normalized rows.\n",
    "    extremes = _np.max(matrix, axis=1)\n",
    "    argextremes = _np.argsort(extremes)\n",
    "    \n",
<<<<<<< HEAD
    "    clusters_completed = 0\n",
    "    \n",
=======
>>>>>>> subcluster
    "    while len(matrix) > 0:           \n",
    "        # Most extreme point (without picking same point twice)\n",
    "        seed = argextremes[-seed_index -1]\n",
    "        \n",
    "        # Find medoid using iterative sampling function above\n",
<<<<<<< HEAD
    "        sampling = _sample_clusters(matrix, seed, max_steps, inner_threshold, outer_threshold)\n",
    "        medoid, inner_points, outer_points = sampling\n",
    "        \n",
    "        # Write data to output if the cluster is not too small\n",
    "        if len(outer_points) >= min_size:\n",
    "            yield labels[medoid], labels[outer_points]\n",
    "            clusters_completed += 1\n",
=======
    "        sampling = _sample_clusters(matrix, seed, max_steps, inner_threshold, outer_threshold, randomstate)\n",
    "        medoid, inner_points, outer_points = sampling\n",
    "        \n",
    "        # Write data to output\n",
    "        yield labels[medoid], set(labels[outer_points])\n",
>>>>>>> subcluster
    "            \n",
    "        seed_index += 1\n",
    "        \n",
    "        for point in inner_points:\n",
    "            toremove.append(point)\n",
    "\n",
    "        # Only remove points if we have more than 1 point in cluster\n",
    "        # Note that these operations are really expensive.\n",
    "        if len(inner_points) > 1 or len(argextremes) == seed_index:\n",
    "            matrix = _np.delete(matrix, toremove, 0)\n",
    "            labels = _np.delete(labels, toremove, 0)\n",
    "            extremes = _np.delete(extremes, toremove, 0)\n",
    "            argextremes = _np.argsort(extremes)\n",
    "            seed_index = 0\n",
    "            toremove.clear()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeclusters(filehandle, clusters, contignames=None):\n",
    "    \"\"\"Writes clusters to an open filehandle.\n",
    "    \n",
    "    Inputs:\n",
    "        clusters: An iterator generated by function `clusters`\n",
    "        filehandle: An open filehandle that can be written to.\n",
    "        contignames: None or an indexable array of contignames.\n",
=======
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _precluster(matrix, labels, randomstate, nremove=10000, nextract=20000):\n",
    "    \"\"\"Does rough preclustering, splits matrix and labels into multiple,\n",
    "    overlapping matrixes/labels. Uses a version of Canopy clustering:\n",
    "    \n",
    "    1) Pick random seed observation, calculate distances P to all other obs\n",
    "    2) Inner threshold is `nremove`th smallest dist, other is `nextract`th\n",
    "    3) Create new matrix, labels pair for all obs within outer threshold\n",
    "    4) Remove all obs within inner dist from set\n",
    "    5) Continue from 1) until max `nextract` observations are left\n",
    "    \"\"\"\n",
    "    \n",
    "    if nextract < nremove:\n",
    "        raise ValueError('nextract must exceed or be equal to nremove')\n",
    "    \n",
    "    while len(matrix) > nextract:\n",
    "        seed = randomstate.randint(len(matrix))\n",
    "        distances = _distances_to_vector(matrix, seed)\n",
    "        \n",
    "        sorted_distances = _np.sort(distances)\n",
    "        innerdistance = sorted_distances[nremove]\n",
    "        outerdistance = sorted_distances[nextract]\n",
    "        del sorted_distances\n",
    "        \n",
    "        innerindices = _np.where(distances <= innerdistance)[0]\n",
    "        outerindices = _np.where(distances <= outerdistance)[0]\n",
    "        \n",
    "        yield matrix[outerindices], labels[outerindices]\n",
    "        \n",
    "        matrix = _np.delete(matrix, innerindices, 0)\n",
    "        labels = _np.delete(labels, innerindices, 0)\n",
    "    \n",
    "    yield matrix, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mergeclusters(contigsof):\n",
    "    \"\"\"Given a cluster dict, for each obs in multiple clusters, only keep\n",
    "    it in the largest cluster it appears in. Modifies dict in-place.\"\"\"\n",
    "    \n",
    "    clustersof = _defaultdict(list)\n",
    "    \n",
    "    for cluster, contigs in contigsof.items():\n",
    "        for contig in contigs:\n",
    "            clustersof[contig].append(cluster)\n",
    "            \n",
    "    # We don't have to do anything about contigs in only one cluster\n",
    "    clustersof = {co: cl for co, cl in clustersof.items() if len(cl) > 1}\n",
    "    \n",
    "    # Here, we assign each contig to the largest cluster it appears in\n",
    "    while clustersof:\n",
    "        contig, clusters = clustersof.popitem()\n",
    "        largestcluster = max(clusters, key=lambda x: len(contigsof[x]))\n",
    "        \n",
    "        for cluster in clusters:\n",
    "            if cluster is not largestcluster:\n",
    "                contigsof[cluster].remove(contig)\n",
    "                \n",
    "    # Finally, we remove all clusters with less than minclusters members\n",
    "    toremove = [cl for cl, co in contigsof.items() if len(co) == 0]\n",
    "    \n",
    "    for key in toremove:\n",
    "        contigsof.pop(key)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tandemcluster(matrix, labels, inner, outer=None, max_steps=15):\n",
    "    \"\"\"Splits the datasets, then clusters each partition before merging\n",
    "    the resulting clusters. This is faster, especially on larger datasets, but\n",
    "    less accurate than normal clustering.\n",
    "    \n",
    "    Inputs:\n",
    "        matrix: A (obs x features) Numpy matrix of values\n",
    "        labels: Numpy array with labels for matrix rows. None or 1-D array\n",
    "        inner: Optimal medoid search within this distance from medoid\n",
    "        outer: Radius of clusters extracted from medoid. If None, same as inner\n",
    "        max_steps: Stop searching for optimal medoid after N futile attempts\n",
    "    \n",
    "    Output: {medoid: set(labels_in_cluster) dictionary}\n",
    "    \"\"\"\n",
    "    \n",
    "    if outer is None:\n",
    "        outer = inner\n",
    "        \n",
    "    elif outer < inner:\n",
    "        raise ValueError('outer must exceed or be equal to inner')\n",
    "    \n",
    "    labels = _checklabels(labels, len(matrix))\n",
    "    matrix = _normalize(matrix)\n",
    "    \n",
    "    randomstate = _np.random.RandomState(324645) \n",
    "    contigsof = dict()\n",
    "    \n",
    "    for submatrix, sublabels in _precluster(matrix, labels, randomstate, nremove=10000, nextract=20000):\n",
    "        for medoid, cluster in _cluster(submatrix, sublabels, inner, outer, max_steps):\n",
    "            contigsof[medoid] = cluster\n",
    "        \n",
    "    _mergeclusters(contigsof)\n",
    "    \n",
    "    return contigsof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(matrix, labels, inner, outer=None, max_steps=15):\n",
    "    \"\"\"Iterative medoid cluster generator. Yields (medoid), set(labels) pairs.\n",
    "    \n",
    "    Inputs:\n",
    "        matrix: A (obs x features) Numpy matrix of values\n",
    "        labels: Numpy array with labels for matrix rows. None or 1-D array\n",
    "        inner: Optimal medoid search within this distance from medoid\n",
    "        outer: Radius of clusters extracted from medoid. If None, same as inner\n",
    "        max_steps: Stop searching for optimal medoid after N futile attempts\n",
    "    \n",
    "    Output: Generator of (medoid, set(labels_in_cluster)) tuples.\n",
    "    \"\"\"\n",
    "\n",
    "    if outer is None:\n",
    "        outer = inner\n",
    "        \n",
    "    elif outer < inner:\n",
    "        raise ValueError('outer must exceed or be equal to inner')\n",
    "    \n",
    "    labels = _checklabels(labels, len(matrix))\n",
    "    matrix = _normalize(matrix)\n",
    "    \n",
    "    return _cluster(matrix, labels, inner, outer, max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeclusters(filehandle, clusters, max_clusters=None, min_size=1):\n",
    "    \"\"\"Writes clusters to an open filehandle.\n",
    "    \n",
    "    Inputs:\n",
    "        filehandle: An open filehandle that can be written to\n",
    "        clusters: An iterator generated by function `clusters`\n",
    "        min_size: Don't output clusters smaller than N contigs\n",
>>>>>>> subcluster
    "        \n",
    "    Output: None\n",
    "    \"\"\"\n",
    "    \n",
    "    if not hasattr(filehandle, 'writable') or not filehandle.writable():\n",
    "        raise ValueError('Filehandle must be a writable file')\n",
<<<<<<< HEAD
    "    \n",
    "    if contignames is None:\n",
    "        print('#clustername', 'contigindex(1-indexed)', sep='\\t', file=filehandle)\n",
    "    else:\n",
    "        print('#clustername', 'contigheader', sep='\\t', file=filehandle)\n",
    "        \n",
    "    for clusternumber, (medoid, cluster) in enumerate(clusters):\n",
    "        clustername = 'cluster_' + str(clusternumber + 1)\n",
    "\n",
    "        for contigindex in cluster:\n",
    "            if contignames is None:\n",
    "                contigname = contigindex + 1\n",
    "            else:\n",
    "                contigname = contignames[contigindex]\n",
    "            \n",
    "            print(clustername, contigname, sep='\\t', file=filehandle)"
=======
    "        \n",
    "    if iter(clusters) is not clusters:\n",
    "        clusters = clusters.items()\n",
    "    \n",
    "    print('#clustername', 'contignames', sep='\\t', file=filehandle)\n",
    "    \n",
    "    clusternumber = 0\n",
    "    for clustername, contigs in clusters:\n",
    "        if clusternumber == max_clusters:\n",
    "            break\n",
    "        \n",
    "        if len(contigs) < min_size:\n",
    "            continue\n",
    "        \n",
    "        clustername = 'cluster_' + str(clusternumber + 1)\n",
    "        \n",
    "        for contig in contigs:\n",
    "            print(clustername, contig, sep='\\t', file=filehandle)\n",
    "            \n",
    "        clusternumber += 1"
>>>>>>> subcluster
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 56,
=======
   "execution_count": 28,
>>>>>>> subcluster
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "usage: medoid_clustering.py [options]\n",
      "medoid_clustering.py: error: the following arguments are required: output\n"
=======
      "usage: python cluster.py [OPTIONS ...] INPUT OUTPUT\n",
      "ipykernel_launcher.py: error: the following arguments are required: output\n"
>>>>>>> subcluster
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "/usr/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
=======
      "/home/jakni/miniconda3/envs/dev/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
>>>>>>> subcluster
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    usage = \"python cluster.py [OPTIONS ...] INPUT OUTPUT\"\n",
    "    parser = _argparse.ArgumentParser(\n",
    "        description=__doc__,\n",
    "        formatter_class=_argparse.RawDescriptionHelpFormatter,\n",
    "        usage=usage)\n",
    "   \n",
    "    # create the parser\n",
    "    parser.add_argument('input', help='input dataset')\n",
    "    parser.add_argument('output', help='output clusters')\n",
<<<<<<< HEAD
    "    parser.add_argument('-i', dest='inner', help='inner threshold [0.08]', default=0.08, type=float)\n",
    "    parser.add_argument('-o', dest='outer', help='outer threshold [0.10]', default=0.10, type=float)\n",
    "    parser.add_argument('-c', dest='max_clusters', help='max seeds before exiting [30000]', default=30000, type=int)\n",
    "    parser.add_argument('-m', dest='moves', help='Moves before cluster is stable [15]', default=15, type=int)\n",
    "    parser.add_argument('-s', dest='min_size', help='Minimum cluster size to output [1]', default=1, type=int)\n",
=======
    "    parser.add_argument('-i', dest='inner', help='inner distance threshold', type=float)\n",
    "    parser.add_argument('-o', dest='outer', help='outer distnace threshold', type=float)\n",
    "    parser.add_argument('-c', dest='max_clusters',\n",
    "                        help='stop after creating N clusters [None, i.e. infinite]', type=int)\n",
    "    parser.add_argument('-m', dest='max_steps',\n",
    "                        help='stop searchin for optimal medoid after N attempts [15]',\n",
    "                        default=15, type=int)\n",
    "    parser.add_argument('-s', dest='min_size',\n",
    "                        help='minimum cluster size to output [1]',default=1, type=int)\n",
    "    parser.add_argument('--precluster', help='precluster first [False]', action='store_true')\n",
>>>>>>> subcluster
    "    \n",
    "    # Print help if no arguments are given\n",
    "    if len(_sys.argv) == 1:\n",
    "        parser.print_help()\n",
    "        _sys.exit()\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \n",
<<<<<<< HEAD
=======
    "    if args.outer is None:\n",
    "        args.outer = args.inner\n",
    "    \n",
    "    elif args.outer < args.inner:\n",
    "        raise ValueError('outer threshold must exceed or be equal to inner threshold')\n",
    "        \n",
    "    if args.max_clusters is not None and args.precluster:\n",
    "        raise ValueError('Conflicting arguments: precluster and max_clusters')\n",
    "    \n",
>>>>>>> subcluster
    "    if not _os.path.isfile(args.input):\n",
    "        raise FileNotFoundError(args.input)\n",
    "    \n",
    "    if _os.path.isfile(args.output):\n",
    "        raise FileExistsError(args.output)\n",
    "   \n",
    "    matrix = _np.loadtxt(args.input, delimiter='\\t', dtype=_np.float32)\n",
    "    \n",
<<<<<<< HEAD
    "    clusters = cluster(matrix, args.inner, args.outer, args.max_clusters, \n",
    "                       args.moves, args.min_size)\n",
    "    \n",
    "    with open(args.output, 'w') as filehandle:\n",
    "        writeclusters(filehandle, clusters, contignames=None)"
=======
    "    if args.precluster:\n",
    "        contigsof = tandemcluster(matrix, None, args.inner, outer=None,\n",
    "                               max_steps=args.max_steps)\n",
    "        \n",
    "    else:\n",
    "        contigsof = cluster(matrix, None, args.inner, outer=None,\n",
    "                                max_steps=args.max_steps)\n",
    "            \n",
    "    with open(args.output, 'w') as filehandle:\n",
    "        writeclusters(filehandle, contigsof, args.max_clusters, args.min_size)"
>>>>>>> subcluster
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
