{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step four: Clustering (binning) the latent representation\n",
    "\n",
    "__The role of clustering in Vamb__\n",
    "\n",
    "Fundamentally, the process of binning is just clustering sequences based on some of their properties. The purpose of encoding the contigs to a lossy latent representation is to ease the process of clustering because contigs with similar properties are placed close together in latent space, and the latent space is smaller than the input feature space.\n",
    "\n",
    "With the latent representation conveniently represented by an (n_contigs x n_features) matrix, you could use any clustering algorithm to cluster them (such as the ones in `sklearn.cluster`). In practice though, you have perhaps a million contigs and prior constrains on the diameter, shape and size of the clusters, so non-custom clustering algorithms will probably be slow and inaccurate.\n",
    "\n",
    "The module `vamb.cluster` implements a simple and fast iterative medoid clustering algorithm. It is well suited for spherical clusters with a maximum size and for many samples. The algorithm is similar, but subtly different from that used in the metagenomic binner Canopy:\n",
    "\n",
    "    Clustering algorithm:\n",
    "    (1): Pick random seed observation S\n",
    "    (2): Define inner_obs(S) = all observations with Pearson distance from S < INNER\n",
    "    (3): Sample MOVES observations I from inner_obs\n",
    "    (4): If any inner_obs(i) > inner_obs(S) for i in I: Let S be i, go to (2)\n",
    "         Else: Outer_obs(S) = all observations with Pearson distance from S < OUTER\n",
    "    (5): Output outer_obs(S) as cluster, remove inner_obs(S) from observations\n",
    "    (6): If no more observations or MAX_CLUSTERS have been reached: Stop\n",
    "         Else: Go to (1)\n",
    "\n",
    "__Determining clustering threshold__\n",
    "\n",
    "You will notice that this algorithm depends on the parameters `INNER` and `OUTER`. This corresponds to the thresholds for Pearson distance, any contigs within which is considered part of the same bin. Getting this measure right is crucial for the binning to work well. Put them too low, and the bins will be highly fragmented. Too high, and distinct genomes will be binned together.\n",
    "\n",
    "In order to estimate a good threshold for this distance, we have written the module `threshold`, in where there is the function `getthreshold`:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(vamb.threshold.getthreshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "This function samples multiple random contigs (1000 by default) and calculates the distance to all other contigs. Sometimes, these distances separate neatly into a small group of close contigs and all the other contigs. For sampled contigs where this is the case, `getthreshold` records the distance which separates those two groups, and then it returns the median of those values.\n",
    "\n",
    "The world isn't always so neat however, and sometimes the clusters are not very well separated - there is no distance between the \"close\" contigs and the \"far\" ones where the contig density drops to zero - i.e. there are contigs at any given distance from the sampled one. When that's the case, `getthreshold` uses the first distance where you see a dip in the contig density and flags this distance as \"not well separated\". This is usually a little smaller than the valley of the dip, because the lack of separation means we would get too much noise if we close the valley as the threshold.\n",
    "\n",
    "Sometimes it's even worse and there isn't even a dip in density, the other contigs appear to be randomly distributed around the sampled contigs. For those contigs, it does not find any threshold.\n",
    "\n",
    "The `support` and `separation` measures the fraction of contigs for which any threshold is found, and which are \"well separated\", reprectively. If too many  (> 25%, we believe) has no threshold, the latent representation failed to properly separate the bins. If too many are not well separated, the clustering will work but perform suboptimally.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = vamb.threshold.getthreshold(latent, samples=1000)\n",
    "threshold, support, separation = _\n",
    "\n",
    "print('Estimated threshold:', round(threshold, 3))\n",
    "print('Support:', round(support * 100, 1), '%')\n",
    "print('Separation:', round(separation * 100, 1), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Uh oh, only 5.3% separation. Around 50% support, so it *can* find thresholds, it just relies on finding the \"dip\" in the density as mentioned above.  This means we shouldn't have so high hopes for the quality of the bins but it should be totally rubbish. Now, to the clustering itself:\n",
    "\n",
    "In Vamb, we have implemented the algorithm as two distinct functions: \n",
    "\n",
    "* `vamb.cluster.cluster`, simply clusters a matrix, and so scales approximately O(n<sup>2</sup>).\n",
    "\n",
    "* `vamb.cluster.tandemcluster` does some very rough preclustering and then clusters each precluster using `vamb.cluster.cluster`. Each observation is then assigned uniquely to the largest cluster it's a member of. This scales better with number of contigs, but is also significantly less accurate.\n",
    "\n",
    "You can use the slow-but-accurate with up to one or two million contigs depending on your patience or ~10 million contigs if you're alright with running it for days.\n",
    "\n",
    "The heavy lifting here is done in Numpy, so it might be worth making sure the BLAS library your Numpy is using is fast. You can check it with `numpy.__config__.show()` and if it says anything other than `NOT AVAILABLE` under the `mkl` or `openblas` entries, you're golden.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(vamb.cluster.cluster)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# As written in the help above, labels must be a Numpy array!\n",
    "labels = np.array(contignames)\n",
    "\n",
    "# Unlike tandemcluster, which outputs the dictionary directly,\n",
    "# the output of cluster is a generator\n",
    "cluster_iterator = vamb.cluster.cluster(latent, labels, threshold)\n",
    "\n",
    "clusters = dict()\n",
    "for medoid, contigs in cluster_iterator:\n",
    "    clusters[medoid] = contigs\n",
    "\n",
    "print('Last key:', medoid, '(of type:', type(medoid), ')')\n",
    "print('Type of values:', type(contigs))\n",
    "print('First element of value:', next(iter(contigs)), 'of type:', type(next(iter(contigs))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step five: Postprocessing the clusters\n",
    "\n",
    "We haven't written any postprocessing modules because how to postprocess really depends on what you're looking for in your data.\n",
    "\n",
    "One of the greatest weaknesses of Vamb - probably of metagenomic binners in general - is that the bins tend to be highly fragmented. You'll have lots of tiny bins, some of which are legitimate (viruses, plasmids), but most are parts of larger genomes that didn't get binned properly.\n",
    "\n",
    "We're in the process of developing a tool for annotating, cleaning and merging bins based on phylogenetic analysis of the genes in the bins. That would be extremely helpful, but for now, we'll have to use more crude approaches:\n",
    "\n",
    "We throw away all bins with less than 250,000 basepairs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's make a contignames: length dict\n",
    "lengthof = dict(zip(contignames, lengths))\n",
    "\n",
    "# Now filter away the small bins\n",
    "filtered_bins = dict()\n",
    "\n",
    "for medoid, contigs in clusters.items():\n",
    "    binsize = sum(lengthof[contig] for contig in contigs)\n",
    "    \n",
    "    if binsize >= 250000:\n",
    "        filtered_bins[medoid] = contigs\n",
    "\n",
    "print('Number of bins before filtering:', len(clusters))\n",
    "print('Number of bins after filtering:', len(filtered_bins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now, let's save the clusters to disk. For this we will use two writer functions:\n",
    "\n",
    "1) `vamb.cluster.writeclusters`, that writes which clusters contains which contigs to a simple tab-separated file, and\n",
    "\n",
    "2) `vamb.vambtools.writebins`, that writes FASTA files corresponding to each of the bins to a directory.\n",
    "\n",
    "We will need to load all the contigs belonging to any bin into memory to use `vamb.vambtools.writebins`. If your bins don't fit in memory, sorry, you gotta find another way to make those FASTA bins.\n",
    "\n",
    "The cluster name when printing either way will be the dictionary key of the bins. Right now, our bins have names like `s101_NODE_663_length_11806_cov_4.19433` - not exactly poetic. We'll rename the bins first.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename bin keys to something less horrible as a file name\n",
    "filtered_bins = {'cluster_' + str(i+1): v for i, v in enumerate(filtered_bins.values())}\n",
    "\n",
    "with open('/home/jakni/Downloads/example/bins.tsv', 'w') as file:\n",
    "    vamb.cluster.writeclusters(file, filtered_bins)\n",
    "\n",
    "# Only keep contigs in any filtered bin in memory\n",
    "allcontigs = set.union(*filtered_bins.values())\n",
    "\n",
    "with open('/home/jakni/Downloads/example/contigs.fna', 'rb') as file:\n",
    "    fastadict = vamb.vambtools.loadfasta(file, keep=allcontigs)\n",
    "    \n",
    "vamb.vambtools.writebins('/home/jakni/Downloads/example/bins/', filtered_bins, fastadict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
