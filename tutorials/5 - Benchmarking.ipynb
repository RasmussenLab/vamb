{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (If you have a reference: Benchmark the output)\n",
    "\n",
    "For this to make any sense, you need to have a *reference*, that is, a list of bins that are deemed true and complete.\n",
    "\n",
    "The reference could be a {clustername: set(contigs)} dict along with a {contigname: length} dict, just like the `clusters` and `lengthof` we made. It could also be a tab-separated file with (clustername, contigname, length)-rows, one row per contig.\n",
    "\n",
    "Now, I have no reference for this dataset, so I created a reference file completely randomly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head /home/jakni/Downloads/example/reference.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We of course expect the benchmark to show we have at most a handful of very incomplete bins, since the reference is random.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_path = '/home/jakni/Downloads/example/reference.tsv'\n",
    "\n",
    "with open(reference_path) as filehandle:\n",
    "    reference = vamb.benchmark.Reference.fromfile(filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We also need to instantiate the Observed bins (which we created above!), and a BenchMarkResult\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could also do this from the bins.tsv we created in the previous section,\n",
    "# but here we have the dictionary with bins in memory already.\n",
    "observed = vamb.benchmark.Observed(filtered_bins, reference)\n",
    "\n",
    "# Keyword-only arguments to make sure you don't accidentally swap them around.\n",
    "# It'll raise an error if you use non-keyword arguments.\n",
    "result = vamb.benchmark.BenchMarkResult(reference=reference, observed=observed)\n",
    "\n",
    "result = vamb.benchmark.BenchMarkResult(reference=reference, observed=observed)\n",
    "\n",
    "# Okay, how did we do?\n",
    "result.printmatrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "This matrix shows the number of reference bins where, for each value of recall and precision there is at least one observed bin that passes those criteria.\n",
    "\n",
    "As expected (because the reference was randomly generated), the results are terrible - in fact, they couldn't be worse.\n",
    "\n",
    "To check what else the BenchMarkResult measures, check `help(vamb.benchmark.BenchMarkResult)`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
