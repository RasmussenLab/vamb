{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step three: Train the autoencoder and encode input data\n",
    "\n",
    "Again, you can use `help` to see how to use the module\n",
    "\n",
    "`>>> help(vamb.encode)`\n",
    "\n",
    "    Help on module vamb.encode in vamb:\n",
    "\n",
    "    NAME\n",
    "        vamb.encode - Encode a depths matrix and a tnf matrix to latent representation.\n",
    "\n",
    "    DESCRIPTION\n",
    "        Creates a variational autoencoder in PyTorch and tries to represent the depths\n",
    "        and tnf in the latent space under gaussian noise.\n",
    "\n",
    "        usage:\n",
    "        >>> vae, dataloader = trainvae(depths, tnf) # Make & train VAE on Numpy arrays\n",
    "        >>> latent = vae.encode(dataloader) # Encode to latent representation\n",
    "        >>> latent.shape\n",
    "        (183882, 40)\n",
    "        \n",
    "    [ lines elided ]\n",
    "    \n",
    "---\n",
    "Aha, so we need to use the `trainvae` function first, then the `VAE.encode` method. You can call the `help` functions on those, but I'm not showing that here.\n",
    "\n",
    "Training networks always take some time. If you have a GPU and CUDA installed, you can pass `cuda=True` to `encode.trainvae` to train on your GPU for increased speed. With a beefy GPU, this can make quite a difference. I run this on my laptop, so I'll just use my CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, we import stuff\n",
    "import sys\n",
    "sys.path.append('/home/jakni/Documents/scripts/')\n",
    "import vamb\n",
    "\n",
    "# And load the data we just saved - of course, if this wasn't in different\n",
    "# notebooks, we could have just kept it in memory\n",
    "with open('/home/jakni/Downloads/example/rpkms.npz', 'rb') as file:\n",
    "    rpkms = vamb.vambtools.read_npz(file)\n",
    "    \n",
    "with open('/home/jakni/Downloads/example/tnfs.npz', 'rb') as file:\n",
    "    tnfs = vamb.vambtools.read_npz(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tLoss: 2.4797\tBCE: 2.4665\tMSE: 0.00375\tKLD: 0.0094\n",
      "Epoch: 2\tLoss: 2.2734\tBCE: 2.2587\tMSE: 0.00562\tKLD: 0.0091\n",
      "Epoch: 3\tLoss: 2.1193\tBCE: 2.1024\tMSE: 0.00798\tKLD: 0.0089\n",
      "Epoch: 4\tLoss: 1.9853\tBCE: 1.9657\tMSE: 0.01085\tKLD: 0.0087\n",
      "Epoch: 5\tLoss: 1.8654\tBCE: 1.8433\tMSE: 0.01379\tKLD: 0.0083\n"
     ]
    }
   ],
   "source": [
    "# I'm training just 5 epochs for this demonstration.\n",
    "# When actually using the VAE, 200-300 epochs are suitable\n",
    "vae, dataloader = vamb.encode.trainvae(rpkms, tnfs, nepochs=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The VAE encodes the high-dimensional (n_samples + 136 features) input data in a lower dimensional space (nlatent features). When training, it learns both the encoding scheme and attempts to reconstruct the input data given the latent representation influenced by gaussian noise.\n",
    "\n",
    "The theory here is that the latent representation should be a more efficient encoding of the input data. If the input data for the contigs indeed do fall into bins, an efficient encoding should be to simply encode the bin they belong to, then use the \"bin identity\" to reconstruct the data. We add noise to prevent it from learning a huge number of slightly different bins, in the most extreme, each bin contains only one contig.\n",
    "\n",
    "The loss of the VAE is the sum of three measures:\n",
    "\n",
    "* Binary cross entropy (BCE) measures the dissimilarity of the reconstructed abundances to observed abundances\n",
    "* Mean squared error (MSE) measures the dissimilary of reconstructed versus observed TNF\n",
    "* Kullback-Leibler divergence (KLD) measures the dissimilarity between the standard normal distribution and the distribution of encoded values with noise added\n",
    "\n",
    "At least in principle, the latter term indudes the VAE to not crazily overfit by imposing some sensible prior on the kind of encodings it can choose.\n",
    "\n",
    "We can see the Mean Squared Error (which is the TNF-related loss) is rising these first 5 epochs, presumably as it sacrifices an efficient representation of the TNF in order to learn the depths (whose loss is BCE) better. This happens sometimes, and it's alright - after all, co-abundance usually contain more information that TNF, and so we have chosen the BCE to be several orders of magnitude higher than the MSE in order for the VAE to be able to make this choice.\n",
    "\n",
    "Okay, so now we have the trained `vae` and the `dataloader`. Let's feed the dataloader to the VAE in order to get the latent representation:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39551, 40)\n"
     ]
    }
   ],
   "source": [
    "# No need to pass gpu=True to the encode function to encode on GPU\n",
    "# If you trained the VAE on GPU, it already resides there\n",
    "latent = vae.encode(dataloader)\n",
    "\n",
    "print(latent.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "That's 39551 contigs each represented by the (non-noisy) value of 40 latent neurons.\n",
    "\n",
    "Now we need to cluster this. That's for the next notebook, so again, I'll save the results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/jakni/Downloads/example/latent.npz', 'wb') as file:\n",
    "    vamb.vambtools.write_npz(file, latent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
