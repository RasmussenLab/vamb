{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walkthough of Vamb from the Python interpreter\n",
    "\n",
    "The Vamb pipeline consist of a series of tasks each which have a dedicated module:\n",
    "\n",
    "1) Parse fasta file and get TNF of each sequence, as well as sequence length and names (module `parsecontigs`)\n",
    "\n",
    "2) Parse the BAM files and get abundance estimate for each sequence in the fasta file (module `parsebam`)\n",
    "\n",
    "3) Train a VAE with the abundance and TNF matrices, and encode it to a latent representation (module `encode`)\n",
    "\n",
    "4) Cluster the encoded inputs to metagenomic bins (module `cluster`)\n",
    "\n",
    "Additionally, for developing and testing Vamb, we use:\n",
    "\n",
    "5) Benchmark the resulting bins against a gold standard (module `benchmark`)\n",
    "\n",
    "In the following chapters of this walkthrough, we will go through each step in more detail from within the Python interpreter. We will show how to use Vamb by example, what each step does, some of the theory behind the actions, and the different parameters that can be set. With this knowledge, you should be able to extend or alter the behaviour of Vamb more easily.\n",
    "\n",
    "For the examples, we will assume the following relevant prerequisite files exists in the directory `/Users/jakni/Downloads/example/`:\n",
    "\n",
    "* `contigs.fna` - The filtered FASTA contigs which were mapped against, and\n",
    "* `bamfiles/*.bam` - The 6 BAM files from mapping the reads to the contigs above.\n",
    "\n",
    "## Table of contents:\n",
    "\n",
    "### 1. [Introduction to metagenomic binning and best practices](#introduction)\n",
    "\n",
    "### 2. [Importing Vamb and getting help](#importing)\n",
    "\n",
    "### 3. [Calculate the sequence tetranucleotide frequencies](#parsecontigs)\n",
    "\n",
    "### 4. [Calculate the abundance matrix](#parsebam)\n",
    "\n",
    "### 5. [Train the autoencoder and encode input data](#encode)\n",
    "\n",
    "### 6. [Binning the encoding](#cluster)\n",
    "\n",
    "### 7. [Postprocessing the bins](#postprocessing)\n",
    "\n",
    "### 8. [Summary of full workflow](#summary)\n",
    "\n",
    "### 9. [Running VAMB with low memory (RAM)](#memory)\n",
    "\n",
    "### 10. [Optional: Benchmarking your bins](#benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"introduction\"></a>\n",
    "## Introduction to metagenomic binning and best practices\n",
    "\n",
    "When we began working on the Vamb project, we had been doing reference-free metagenomics for years using MetaBAT2, and thought we had a pretty good idea of how metagenomic binning worked and what to watch out for. Turns out, there's more to it than we had thought about. During this project, our group have revised the way we use binners multiple times as we've learned the dos and don'ts. In this introduction I will cover some facts of binning that is useful for the end user.\n",
    "\n",
    "__1. Binning is a hard problem__\n",
    "\n",
    "Various binning papers (including ours), typically brag about how many high-quality genomes they've been able to reconstruct from data. However, examining the actual number of genomes reconstructed compared to the estimated diversity of the samples can be sobering: Only a very small fraction of the organisms are binned decently. In fact, only a small fraction of the *sequenced* organisms are binned decently!\n",
    "\n",
    "Information is lost at every step along the way: In the sequencing itself, during assembly, when estimating contig abundance, and during the binning. For now, this is just a fact to accept, but we hope it will improve in the future, especially with better long-read sequencing technologies.\n",
    "\n",
    "__2. There is a tradeoff in what contigs to discard__\n",
    "\n",
    "Typically, the inputs to the binners are sequence composition and/or sequence abundance. Both these metrics are noisy for smaller contigs, as the kmer composition is not stable for small contigs, and the estimated depth across a genome varies wildly. This causes small contigs to be hard to handle for binners, as the noise swamps the signal.\n",
    "\n",
    "Worse, the presence of hard-to-bin contigs may adversely affect the binning of easy-to-bin contigs depending on the clustering algorithm used. One solution is to discard small contigs, but here lies a tradeoff: When are you just throwing away good data? This is especially bitter when the large majority of metagenomic assemblies usually lies in short contigs. With current (2019) assembly and mapping technology, we are usign a threshold of around 2000 bp.\n",
    "\n",
    "__3. Garbage in, garbage out__\n",
    "\n",
    "Binners like MaxBin, MetaBAT2 and Vamb uses kmer frequencies and estimated depths to bin sequences. While the kmer frequency is easy to calculate (but sensitive to small contigs - see above), this is not the case for depth. Depths are estimated by mapping reads back to the contigs and counting where they map to. Here especially, it's worth thinking about exactly how you map:\n",
    "\n",
    "* In general, if reads are allowed to map to subjects with a nucleotide identity of X %, then it is not possible to distinguish genomes with a higher nucleotide identity than X % using co-abundance, and these genomes will be binned together. This means you want to tweak your alignment tool to only output alignments with the desired minimum query/subject identity - for example 97%.\n",
    "\n",
    "* The MAPQ field of a BAM/SAM file is the probability that the mapping position is correct. Typically, aligners outputs a low MAPQ score if a read can map to multiple sequences. However, because we expect to have multiple similar sequences in metagenomics (e.g. the same strain from different samples), filtering low MAPQ-alignments away using e.g. Samtools will give bad results.\n",
    "\n",
    "So when creating your BAM files, make sure to think of the above. Furthermore, there are some issues to think about when estimating depths:\n",
    "\n",
    "* If a read maps well to N references, the read needs to count 1/N towards the depth of each of those references. Not counting secondary alignments is misleading (see above), and having it count 1 towards each reference will cause the depth of conserved genomic regions to be overestimated, which can affect binning.\n",
    "\n",
    "* You need to consider how to count a properly mapped read pair versus a pair with each mate mapping to distinct references. Some aligners e.g. BWA MEM will assign reads seemingly independent of their mates, in which case each mate should count independently towards the depth.\n",
    "\n",
    "The BAM parsing module in Vamb takes care of these two latter points.\n",
    "\n",
    "__4. It's hard to figure out if a binner is doing well__\n",
    "\n",
    "What makes a good binner? This question quickly becomes subjective or context dependent:\n",
    "\n",
    "* What's the ideal precision/recall tradeoff? One researcher might prefer pure but fragmented genomes, whereas another might want complete genomes, and care less about the purity.\n",
    "\n",
    "* What's the ideal cutoff in bin quality where the binner should not output the bin? Is it better to only produce good bins, or produce all bins, including the dubious ones?\n",
    "\n",
    "* Which taxonomic level is the right level to bin at? If nearly identical genomes from different samples go in different bins, is that a failure of the binner, or successfully identitfying microdiversity? What about splitting up strains? Species? Since the concept of a bacterial species is arbitrary, should the binner ideally group along nucleotide identity levels, regardless of the taxonomic annotation?\n",
    "\n",
    "* Are plasmids, prophages etc. considered a necessary part of a bin? Should a binner strive to bin plasmids with their host? What if it comes at a cost of genomic recall or precision?\n",
    "\n",
    "* If a genome is split equally into three bins, with the genome being a minority in each bin does that mean the genome is present at a recall of 0%, because each bin by majority vote will be assigned to another genome, a recall of 33%, because that's the maximal recall in any particular bin, or at 100%, because the whole genome is present in the three bins?\n",
    "\n",
    "* Should contigs be allowed to be present in multiple bins? If so, any ambiguity in binning can be trivially resolved by producing multiple bins where there is ambiguity (in extrema: Outputting the powerset of input contigs) - but surely, this just pushes the problems to the next point in the pipeline. If not, then conserved genomic regions, where reads from multiple species can be assembled into a single contig, cannot be binned in the multiple bins where it is actually present.\n",
    "\n",
    "* What postprocessing can you assume people are doing after the binning? Given e.g. a bin deduplication postprocessing step, creating multiple similar bins does not matter so much. However, if no postprocessing is done, doing so will give misleading results. \n",
    "\n",
    "Unfortunately, all these choices have a significant impact on how you asses performance of a binner - and thus, *how you create a binner in the first place*. That means, given different definitions of binning quality, different binners might to worse or better.\n",
    "\n",
    "__5. In summary__\n",
    "\n",
    "Here's the workflow we are using now:\n",
    "\n",
    "* Assemble your bins one sample at a time using a dedicated metagenomic assembler. We recommend metaSPAdes.\n",
    "* Concatenate the contigs/scaffolds to a single FASTA file, making sure that your FASTA headers are all unique.\n",
    "* Remove contigs < 2000 bp from the FASTA file.\n",
    "* Map reads from each sample to the FASTA file. Make sure to set the minimum accepted mapping identity threshold to be similar to the identity threshold with which you want to bin. Do not sort the BAM files, or if they are already sorted, sort again by read name. Do not filter for poor MAPQ. Output all secondary alignments.\n",
    "* Run Vamb with default parameters, unless you really know what you are doing.\n",
    "* After binning, split your bins according to the sample they originated from. In this way, you can bin using co-abundance across samples, while still seeing microdiversity from sample to sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"importing\"></a>\n",
    "## Importing Vamb and getting help\n",
    "\n",
    "First step is to get Vamb imported. If you installed with `pip`, it should be directly importable. Else, you might have to add the path of Vamb to `sys.path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vamb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using Vamb, you'll almost certianly need help (we wish it was so easy you didn't, but making user friendly software is *hard!*).\n",
    "\n",
    "Luckily, there's the built-in `help` function in Python.\n",
    "\n",
    "---\n",
    "\n",
    "`>>> help(vamb)`\n",
    "    \n",
    "    Help on package vamb:\n",
    "\n",
    "    NAME\n",
    "        vamb - Variational Autoencoder for Metagenomic Binning\n",
    "\n",
    "    DESCRIPTION\n",
    "        Vamb does what it says on the tin - bins metagenomes using a variational autoencoder.\n",
    "        \n",
    "    [ lines elided ]\n",
    "    \n",
    "        General workflow:\n",
    "        1) Filter contigs by size using vamb.vambtools.filtercontigs\n",
    "        2) Map reads to contigs to obtain BAM file\n",
    "        3) Calculate TNF of contigs using vamb.parsecontigs\n",
    "        4) Create RPKM table using vamb.parsebam\n",
    "        5) Train autoencoder using vamb.encode\n",
    "        6) Cluster latent representation using vamb.cluster\n",
    "    \n",
    "    [ lines elided ]\n",
    "    \n",
    "---\n",
    "    \n",
    "The `PACKAGE CONTENTS` under `help(vamb)` is just a list of all *importable* files in the `vamb` directory - some of these aren't part of the Vamb package proper and really shouldn't be imported, so ignore that.\n",
    "\n",
    "---\n",
    "You can also get help for each of the modules, for example the `cluster` module:\n",
    "\n",
    "`>>> help(vamb.cluster)`\n",
    "\n",
    "    Help on module vamb.cluster in vamb:\n",
    "\n",
    "    NAME\n",
    "        vamb.cluster - Iterative medoid clustering.\n",
    "\n",
    "    DESCRIPTION\n",
    "        Usage:\n",
    "        >>> cluster_iterator = cluster(rpkms, tnfs, labels=contignames)\n",
    "        >>> clusters = dict(cluster_iterator)\n",
    "\n",
    "        Implements one core function, cluster, along with the helper\n",
    "        functions write_clusters and read_clusters.\n",
    "        For all functions in this module, a collection of clusters are represented as\n",
    "        a {clustername, set(elements)} dict.\n",
    "\n",
    "        cluster algorithm:\n",
    "    \n",
    "    [ lines elided ]\n",
    "        \n",
    "---\n",
    "And for functions:\n",
    "\n",
    "`>>> help(vamb.cluster.cluster)`\n",
    "\n",
    "    Help on function cluster in module vamb.cluster:\n",
    "\n",
    "    cluster(matrix, labels=None, maxsteps=25, windowsize=200, minsuccesses=20, default=0.09, destroy=False, normalized=False, logfile=None)\n",
    "        Iterative medoid cluster generator. Yields (medoid), set(labels) pairs.\n",
    "\n",
    "        Inputs:\n",
    "            matrix: A (obs x features) Numpy matrix of data type numpy.float32\n",
    "            labels: None or Numpy array/list with labels for seqs [None = indices+1]\n",
    "            maxsteps: Stop searching for optimal medoid after N futile attempts [25]\n",
    "            default: Fallback threshold if cannot be estimated [0.09]\n",
    "            windowsize: Length of window to count successes [200]\n",
    "            minsuccesses: Minimum acceptable number of successes [15]\n",
    "            destroy: Save memory by destroying matrix while clustering [False]\n",
    "            normalized: Matrix is already preprocessed [False]\n",
    "            logfile: Print threshold estimates and certainty to file [None]\n",
    "\n",
    "        Output: Generator of (medoid, set(labels_in_cluster)) tuples.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"parsecontigs\"></a>\n",
    "## Calculate the sequence tetranucleotide frequencies\n",
    "\n",
    "If you forget what to do at each step, remember that `help(vamb)` said:\n",
    "\n",
    "    General workflow:\n",
    "    1) Filter contigs by size using vamb.vambtools.filtercontigs\n",
    "    2) Map reads to contigs to obtain BAM file\n",
    "    3) Calculate TNF of contigs using vamb.parsecontigs\n",
    "    \n",
    "    [ lines elided ]\n",
    "\n",
    "Okay, we already have filtered contigs. I could have used the `vamb.vambtools.filtercontigs` to filter the FASTA file, but here, they were already filtered. We have also already mapped reads to them and gotten BAM files, so we begin with the third step, using the `vamb.parsecontigs` module. How do you use that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module vamb.parsecontigs in vamb:\n",
      "\n",
      "NAME\n",
      "    vamb.parsecontigs - Calculate tetranucleotide frequency from a FASTA file.\n",
      "\n",
      "DESCRIPTION\n",
      "    Usage:\n",
      "    >>> with open('/path/to/contigs.fna', 'rb') as filehandle\n",
      "    ...     tnfs, contignames, lengths = read_contigs(filehandle)\n",
      "\n",
      "FUNCTIONS\n",
      "    read_contigs(filehandle, minlength=100, preallocate=False)\n",
      "        Parses a FASTA file open in binary reading mode.\n",
      "        \n",
      "        Input:\n",
      "            filehandle: Filehandle open in binary mode of a FASTA file\n",
      "            minlength: Ignore any references shorter than N bases [100]\n",
      "            preallocate: [DEPRECATED] Read contigs twice, saving memory [False]\n",
      "        \n",
      "        Outputs:\n",
      "            tnfs: An (n_FASTA_entries x 136) matrix of tetranucleotide freq.\n",
      "            contignames: A list of contig headers\n",
      "            lengths: A Numpy array of contig lengths\n",
      "\n",
      "FILE\n",
      "    /Users/jakni/Documents/scripts/vamb/vamb/parsecontigs.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(vamb.parsecontigs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "I use `vamb.parsecontigs.read_contigs` with the inputs and outputs as written:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File must be opened in binary mode\n",
    "with open('/Users/jakni/Downloads/example/contigs.min2kbp.fna', 'rb') as filehandle:\n",
    "    tnfs, contignames, lengths = vamb.parsecontigs.read_contigs(filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let's have a look at the resulting data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of tnfs: <class 'numpy.ndarray'> of dtype float32\n",
      "Shape of tnfs: (50658, 136)\n",
      "\n",
      "Type of contignames: <class 'list'>\n",
      "Length of contignames: 50658\n",
      "\n",
      "First 5 elements of contignames:\n",
      "30_NODE_1_length_245508_cov_18.4904\n",
      "30_NODE_2_length_222690_cov_39.7685\n",
      "30_NODE_3_length_222459_cov_20.3665\n",
      "30_NODE_4_length_173155_cov_20.1181\n",
      "30_NODE_5_length_161239_cov_20.1237\n",
      "\n",
      "Type of lengths: <class 'numpy.ndarray'> of dtype int64\n",
      "Length of lengths: 50658\n",
      "\n",
      "First 5 elements of lengths:\n",
      "245508\n",
      "222690\n",
      "222459\n",
      "173155\n",
      "161239\n"
     ]
    }
   ],
   "source": [
    "print('Type of tnfs:', type(tnfs), 'of dtype', tnfs.dtype)\n",
    "print('Shape of tnfs:', tnfs.shape, end='\\n\\n')\n",
    "\n",
    "print('Type of contignames:', type(contignames))\n",
    "print('Length of contignames:', len(contignames), end='\\n\\n')\n",
    "\n",
    "print('First 5 elements of contignames:')\n",
    "for i in range(5):\n",
    "    print(contignames[i])\n",
    "\n",
    "print('\\nType of lengths:', type(lengths), 'of dtype', lengths.dtype)\n",
    "print('Length of lengths:', len(lengths), end='\\n\\n')\n",
    "\n",
    "print('First 5 elements of lengths:')\n",
    "for i in range(5):\n",
    "    print(lengths[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "__For a zipped FASTA file__, simply use `with open vamb.vambtools.Reader('/path/to/contigs.fna' ,'rb')`. The `Reader` automatically and transparently reads plaintext, gzipped, bzip2 and .xz files, and return an opened file object.\n",
    "\n",
    "Note that reading zipped files will slow down the FASTA parsing quite a bit. But the time spent parsing the FASTA file will likely still be insignificant compared to the other steps of Vamb.\n",
    "\n",
    "__The rationale for parsing the contigs__ is that it turns out that related organisms tend to share a similar kmer-distribution across most of their genome. The reason for that is not understood, even though it's believed that common functional motifs, GC-content and presence/absence of endonucleases explains some of the observed similary.\n",
    "\n",
    "The `tnfs` is the tetranucleotide frequency - it's the frequency of the canonical kmer of each 4mer in the contig. We use 4-mers because there are 256 4-mers, which is an appropriate number of features to cluster - not so few that there's no signal and not so many it becomes unwieldy and the estimates of the frequencies become uncertain. We could also have used 3-mers. In tests we have made, 3-mers are *almost*, but not quite as good as 4-mers for separating different species. You could probably switch tetranucleotide frequency to trinucleotide frequency in Vamb without any significant drop of accuracy. However, there are 1024 5-mers, that would be too many features to handle comfortably, and it could easily cause memory issues.\n",
    "\n",
    "We do not work with tetranucleotide frequencies (TNF) directly. TNFs are highly correlated, for example, we would expect the frequency of `AAGA` to be very similar to `AGAA`. Some of these correlations are due to e.g. GC content which is a valid signal. But some of these correlations are due to interdependencies between the TNFs which does NOT constitute a signal and is pure redundancy in the data. Removing this saves RAM, computing resources, and may also make the VAE converge better. There are three such dependencies:\n",
    "\n",
    "* Being frequencies, they must sum to 1.\n",
    "* The frequency of a kmer must be the same as the reverse complement (RC), because the RC is simply present on the other strand. We only observe one strand, which one is arbitrary. So we average between kmers and RC. For example, if we see 31 AAGA and 24 TCTT, we will note (31+24)/2 = 27.5 of each instead.\n",
    "* Every time we observe a kmer with the bases ABCD, we *must* also observe the immediately following kmer, BCDE. Hence, for every 3-mer XYZ, it is true that (AXYZ + CXYZ + GXYZ + TXYZ) = (XYZA + XYZC + XYZG + XYZT). This is not always true for finite contigs because the 4mer at the end has no \"next\" 4mer. But that fact is a measurement error in the first place, so we do not care about that.\n",
    "\n",
    "Based on these constraints, we can create a set of linear equations, and then find the kernel matrix L which projects any matrix of 4mer frequencies T to a matrix P with fewer features (103 in this case):\n",
    "\n",
    "$$\\textbf{P} = \\textbf{TL}$$\n",
    "\n",
    ". We enforce constraint 2 by tallying the mean of each kmer and it's frequency. This reverse complement averaging can be done using a matrix multiply on the raw tetranucleotide frequencies F with the matrix R:\n",
    "\n",
    "$$\\textbf{T} = \\textbf{FR}$$\n",
    "\n",
    "And since matrix multiplication is associative, we can create a single kernel K which does both the reverse complementation averaging and projects down to 103 features in one matrix multiply:\n",
    "\n",
    "$$\\textbf{P} = \\textbf{TL} = (\\textbf{FR})\\textbf{L} = \\textbf{F}(\\textbf{RL}) = \\textbf{FK}$$\n",
    "\n",
    "The kernel K is calculated in `src/create_kernel.py`, but comes pre-calculated with Vamb. See the method section of [Kislyuk et al. 2009](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2765972/) for more details.\n",
    "\n",
    "__The argument `minlength`__ sets the filter removing any contigs shorter than this. We don't know exactly what the sweet spot is, but it's probably somewhere around ~2000 bp.\n",
    "\n",
    "The problem with filtering contigs using `minlength` is that the smaller contigs which are thrown away will still recruit reads during the mapping that creates the BAM files, thus removing information from those reads. For that reason, we recommend filtering the contigs *before* mapping.\n",
    "\n",
    "__The argument `preallocate` is deprecated in the latest version of Vamb.__ Don't touch it.\n",
    "\n",
    "__The memory consumption of Vamb can be an issue__, so at this point, you should probably consider whether you have enough RAM. This is a small dataset, so there's no problem. With hundreds of samples and millions of contigs however, this becomes a problem, even though Vamb is fairly memory-friendly. If you think memory might be an issue, see the [Running VAMB with low memory (RAM)](#memory) section.\n",
    "\n",
    "<a id=\"parsebam\"></a>\n",
    "## Calculate the abundance matrix\n",
    "\n",
    "Again, we can use the help function to see what we need to do\n",
    "    \n",
    "`>>> help(vamb.parsebam)`\n",
    "\n",
    "    Help on module vamb.parsebam in vamb:\n",
    "\n",
    "    NAME\n",
    "        vamb.parsebam - Estimate RPKM (depths) from BAM files of reads mapped to contigs.\n",
    "\n",
    "    DESCRIPTION\n",
    "        Usage:\n",
    "        >>> bampaths = ['/path/to/bam1.bam', '/path/to/bam2.bam', '/path/to/bam3.bam']\n",
    "        >>> rpkms = read_bamfiles(bampaths)\n",
    "\n",
    "    FUNCTIONS\n",
    "    \n",
    "    [ lines elided ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/jakni/Downloads/example/bam/101.bam',\n",
       " '/Users/jakni/Downloads/example/bam/178.bam',\n",
       " '/Users/jakni/Downloads/example/bam/179.bam',\n",
       " '/Users/jakni/Downloads/example/bam/196.bam',\n",
       " '/Users/jakni/Downloads/example/bam/198.bam',\n",
       " '/Users/jakni/Downloads/example/bam/30.bam']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's do it:\n",
    "\n",
    "bamfiles = !ls /Users/jakni/Downloads/example/bam\n",
    "bamfiles = ['/Users/jakni/Downloads/example/bam/' + p for p in bamfiles]\n",
    "bamfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of rpkms: <class 'numpy.ndarray'> of dtype float32\n",
      "Shape of rpkms (50658, 6)\n"
     ]
    }
   ],
   "source": [
    "# Yep, those file paths look right.\n",
    "\n",
    "rpkms = vamb.parsebam.read_bamfiles(bamfiles) # This takes some time.\n",
    "print('Type of rpkms:', type(rpkms), 'of dtype', rpkms.dtype)\n",
    "print('Shape of rpkms', rpkms.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The idea here is that two contigs from the same genome will always be physically present together, and so they should have a similar abundance in all samples. Some contigs represent repeats like duplicated segments - these contigs should have a fixed ratio of abundance to other contigs. Thus, even when considering repeated contigs, there should be a tight cosine distance correlation between abundances of contigs from the same genome.\n",
    "\n",
    "The `vamb.parsebam` module takes a rather crude approach to estimating abundance, namely by simply counting the number of mapped reads to each contig, normalized by total number of reads and the contig's length. This measure is in trancriptomics often called RPKM, *reads per kilobase per million mapped reads*. Other metagenomic binners like Metabat and Canopy uses an average of per-nucleotide depth of coverage instead. We do not believe there is any theoretical or practical advantage of using depth over RPKM. We will use the terms *abundance*,  *depth* and *RPKM* interchangably.\n",
    "\n",
    "---\n",
    "`>>> help(vamb.parsebam.read_bamfiles)`\n",
    "\n",
    "    Help on function read_bamfiles in module vamb.parsebam:\n",
    "\n",
    "    read_bamfiles(paths, dumpdirectory=None, refhash=None, minscore=None, minlength=None, minid=None, subprocesses=8, logfile=None)\n",
    "        Spawns processes to parse BAM files and get contig rpkms.\n",
    "\n",
    "        Input:\n",
    "            path: List or tuple of paths to BAM files\n",
    "            dumpdirectory: [None] Dir to create and dump per-sample depths NPZ files to\n",
    "            refhash: [None]: Check all BAM references md5-hash to this (None = no check)\n",
    "            minscore [None]: Minimum alignment score (AS field) to consider\n",
    "            minlength [None]: Ignore any references shorter than N bases\n",
    "            minid [None]: Discard any reads with nucleotide identity less than this\n",
    "            subprocesses [8]: Number of subprocesses to spawn\n",
    "            logfile: [None] File to print progress to\n",
    "\n",
    "        Output: A (n_contigs x n_samples) Numpy array with RPKM\n",
    "\n",
    "We can see (in the default value for the `subprocesses` argument to `read_bamfiles`) that the default number of parallel BAM-reading processes it will spawn is 8. This is because Python detected 8 threads on my laptop. In general, Vamb's default here is to use the number of availble threads, or 8 threads if more than 8 is detected, as the BAM-reading function will almost certainly become IO bound at more than 8 threads.\n",
    "\n",
    "As with the `vamb.parsecontigs.read_contigs` function, I don't care about the `minlength` argument, since our fasta file is already filtered. Again, I will re-iterate that filtering the FASTA file _before_ mapping leads to the best results.\n",
    "\n",
    "The function ignores all alignments with alignment score less than `minscore` (as determined by the auxiliary `AS:i` field in the BAM file, which Vamb assumes is present if `minscore` is not `None`). Ideally, the user should construct the BAM files so that they only contain alignments that the user believes are true (i.e. set reasonable alignment and filtering criteria).\n",
    "\n",
    "Similarly to `minscore`, this function has a `minid` option. The parser will ignore any alignments where the nucleotide identity fraction (i.e. in [0;1)) is smaller than `minid` across the entire alignment.\n",
    "\n",
    "The argument `refhash` is used to verify that the headers of the FASTA file given to VAMB are identical and in the same order as that of the BAM files. If this is `None`, no check is done. Else, this should be a length 16 `bytes` object, representing the md5 hash of the reference sequences in the FASTA file as calculated by `vamb.parsebam._hash_refnames`. If the hash of the references in any BAM file does not match his hash, Vamb raises an exception. Setting the flag `--norefcheck` when using Vamb on command-line sets `refhash` to `None` .\n",
    "\n",
    "Lastly, the argument `logfile` should be `None` or the filehandle of an opened, writeable file. If the latter, it will print status updates to the logfile.\n",
    "\n",
    "---\n",
    "Now, I tend to be a bit ~~paranoid~~<sup>careful</sup>, so if I loaded in 500 GB of BAM files, I'd want to save the work I have now in case something goes wrong - and we're about to fire up the VAE so lots of things can go wrong.\n",
    "\n",
    "What importants objects do I have in memory right now?\n",
    "\n",
    "* `tnfs`: A Numpy array of tnfs\n",
    "* `contignames`: A list of contignames\n",
    "* `lengths`: A Numpy array of contig lengths\n",
    "* `rpkms`: A Numpy array of rpkms\n",
    "\n",
    "I'm going to use `vamb.vambtools.write_npz` to save the Numpy arrays (that function is just a thin convenience wrapper for `numpy.savez_compressed`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open('/Users/jakni/Downloads/example/contignames.npz', 'wb') as file:\n",
    "    vamb.vambtools.write_npz(file, np.array(contignames))\n",
    "\n",
    "with open('/Users/jakni/Downloads/example/lengths.npz', 'wb') as file:\n",
    "    vamb.vambtools.write_npz(file, lengths)\n",
    "\n",
    "with open('/Users/jakni/Downloads/example/tnfs.npz', 'wb') as file:\n",
    "    vamb.vambtools.write_npz(file, tnfs)\n",
    "    \n",
    "with open('/Users/jakni/Downloads/example/rpkms.npz', 'wb') as file:\n",
    "    vamb.vambtools.write_npz(file, rpkms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"encode\"></a>\n",
    "## Train the autoencoder and encode input data\n",
    "\n",
    "Again, you can use `help` to see how to use the module\n",
    "\n",
    "`>>> help(vamb.encode)`\n",
    "\n",
    "    Help on module vamb.encode in vamb:\n",
    "\n",
    "    NAME\n",
    "        vamb.encode - Encode a depths matrix and a tnf matrix to latent representation.\n",
    "\n",
    "    DESCRIPTION\n",
    "        Creates a variational autoencoder in PyTorch and tries to represent the depths\n",
    "        and tnf in the latent space under gaussian noise.\n",
    "\n",
    "        Usage:\n",
    "        >>> vae = VAE(nsamples=6)\n",
    "        >>> dataloader, mask = make_dataloader(depths, tnf)\n",
    "        >>> vae.trainmodel(dataloader)\n",
    "        >>> latent = vae.encode(dataloader) # Encode to latent representation\n",
    "        >>> latent.shape\n",
    "        (183882, 40)\n",
    "        \n",
    "    [ lines elided ]\n",
    "    \n",
    "---\n",
    "Aha, so we need to create the VAE, create the dataloader (and the mask), then use the `trainmodel` method first, then the `VAE.encode` method. You can call the `help` functions on those, but I'm not showing that here.\n",
    "\n",
    "Training networks always take some time. If you have a GPU and CUDA installed, you can pass `cuda=True` to the VAE to train on your GPU for increased speed. With a beefy GPU, this can make quite a difference. I run this on my laptop (with a puny Intel GPU), so I'll just use my CPU. And I'll run just 10 epochs rather than the more suitable 500:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNetwork properties:\n",
      "\tCUDA: False\n",
      "\tAlpha: 0.15\n",
      "\tBeta: 200\n",
      "\tDropout: 0.2\n",
      "\tN hidden: 512, 512\n",
      "\tN latent: 32\n",
      "\n",
      "\tTraining properties:\n",
      "\tN epochs: 10\n",
      "\tStarting batch size: 64\n",
      "\tBatchsteps: None\n",
      "\tLearning rate: 0.001\n",
      "\tN sequences: 50658\n",
      "\tN samples: 6\n",
      "\n",
      "\tEpoch: 1\tLoss: 0.462706\tCE: 0.7746411\tSSE: 71.914618\tKLD: 101.7787\tBatchsize: 64\n",
      "\tEpoch: 2\tLoss: 0.370264\tCE: 0.6185148\tSSE: 55.935485\tKLD: 96.9660\tBatchsize: 64\n",
      "\tEpoch: 3\tLoss: 0.361013\tCE: 0.6056179\tSSE: 53.576321\tKLD: 93.5645\tBatchsize: 64\n",
      "\tEpoch: 4\tLoss: 0.355792\tCE: 0.5986304\tSSE: 52.130304\tKLD: 91.5784\tBatchsize: 64\n",
      "\tEpoch: 5\tLoss: 0.352974\tCE: 0.5950006\tSSE: 51.313760\tKLD: 90.3216\tBatchsize: 64\n",
      "\tEpoch: 6\tLoss: 0.350003\tCE: 0.5909698\tSSE: 50.590515\tKLD: 88.6565\tBatchsize: 64\n",
      "\tEpoch: 7\tLoss: 0.347771\tCE: 0.5880336\tSSE: 50.021146\tKLD: 87.3046\tBatchsize: 64\n",
      "\tEpoch: 8\tLoss: 0.345614\tCE: 0.5847925\tSSE: 49.599768\tKLD: 86.3096\tBatchsize: 64\n",
      "\tEpoch: 9\tLoss: 0.344622\tCE: 0.5836782\tSSE: 49.175052\tKLD: 86.3465\tBatchsize: 64\n",
      "\tEpoch: 10\tLoss: 0.342854\tCE: 0.5808163\tSSE: 48.930381\tKLD: 85.4460\tBatchsize: 64\n"
     ]
    }
   ],
   "source": [
    "vae = vamb.encode.VAE(nsamples=rpkms.shape[1])\n",
    "dataloader, mask = vamb.encode.make_dataloader(rpkms, tnfs)\n",
    "\n",
    "with open('/tmp/model.pt', 'wb') as modelfile:\n",
    "    vae.trainmodel(dataloader, nepochs=10, modelfile=modelfile, batchsteps=None, logfile=sys.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create the VAE, then we create the dataloader and the mask. The dataloader normalizes the TNF such that the mean and standard deviation for each tetranucleotide across all contigs (i.e. a column) is 0 and 1, respectively, and normalizes the rpkm such that each contig (i.e. a row) sums to 1. Furthermore, the dataloader shuffles the contigs at each epoch.\n",
    "\n",
    "The dataloader also discards all contigs where either the TNF vector or the depths vector is all zeros - we call these contigs *zero contigs*. A zero contig implies that it is either shorter than 4 basepairs, or that no reads mapped to it from any sample. The `mask` it returns is a boolean vector with `True` if a contig was kept, and `False` if it was discarded. We began with 50658 contigs, and the log above states that it was trained with 50658 contigs, implying that 50658 - 50658 = 0 contigs were discarded.\n",
    "\n",
    "Here, we kept the default value `False` to the `destroy` keyword of `make_dataloader`. If this is set to `True`, the input arrays are normalized and masked in-place, modifying them. This prevents another in-memory copy of the data, which can be critical for large array sizes.\n",
    "\n",
    "I passed a few keywords to the `trainmodel` function: \n",
    "* `nepochs` should be obvious - the number of epochs trained\n",
    "* `modelfile` will be discussed in a bit\n",
    "* `logfile` is the file to which the progress text will be printed. Here, I passed `sys.stdout` in order to print the progress to this notebook\n",
    "* `batchsteps` should be `None`, or an iterable of integers. If `None`, it does nothing. Else, the batch size will double when it reaches an epoch in `batchsteps`. Increasing batch size helps find better minima; see the paper \"Don't Decay the Learning Rate, Increase the Batch Size\".\n",
    "\n",
    "The VAE encodes the high-dimensional (n_samples + 136 features) input data in a lower dimensional space (nlatent features). When training, it learns an encoding scheme, with which it encodes the input data to a series of normal distributions, and a decoding scheme, in which it uses one value sampled from each normal distribution to reconstruct the input data.\n",
    "\n",
    "The theory here is that if the VAE learns to reconstruct the input, the distributions must be a more efficient encoding of the input data, since the same information is contained in fewer neurons. If the input data for the contigs indeed do fall into bins, an efficient encoding would be to simply encode the bin they belong to, then use the \"bin identity\" to reconstruct the data. We force it to encode to *distributions* rather than single values because this makes it more robust - it will not as easily overfit to interpret slightly different values as being very distinct if there is an intrinsic noise in each encoding.\n",
    "\n",
    "### The loss function\n",
    "\n",
    "The loss of the VAE consists of three major terms:\n",
    "\n",
    "* Cross entropy (CE) measures the dissimilarity of the reconstructed abundances to observed abundances. This penalizes a failure to reconstruct the abundances accurately.\n",
    "* Sum of squared error (SSE) measures the dissimilary of reconstructed versus observed TNF. This penalizes failure to reconstruct TNF accurately.\n",
    "* Kullback-Leibler divergence (KLD) measures the dissimilarity between the encoded distributions and the standard gaussian distribution N(0, 1). This penalizes learning.\n",
    "\n",
    "All three terms are important. CE and SSE is necessary, because we believe the VAE can only learn to effectively reconstruct the input if it learns to encode the signal from the input into the latent layers. In other words, these terms incentivize the network to learn something. KLD is necessary because we care that the encoding is *efficient*, viz. it is contained in as little information as possible. The entire point of encoding is to encode a majority of the signal while shedding the noise, and this is only achieved if we place contrains on how much the network is allowed to learn. Without KLD, the network can theoretically learn an infinitely complex encoding, and the network will learn to encode both noise and signal.\n",
    "\n",
    "In normal autoencoders, people use binary cross-entropy rather than crossentropy. We believe crossentropy is more correct here, since we normalize by letting the depths sum to one across a contig. In this way, you can view the depths distribution across samples as a probability distribution that a random mapping read will come from each sample.\n",
    "\n",
    "In `encode.py`, the loss function is written as:\n",
    "\n",
    "$L = \\frac{(1 - \\alpha)}{ln(S)} \\cdot CE + \\frac{\\alpha}{136} \\cdot SSE + \\frac{1}{N_{L}\\beta} \\cdot KLD$\n",
    "\n",
    "where $N_{L}$ is number of latent neurons and S is number of samples. It is hardly obvious where the scaling factors come from, so let me try to explain it.\n",
    "\n",
    "As the learning rate is fixed and optimized for a specific gradient, this means the total reconstruction loss $\\frac{(1 - \\alpha)}{ln(S)} \\cdot CE + \\frac{\\alpha}{136} \\cdot SSE$ should sum to a constant, lest the training become ustable. To make things simpler, we want it to sum to 1. It would probably be more precise to say that L should be 1, but since $\\frac{1}{N_{L}\\beta} \\cdot KLD <<  \\frac{(1 - \\alpha)}{ln(S)} \\cdot CE + \\frac{\\alpha}{136} \\cdot SSE$ for any values of $\\alpha$ and $\\beta$ that seem to work, setting the reconstruction loss to 1 is basically the same.\n",
    "\n",
    "When optimizing the network, we want a single variable to control the ratio between $SSE$ and $CE$ - we call this $\\alpha$. This scales CE and SSE so that $\\alpha = \\frac{SSE}{SSE + CE}$ \n",
    "\n",
    "But here comes a problem. While we want to scale SSE and CE so that the two constrains above (namely $CE+SSE=1$ and  $\\alpha = \\frac{SSE}{SSE + CE}$) are true, we can't know *beforehand* what CE, KLD or SSE is. And, in any rate, these values changes across the training run (that's the point of training!).\n",
    "\n",
    "What we *can* reason about is the values of CE and SSE in a totally *naive*, network which had *no knowledge* of the input dataset. This represents the state of the network before *any* learning is done. What would such a network predict? Well, since we normalize the reconstructed depths across $S$ samples to be between 0 and 1 and sum to 1, the outputs would be close to $[S^{-1}, S^{-1} ... S^{-1}]^{T}$, which, by the definition of cross entropy, would yield a CE of $ln(S)$. We normalize both TNF inputs and outputs to follow an approximately normal distribution with mean around 0, which means the expected SSE for TNF is 1 per input TNF neuron, for a total of $SSE = 136$.\n",
    "\n",
    "Importantly, if we actually check the CE and SSE values for untrained networks, they are quite close to these expected values of $ln(S)$ and 136. Since neither CE nor SSE is reduced by more than an order of magnitude from their starting values during training on realistic datasets (usually much less), these expected values work as stand-ins for what we can expect CE and SSE to be across training.\n",
    "\n",
    "So the purpose of the scaling factors in front of the CE and SEE terms is to scale CE from the expected value of $ln(S)$ to the target value of (1-$\\alpha$) and SSE from 136 to $\\alpha$. Hence the scaling factors $\\frac{(1 - \\alpha)}{ln(S)}$ and $\\frac{\\alpha}{136}$.\n",
    "\n",
    "For KLD, we want the user to be able to control the ratio $\\frac{C+S}{K}$, where C, S and K are each of the terms from CE, SSE and KLD in the loss, respectively. Since KLD is defined as a sum of individual KLD for each of latent neurons, its value is proportional to $N_{L}$. So we let the user set a ratio\n",
    "\n",
    "$\\beta = \\frac{KLD}{N_{L}K} \\cdot \\frac{(1 - \\alpha)}{ln(S)} \\cdot CE + \\frac{\\alpha}{136} \\cdot SSE$,\n",
    "\n",
    "Where again, K is the weighed KLD in the loss. In other words, we allow the user to weigh the KLD-related loss relative to the reconstruction loss. However, as we just scaled the CE and SSE terms to make sure they sum to one, this simplifies to:\n",
    "\n",
    "$\\beta = \\frac{KLD}{N_{L}K} \\Leftrightarrow K = \\frac{1}{N_{L}\\beta} \\cdot KLD$.\n",
    "\n",
    "Hence the scaling factor $\\frac{1}{N_{L}\\beta}$ in front of KLD.\n",
    "\n",
    "If you look at the outputs from the 10 epochs, you can see the KL-divergence rises the first epoch as it learns the dataset and the latent layer drifts away from its prior. At epoch 2, the penalty associated with KL-divergence outweighs the CE and SSE losses. At this point, the KL will stall, and then fall. This point depends on $\\beta$ and the complexity of the dataset.\n",
    "\n",
    "Okay, so now we have the trained `vae` and gotten the `dataloader`. Let's feed the dataloader to the VAE in order to get the latent representation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50658, 32)\n"
     ]
    }
   ],
   "source": [
    "# No need to pass gpu=True to the encode function to encode on GPU\n",
    "# If you trained the VAE on GPU, it already resides there\n",
    "latent = vae.encode(dataloader)\n",
    "\n",
    "print(latent.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "That's 50658 contigs each represented by the mean of their latent distribution.\n",
    "\n",
    "Sometimes, you'll want to reuse a VAE you have already trained. For this, I've added the `VAE.save` method of the VAE class, as well as a `VAE.load` method. You will have noticed in the training example above that I defined a `modelfile`, a file the VAE will create and save its parameters to. We can always use that file to recreate the VAE and have a pretrained model. But remember - a trained VAE only works on the dataset it's been trained on, and not necessarily on any other!\n",
    "\n",
    "I want to **show** that we get the exact same network back that we trained, so here I encode the first contig, delete the VAE, reload the VAE and encode the first contig again. The two encodings should be identical.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 28.7819,  33.2829,   5.2399,  -5.7424,  42.9069,  24.8012,  -9.8265,\n",
      "        -29.8857,  29.9306,  -3.5239,  54.1909,  -3.8932,  49.2182,  -2.4263,\n",
      "        -45.1241,  35.4750,  45.4890,  28.1559, -37.8290,  16.4073, -32.6728,\n",
      "         28.5140, -12.4314, -20.3771, -24.6758,   9.1269, -37.3544,   2.3948,\n",
      "        -43.6156,  61.9900,   6.2090,   9.4166], grad_fn=<SelectBackward>)\n",
      "tensor([ 28.7819,  33.2829,   5.2399,  -5.7424,  42.9069,  24.8012,  -9.8265,\n",
      "        -29.8857,  29.9306,  -3.5239,  54.1909,  -3.8932,  49.2182,  -2.4263,\n",
      "        -45.1241,  35.4750,  45.4890,  28.1559, -37.8290,  16.4073, -32.6728,\n",
      "         28.5140, -12.4314, -20.3771, -24.6758,   9.1269, -37.3544,   2.3948,\n",
      "        -43.6156,  61.9900,   6.2090,   9.4166], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Manually create the first mini-batch without shuffling\n",
    "rpkms_in = torch.Tensor(rpkms[:128]).reshape((128, -1))\n",
    "tnfs_in = torch.Tensor(tnfs[:128]).reshape((128, -1))\n",
    "\n",
    "# Put VAE in testing mode - strictly not necessary here, since it goes in test\n",
    "# mode when encoding latent, as we did above\n",
    "vae.eval()\n",
    "\n",
    "# Calling the VAE as a function encodes and decodes the arguments,\n",
    "# returning the outputs and the two distribution layers\n",
    "depths_out, tnf_out, mu, logsigma = vae(rpkms_in, tnfs_in)\n",
    "\n",
    "# The mu layer is the encoding itself\n",
    "print(mu[0])\n",
    "\n",
    "# Now, delete the VAE\n",
    "del vae\n",
    "\n",
    "# And reload it from the model file we created:\n",
    "# Here we can specify whether to put it on GPU and whether it should start\n",
    "# in training or evaluation (encoding) mode. By default, it's not on GPU and \n",
    "# in testing mode\n",
    "vae = vamb.encode.VAE.load('/tmp/model.pt')\n",
    "depths_out, tnf_out, mu, logsigma = vae(rpkms_in, tnfs_in)\n",
    "print(mu[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We get the same values back, meaning the saved network is the same as the loaded network!\n",
    "\n",
    "<a id=\"cluster\"></a>\n",
    "## Binning the encoding\n",
    "\n",
    "__The role of clustering in Vamb__\n",
    "\n",
    "Fundamentally, the process of binning is just clustering sequences based on some of their properties, with the expectation that sequences from the same organism cluster together.\n",
    "\n",
    "In the previous step, we encoded each sequence to a vector of real numbers. The idea behind encoding is that, because the neural network attempts to strike a balance between having low reconstruction error (i.e. high fidelity) and having low Kullback-Leibler divergence (i.e. containing as little information as possible), then the network will preferentially encode signal over noise. In this way, the VAE act like a denoiser. This has been explored in other papers by people before us. Furthermore, because the latent space has fewer dimensions than the input space, clustering is quicker and more memory efficient.\n",
    "\n",
    "With the latent representation conveniently represented by an (n_contigs x n_features) matrix, you could use any clustering algorithm to cluster them (such as the ones in the Python package `sklearn.cluster`). In practice though, you have perhaps a million contigs and prior constrains on the diameter, shape and size of the clusters, so non-custom clustering algorithms will probably be slow and inaccurate.\n",
    "\n",
    "The module `vamb.cluster` implements a iterative medoid clustering algorithm. The algorithm is complicated and not very elegant, so I will go through it here in painstaking detail.\n",
    "\n",
    "### Overview of Vamb clustering algorithm\n",
    "\n",
    "In very broad strokes, the algorithm works like this:\n",
    "\n",
    "    A) Begin with an arbitrary sequence P, and move from P to a center of a nearby cluster, called the medoid M.\n",
    "    B) Find the distance d from M where the density of other points drops according to certain criteria.\n",
    "    C) If no d can be found, restart from A). If it has restarted too often the last X tries, relax criteria in B). If criteria has been maximally relaxed, choose an arbitrary value of d.\n",
    "    D) Output all points within d of M, and remove them from dataset.\n",
    "    E) Repeat from A) until no points remain in the dataset.\n",
    "\n",
    "The idea behind this algorithm is that we expect the clusters to be roughly spherical, have a high density near the middle, and be separed form other clusters by an area with lower density. Hence, we should be able to move from P to M in step A) by moving towards higher density. And we should be able to find the radius of the cluster from M by considering a distance from M where the density of points drops.\n",
    "\n",
    "This algorithm consists of several steps, discussed below. Most of the steps unfortunately have several parameters which have been chosen by a combination of guessing, gut feeling and testing.\n",
    "\n",
    "As a distance measure, we are using cosine distance. This is particularly useful for a couple of reasons:\n",
    "\n",
    "First, it is extremely quick to calculate. Given two points represented by vectors $\\textbf{a}$ and $\\textbf{b}$, we define cosine distance as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{1}{2} - \\frac{\\textbf{a} \\cdot \\textbf{b}}{2|\\textbf{a}| |\\textbf{b}|}\n",
    "\\end{equation*}\n",
    "\n",
    "This is slightly different from the ordinary definition, because it is squeezed between 0 and 1, rather than -1 and 1.\n",
    "\n",
    "Now if we preprocess all points by calculating:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\bar{\\textbf{a}} = \\textbf{a} \\circ \\frac{1}{\\sqrt{2} \\textbf{|a|}}\n",
    "\\end{equation*}\n",
    "\n",
    "Where $\\circ$ is elementwise multiplication, then the cosine distance is simply:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{1}{2} - \\bar{\\textbf{a}} \\cdot \\bar{\\textbf{b}}\n",
    "\\end{equation*}\n",
    "\n",
    "And since the points are represented by a matrix $\\bar{\\textbf{M}}$, with each row vector being a point, finding the distance to from the i'th point of $\\bar{\\textbf{M}}$ to all other points is then a series of vector products as in the equation above, which can be expressed in a single matrix multiply and subtraction:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{1}{2} - \\bar{\\textbf{M}} \\times \\bar{\\textbf{M}}_{i}^{T}\n",
    "\\end{equation*}\n",
    "\n",
    "Second, with all the $N_{latent}$ dimensions being used by the neural network the surface of an $N_{latent}$-dimensional hyperphere is so large that it's not likely that multiple clusters are far in euclidian space but close in cosine space.\n",
    "\n",
    "And third, it works better than the other alternatives we tried in practice, although I don't understand why for instance Euclidian distance is not better.\n",
    "\n",
    "### Finding the medoid\n",
    "\n",
    "First, we shuffle the points pseudorandomly to avoid any bias introduced by the order of the sequences. Then, we use the following algorithm to find the medoid. The idea is that, as you move nearer to a cluster's centre, the mean distance to close points will decrease.\n",
    "\n",
    "Currently, the default parameters DC and MAX_STEPS are 0.05 and 25, respectively.\n",
    "\n",
    "Function `wander_medoid`\n",
    "    1. Pick next point S as the seed. If last point has been reached, begin from top again.\n",
    "    2. Let C be the set of contigs within a small distance DC of S\n",
    "    3. Calculate mean distance of points in C to S, ignoring the distance of S to itself. Let that be mean_S.\n",
    "    4. Sample a point P in C. While you have not yet futilely sampled MAX_STEPS times or exhaused all points in C:\n",
    "        Calculate mean distance of P to all points within DC of P, let that be mean_P\n",
    "        If mean_P < mean_S, set S to P and go to point 2.\n",
    "    7. Return S as medoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the right distance\n",
    "\n",
    "Having found a medoid above, we need to find out if it's the center of a suitable cluster, and if so, what the radius of that cluster is.\n",
    "\n",
    "In order to see how this is done, it is useful to look at these graphs first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.10000000000000002, 97.86, 'Clustering threshold')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot(xs, ys, title, xlabel, ylabel):\n",
    "    figure = plt.figure()\n",
    "    plt.plot(xs, ys, color='k')\n",
    "    plt.tick_params(left=False, labelleft=False, labelsize=13)\n",
    "    plt.ylim(0, plt.ylim()[1])\n",
    "    plt.xlim(0, xs.max())\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel(xlabel, fontsize=14)\n",
    "    plt.ylabel(ylabel, fontsize=14)\n",
    "    return figure\n",
    "\n",
    "histogram = vamb.vambtools.read_npz('histogram.npz')\n",
    "xs = np.linspace(0, 1, 400)\n",
    "fig_a = plot(xs[1:], histogram, \"Figure A: Histogram of all distances\", 'distance', 'density')\n",
    "hist2 = histogram[0:120:2] + histogram[1:120:2]\n",
    "fig_b = plot(xs[0:110:2], hist2[:55], \"Figure B: Zoom-in on low X values\", 'distance', 'density')\n",
    "densities = vamb.cluster._calc_densities(hist2, cuda=False)\n",
    "plt.plot(xs[0:110:2], densities[:55])\n",
    "threshold, success = vamb.cluster._find_threshold(densities, peak_valley_ratio=0.2, default=0.09, cuda=False)\n",
    "plt.vlines(threshold, 0, plt.ylim()[1], linestyles='dotted')\n",
    "plt.text(threshold, plt.ylim()[1]/5, \"Clustering threshold\", rotation=90, fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When picking an arbitrary contig as medoid (here contig 'S4C11236' in the CAMI2 toy human Airways short read dataset), and calculating the distances to all other contigs in the dataset, it follows a distribution as seen in Figure A. Unsurprisingly, the latent representation of most other contigs are mostly uncorrelated with the chosen contig, so the large majority of points are at a distance of around 0.5. No contigs have a larger distance than 0.8. \n",
    "\n",
    "But look at the lower left corner of Figure A: A small group of contigs appear to have a smaller distance to the medoid than most others - those of the cluster it belongs to. If the cluster is well-separated, this small initial peak should be mostly isolated from the main peak around 0.5 by a band of empty space, with a low density. Figure B confirms this is true.\n",
    "\n",
    "Vamb groups the observed distances from the medoid M to a histogram similar to that in Figure B, and then iterates over the y-values of the histogram attempting to find first an initial peak, then a valley. At the bottom of the valley, the cutoff distance threshold is set, and all points closer to M than this point is clustered out. This threshold is depicted in Figure B by the dotted line. Vamb smoothes the histogram in order to mitigate the influence of random variation on the minima/maxima of the function. The smoothed hisotgram is visible in Figure B as a blue line.\n",
    "\n",
    "The algorithm for detecting the threshold is as follows:\n",
    "\n",
    "Parameter DEFAULT is 0.09 by default. RATIO is provided to the function and will be described later.\n",
    "\n",
    "    Function `find_threshold` inputs = (medoid, RATIO), outputs = (threshold, was_successful)\n",
    "    1. Calculate the distances from M to all other points. Group to histogram with bin size of 0.005 and smooth using a simple Gaussian kernel.\n",
    "    2. If there are no other points within 0.03, return (0.025, None) end function\n",
    "    3. Set success to FALSE. For X = 0 to X = 0.3:\n",
    "        If the peak has not yet been declared over:\n",
    "            If density is increasing:\n",
    "                Set peak to X, peak density to density\n",
    "                If X > 0.1, there is no initial peak near X=0, end loop\n",
    "            Else if density < 0.6 * peak density\n",
    "                Declare initial peak to be over\n",
    "                Set minimum to density\n",
    "        Else\n",
    "            If density > 1.5 * minimum, we are entering another peak, end loop\n",
    "            If density < minimum\n",
    "                Set minimum to density\n",
    "                if density < RATIO * peak density, we have successfully found a potential threshold\n",
    "                    set success to TRUE\n",
    "                    set threshold to X\n",
    "                    \n",
    "    4. If threshold is set to above 0.14 + 0.1 * RATIO: Initial peak is not close enough to X=0, set success to FALSE\n",
    "    5. If success is FALSE and RATIO > 0.55: We do not accept failure. Set success to None and threshold to DEFAULT\n",
    "    6. Return (threshold, success)\n",
    "    \n",
    "This function is annoyingly convoluted, but I was not able to find a simpler function that did not return nonsensical thresholds for a variety of different data points.\n",
    "\n",
    "You will notice that the parameter RATIO controls how strict the criteria for accepting a threshold as successful is. If RATIO is low, say, 0.1, the initial peak must lie close to zero, and must be separated from the bulk of the other points with a deep valley. In constrast, a larger RATIO makes the criteria more relaxed. With a RATIO > 0.55, the function does not even accept failure, and will set the threshold to DEFAULT if no valley is found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "With the medoid search function and the threshold detection above, you can see how to begin clustering points. However, one more problem remains: Not all medoids separate the points to a neat small peak of close contigs and a larger one of far contigs as seen in Figure B. In fact, for the majority of medoids M sampled, the density is continuously increasing with distance, or the initial peak is too far away to consider M to be part of the peak, or there is clearly an initial double-peak rather than one. To make things worse, if a medoid M cannot successfully return a threshold, this does not mean M must be discarded - it is quite likely that M is contained in a cluster for which a threshold could be successfully found using another point as medoid.\n",
    "\n",
    "To handle this problem, Vamb takes the approach of beginning with strict requirements for when a threshold is accepted, by setting the RATIO parameter low. If the threshold detection for a medoid returns failure, that medoid is simply skipped in the hope of finding another medoid with better results. Vamb then keeps count of how many of the recent attempts that yielded success - if too few did, RATIO is increased to relax the requirements for clusters.\n",
    "\n",
    "The algorithm is as follows:\n",
    "\n",
    "Parameters WINDOWSIZE and MINSUCCESSES is 200 and 15 by default, respectively. RATIO is initialized as 0.1\n",
    "\n",
    "Function `cluster`\n",
    "    1. Pick a medoid M using function `wander_medoid`\n",
    "    2. Set threshold to None. While threshold is None:\n",
    "           Set (success, threshold) to result of function `find_threshold(M, RATIO)`\n",
    "           If fewer than MINSUCCESSES of last WINDOWSIZE attempts at finding threshold succeded:\n",
    "               Increment RATIO by 0.1\n",
    "               Discard the memory of the success or failure of previous attempts\n",
    "    3. Output all points within threshold of M as a cluster. Remove these points from dataset\n",
    "    4. If no more points remain, end function. Else, go to point 1.\n",
    "    \n",
    "Let's have a look at the actual Python function that does the clustering in Vamb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function cluster in module vamb.cluster:\n",
      "\n",
      "cluster(matrix, labels=None, maxsteps=25, windowsize=200, minsuccesses=20, default=0.09, destroy=False, normalized=False, logfile=None, cuda=False)\n",
      "    Iterative medoid cluster generator. Yields (medoid), set(labels) pairs.\n",
      "    \n",
      "    Inputs:\n",
      "        matrix: A (obs x features) Numpy matrix of data type numpy.float32\n",
      "        labels: None or Numpy array/list with labels for seqs [None = indices]\n",
      "        maxsteps: Stop searching for optimal medoid after N futile attempts [25]\n",
      "        default: Fallback threshold if cannot be estimated [0.09]\n",
      "        windowsize: Length of window to count successes [200]\n",
      "        minsuccesses: Minimum acceptable number of successes [15]\n",
      "        destroy: Save memory by destroying matrix while clustering [False]\n",
      "        normalized: Matrix is already preprocessed [False]\n",
      "        logfile: Print threshold estimates and certainty to file [None]\n",
      "    \n",
      "    Output: Generator of (medoid, set(labels_in_cluster)) tuples.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(vamb.cluster.cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "An explanation of a few of the parameters is in order:\n",
    "    \n",
    "`matrix` is the latent encoding.\n",
    "\n",
    "`labels` is an array or a list of the name of each sequence, such that the i'th row of `matrix` has the label `labels[i]`\n",
    "\n",
    "`maxsteps`, `default`, `windowsize`, and `minsuccesses` correspond to the parameters described in the algorithms above. It's not realistic to expect users to set these - in other words, they should be OK as they are.\n",
    "\n",
    "The function iteratively deletes the input matrix. To avoid this, the function works on a copy of the matrix. To skip copying the matrix and save memory, set `destroy` to `True`.\n",
    "\n",
    "For clustering, the matrix needs to be preprocessed (as described above in the clustering algorithm). If this has already been done, set `normalized` to `True`. If `destroy` is `True`, the matrix will be normalized in-place.\n",
    "\n",
    "If `cuda` is set to `True`, the clustering will be run on GPU for a significant speedup.\n",
    "\n",
    "---\n",
    "Depending on the size of the latent encoding, clustering can take quite some time. The heavy lifting here is done in PyTorch, so if you're not using a GPU, it might be worth making sure the BLAS library PyTorch is using is fast. The difference between a fast and a slow PyTorch implementation can be quite remarkable. If you use PyTorch v. >= 1.1 You can check it with `torch.__config__.show()` and notice if it says something about using Intel Math Kernel (MKL) library, or OpenBLAS. If it does, you're golden.\n",
    "\n",
    "The output of `vamb.cluster.cluster` is a generator, yielding `(medoid, cluster)` tuples. The generator will compute the clusters on-the-fly, meaning it will only compute the next cluster *once you ask for it*. Having the clustering return a generator gives a lot of flexibility:\n",
    "\n",
    "You can manually iterate over the clusters:\n",
    "    \n",
    "    clusters = dict()\n",
    "    for n, (medoid, cluster) in enumerate(vamb.cluster.cluster(latent)):\n",
    "        clusters[medoid] = cluster\n",
    "        \n",
    "        if n + 1 == 1000: # Stop after 1000 clusters\n",
    "            break\n",
    "            \n",
    "You can just put it directly in a dictionary:\n",
    "\n",
    "    clusters = dict(vamb.cluster.cluster(latent))\n",
    "    \n",
    "Or you can use the `vamb.cluster.writeclusters` function to write the clusters to disk without storing them in memory:\n",
    "\n",
    "    cluster_iterator = vamb.cluster.cluster(latent)\n",
    "    with open('clusters.tsv', 'w') as clusterfile:\n",
    "        vamb.cluster.writeclusters(clusterfile, cluster_iterator)\n",
    "        \n",
    "In this example, we will load it into a dictionary immediately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First key: 196_NODE_5297_length_2508_cov_6.88045 (of type: <class 'numpy.str_'> )\n",
      "Type of values: <class 'set'>\n",
      "First element of value: 196_NODE_5297_length_2508_cov_6.88045 of type: <class 'numpy.str_'>\n"
     ]
    }
   ],
   "source": [
    "# I cheat here and load in a latent representation that has trained for 500\n",
    "# epochs instead of the shorter training we did above.\n",
    "latent = vamb.vambtools.read_npz('/Users/jakni/Downloads/example/out/latent.npz')\n",
    "\n",
    "# Notice we mask the contignames, since the dataloader could have filtered some contigs away\n",
    "cluster_iterator = vamb.cluster.cluster(latent, labels=np.array(contignames)[mask])\n",
    "clusters = dict(cluster_iterator)\n",
    "\n",
    "medoid, contigs = next(iter(clusters.items()))\n",
    "print('First key:', medoid, '(of type:', type(medoid), ')')\n",
    "print('Type of values:', type(contigs))\n",
    "print('First element of value:', next(iter(contigs)), 'of type:', type(next(iter(contigs))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"postprocessing\"></a>\n",
    "## Postprocessing the clusters\n",
    "\n",
    "We haven't written any dedicated postprocessing modules because how to postprocess really depends on what you're looking for in your data.\n",
    "\n",
    "One of the greatest weaknesses of Vamb - probably of metagenomic binners in general - is that the bins tend to be highly fragmented. Unlike other binners, Vamb does not hide this fact, and will bin *all* input contigs. This means you'll have lots of tiny bins, some of which are legitimate (viruses, plasmids), but most are parts of larger genomes that didn't get binned properly - about 2/3 of the bins here, for example, are 1-contig bins. \n",
    "\n",
    "We're in the process of developing a tool for annotating, cleaning and merging bins based on phylogenetic analysis of the genes in the bins. That would be extremely helpful, but for now, we'll have to use more crude approaches.\n",
    "\n",
    "First, we will split up all bins by their sample of origin using `vamb.vambtools.binsplit`. This will cause some bins to be fragmented, but for bins of high per-sample coverage, it will deduplicate them effectively. Second, we'll throw away all bins with less than 200,000 basepairs because we're only interested in genome-sized bins.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bins before splitting and filtering: 2567\n",
      "Number of bins after splitting and filtering: 161\n"
     ]
    }
   ],
   "source": [
    "def filterclusters(clusters, lengthof):\n",
    "    filtered_bins = dict()\n",
    "    for medoid, contigs in clusters.items():\n",
    "        binsize = sum(lengthof[contig] for contig in contigs)\n",
    "    \n",
    "        if binsize >= 200000:\n",
    "            filtered_bins[medoid] = contigs\n",
    "    \n",
    "    return filtered_bins\n",
    "        \n",
    "lengthof = dict(zip(contignames, lengths))\n",
    "filtered_bins = filterclusters(vamb.vambtools.binsplit(clusters, '_'), lengthof)\n",
    "print('Number of bins before splitting and filtering:', len(clusters))\n",
    "print('Number of bins after splitting and filtering:', len(filtered_bins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Even as we split most bins in 6, we're still left with fewer than 1/15 of the starting number of bins! Now, let's save the clusters to disk. For this we will use two writer functions:\n",
    "\n",
    "1) `vamb.cluster.writeclusters`, that writes which clusters contains which contigs to a simple tab-separated file, and\n",
    "\n",
    "2) `vamb.vambtools.writebins`, that writes FASTA files corresponding to each of the bins to a directory. This might be useful for some types of analysis you want to do down the road.\n",
    "\n",
    "We will need to load all the contigs belonging to any bin into memory to use `vamb.vambtools.writebins`. If the contigs in your bins don't fit in memory, sorry, you gotta find another way to make those FASTA bins.\n",
    "\n",
    "The cluster name when printing either way will be the dictionary key of the bins.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This writes a .tsv file with the clusters and corresponding sequences\n",
    "with open('/Users/jakni/Downloads/example/clusters.tsv', 'w') as file:\n",
    "    vamb.cluster.write_clusters(file, filtered_bins)\n",
    "\n",
    "# Only keep contigs in any filtered bin in memory\n",
    "keptcontigs = set.union(*filtered_bins.values())\n",
    "\n",
    "with open('/Users/jakni/Downloads/example/contigs.min2kbp.fna', 'rb') as file:\n",
    "    fastadict = vamb.vambtools.loadfasta(file, keep=keptcontigs)\n",
    "    \n",
    "bindir = '/Users/jakni/Downloads/example/bins'\n",
    "vamb.vambtools.write_bins(bindir, filtered_bins, fastadict, maxbins=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "## Summary of full workflow\n",
    "\n",
    "This is the full default workflow from beginning to end. Calling Vamb from command line does essentially this, except with some input validation, logging, and saving intermediate results to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('/Users/jakni/Documents/scripts/vamb')\n",
    "import vamb\n",
    "\n",
    "# Calculate TNF\n",
    "with open('/Users/jakni/Downloads/example/contigs.fna', 'rb') as contigfile:\n",
    "    tnfs, contignames, contiglengths = vamb.parsecontigs.read_contigs(contigfile)\n",
    "\n",
    "# Calculate RPKM\n",
    "bamdir = '/Users/jakni/Downloads/example/bamfiles/'\n",
    "bampaths = [bamdir + filename for filename in os.listdir(bamdir) if filename.endswith('.bam')]\n",
    "rpkms = vamb.parsebam.read_bamfiles(bampaths)\n",
    "\n",
    "# Encode\n",
    "vae = vamb.encode.VAE(nsamples=rpkms.shape[1])\n",
    "dataloader, mask = vamb.encode.make_dataloader(rpkms, tnfs)\n",
    "vae.trainmodel(dataloader)\n",
    "latent = vae.encode(dataloader)\n",
    "\n",
    "# Cluster and output clusters\n",
    "cluster_iterator = vamb.cluster.cluster(latent, labels=np.array(contignames)[mask])\n",
    "with open('/Users/jakni/Downloads/example/bins.tsv', 'w') as binfile:\n",
    "    vamb.cluster.write_clusters(binfile, cluster_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"memory\"></a>\n",
    "## Running VAMB with low memory (RAM)\n",
    "\n",
    "In the VAMB software, a series of tradeoffs can be taken to decrease RAM consumption, usually to the detriment of some other property like speed or convenience (though not accuracy). With all of those tradeoffs taken, VAMB is relatively memory efficient. By default, several of these tradeoffs are are not taken when running from a Python interpreter, however they are all enabled when running VAMB from command line.\n",
    "\n",
    "The memory consumption of the encoding step is usually the bottleneck. With all memory-saving options enabled, this uses approximately $5*N_{contigs}*(136+N_{latent}+N_{samples})$ bytes, including overhead. As a rule of thumb, if you don't have at least two times that amount of memory, you might want to enable some or all of these memory optimizations.\n",
    "\n",
    "Here's a short list of all the available memory saving options:\n",
    "\n",
    "- Pass a path to the `dumpdirectory` in the `vamb.parsecontigs.read_bamfiles` function. Temporary files will be written to files in this directory to avoid keeping them in memory. This takes a little bit of disk space, but saves lots of RAM\n",
    "- In the `encode.make_dataloader` function, set `destroy` to True. This will in-place modify your RPKM and TNF array, deleting unusable rows and normalizing them. This prevents the creation of a copy of the data.\n",
    "- Similarly, set `destroy=True` when using the `cluster.cluster` function. Again, the input (latent) datasets are modified in-place, saving a copy of the data. This function will completely consume the input array.\n",
    "- When clustering, instead of generating clusters in memory, instantiate the cluster iterator and pass it directly to `cluster.write_clusters`. This way, each cluster will be written to disk after creation and not accumulate in memory. This obviously requires disk space, and you will have to reload the clusters if you are planning on using them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"benchmark\"></a>\n",
    "## Optional: Benchmarking your bins\n",
    "\n",
    "If you want to tweak or enchance Vamb, you'll want to know how well it performs. For this to make any sense, you need to have a *reference*, that is, a list of bins that are deemed true and complete. Otherwise, what do you benchmark against?\n",
    "\n",
    "Vamb's benchmarking works in the following way: You count the number of genomes for which *any* bin has a competeness (recall) above a certain level and a contamination below a certain level (i.e. precision above a certain level). Recall and precision are calculated by the number of basepairs with >= 1x coverage in the right genome versus wrong genomes. In other words, for a bin B and a genome G, the recall is (# basepairs of G covered by a contig in B) / (# basepairs of G covered by any contig), and precision is (# basepairs of G covered by a contig in B) / (# basepairs of any genome covered by a contig in B). An bin *can* count towards multiple genomes given a low enough precision threshold.\n",
    "\n",
    "Let's go through how to benchmark your bins.\n",
    "\n",
    "---\n",
    "#### Genome reference file\n",
    "First, you need a proper reference. For each contig (or sequence) you are binning, you need to provide:\n",
    "* The name of the contig\n",
    "* The name of the genome the contig is coming from (genome names may be arbitrarily chosen)\n",
    "* The name of the reference contig from that genome (may also be arbitrarily chosen)\n",
    "* The leftmost position (start) of where the contig aligns to the reference contig\n",
    "* The rightmost position (end) of where the contig aligns to the reference contig.\n",
    "\n",
    "This information is given as a tab-separated file, with one line per contig. For example, suppose that I have a contig `test_contig_45` which I have located to belong to position 113501 to position 132884 in the reference contig `NC_013740.1` of the genome of `Acidaminococcus fermentans`. Then, one of the lines in the reference file will be:\n",
    "\n",
    "    test_contig_45     Acidaminococcus fermentans       NC_013740.1    113501     132884\n",
    "\n",
    "This file can then be loaded into a `Reference` object:\n",
    "    \n",
    "    with open('/path/to/ref_file.tsv') as ref_file:\n",
    "        reference = Reference.from_file(ref_file)\n",
    "        \n",
    "#### Reference taxonomy file\n",
    "One can choose to benchmark at different taxonomic levels: A bin that is completely uncontaminated on the genus level may not be pure species-wise. Vamb allows you to benchmark at arbitrary taxonomic levels.\n",
    "\n",
    "The reference file described above is taken to represent the lowest (i.e. most specific) taxonomic level. In the taxonomy file, each genome of the reference file is then assigned to one higher rank. Different genomes may be assigned to the same higher rank, but each genome can only be assigned to one. These higher ranks may then be assigned yet higher ranks, and so on. For example's sake, let us assume the lowest \"Genome\" level in the genome reference file represents strains, and the higher ranks are species, genera and families, in order.\n",
    "\n",
    "In the reference file, this is written as a tab-separated text file with one line per genome, and one column per taxonomic level. Note that the leftmost column is taken to be genome names given in the genome reference file. So, for the example line above of the genome reference file, if a reference taxonomy file is loaded, there *must* be a line where the leftmost column is `Acidaminococcus fermentans`. If the true species/genus/whatever of a genome is unknown, you can assign it to an arbitrarily named species/genus. For example, a reference taxnomy file may begin with the following four lines - note that `Strain_118` is assigned an arbitrary species, genus and family.\n",
    "\n",
    "    Strain_455         Bacillus subtilis         Bacillus           Bacillaceae\n",
    "    Strain_255         Bacillus licheniformis    Bacillus           Bacillaceae\n",
    "    Strain_1067        Clostridium tetani        Clostridium        Clostridiaceae\n",
    "    Strain_118         Strain_118                Strain_118         Strain_118\n",
    "    \n",
    "The definitions given above for *recall* and *precision* only makes sense given known mapping locations for contigs, which are specified in the genome reference file. However, the mapping location of a contig to e.g. a genus is a nonsensical concept. So, how is recall and preicison defined for higher taxonomic levels defined?\n",
    "\n",
    "The recall of a higher taxonomic level is the maximum of all of its members. The precision of a higher taxonomic level is the sum of all of its members.\n",
    "\n",
    "A reference taxonomy file can be added to a `Reference` object like so:\n",
    "\n",
    "    with open('/path/to/tax_file.tsv') as tax_file:\n",
    "        reference.load_tax_file(tax_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you need to load your observed bins. The format of this file is expected to be the same as the output of Vamb, namely a two-column file, with arbitrary bin names in the first column and contig names in the second column. Any contig seen in the observed bins is expected to be present in the reference as well. You also need to provide the reference:\n",
    "\n",
    "    with open('/path/to/clusters.tsv') as clusters_file:\n",
    "        bins = Binning.from_file(clusters_file, reference)\n",
    "        \n",
    "You can the query the bins for information about the quality of your bins. Below I'll show an example of this in practice: I will use Vamb's (default) performance on the `metabat_errorfree` dataset and compare it to that of MetaBAT2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gi|224815735|ref|NZ_ACGB01000001.1|_[Acidaminococcus_D21_uid55871]_1-5871\tAcidaminococcus_D21_uid55871\tNZ_ACGB01000001.1\t1\t5871\n",
      "gi|224815735|ref|NZ_ACGB01000001.1|_[Acidaminococcus_D21_uid55871]_5841-8340\tAcidaminococcus_D21_uid55871\tNZ_ACGB01000001.1\t5841\t8340\n",
      "gi|224815735|ref|NZ_ACGB01000001.1|_[Acidaminococcus_D21_uid55871]_8310-10809\tAcidaminococcus_D21_uid55871\tNZ_ACGB01000001.1\t8310\t10809\n",
      "gi|224815735|ref|NZ_ACGB01000001.1|_[Acidaminococcus_D21_uid55871]_10779-29944\tAcidaminococcus_D21_uid55871\tNZ_ACGB01000001.1\t10779\t29944\n",
      "gi|224815735|ref|NZ_ACGB01000001.1|_[Acidaminococcus_D21_uid55871]_29914-33073\tAcidaminococcus_D21_uid55871\tNZ_ACGB01000001.1\t29914\t33073\n",
      "gi|224815735|ref|NZ_ACGB01000001.1|_[Acidaminococcus_D21_uid55871]_33043-41174\tAcidaminococcus_D21_uid55871\tNZ_ACGB01000001.1\t33043\t41174\n",
      "gi|224815735|ref|NZ_ACGB01000001.1|_[Acidaminococcus_D21_uid55871]_41144-44994\tAcidaminococcus_D21_uid55871\tNZ_ACGB01000001.1\t41144\t44994\n",
      "gi|224815735|ref|NZ_ACGB01000001.1|_[Acidaminococcus_D21_uid55871]_44964-53996\tAcidaminococcus_D21_uid55871\tNZ_ACGB01000001.1\t44964\t53996\n",
      "gi|224815735|ref|NZ_ACGB01000001.1|_[Acidaminococcus_D21_uid55871]_53966-59194\tAcidaminococcus_D21_uid55871\tNZ_ACGB01000001.1\t53966\t59194\n",
      "gi|224815735|ref|NZ_ACGB01000001.1|_[Acidaminococcus_D21_uid55871]_59164-65356\tAcidaminococcus_D21_uid55871\tNZ_ACGB01000001.1\t59164\t65356\n"
     ]
    }
   ],
   "source": [
    "# First load in the Reference\n",
    "reference_path = '/Users/jakni/Downloads/vamb/data/metahit/reference.tsv'\n",
    "\n",
    "!head $reference_path # show first 10 lines of reference file\n",
    "\n",
    "with open(reference_path) as reference_file:\n",
    "    reference = vamb.benchmark.Reference.from_file(reference_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The first 10 lines wrap, but you can see the information expected to be present.\n",
    "\n",
    "The `reference` object contains a bunch of attributes which keeps track of which contigs belongs to which bins. You can see which ones using good ol' `help`:\n",
    "\n",
    "`>>> help(reference)`\n",
    "\n",
    "    class Reference(builtins.object)\n",
    "     |  Reference(genomes, taxmaps=[])\n",
    "     |  \n",
    "     |  A set of Genomes known to represent the ground truth for binning.\n",
    "     |  Instantiate with any iterable of Genomes.\n",
    "     |  \n",
    "     |  >>> print(my_genomes)\n",
    "     |  [Genome('E. coli'), ncontigs=95, breadth=5012521),\n",
    "     |   Genome('Y. pestis'), ncontigs=5, breadth=46588721)]\n",
    "     |  >>> Reference(my_genomes)\n",
    "     |  Reference(ngenomes=2, ncontigs=100)\n",
    "     |  \n",
    "     |  Properties:\n",
    "     |  self.genomes: {genome_name: genome} dict\n",
    "     |  self.contigs: {contig_name: contig} dict\n",
    "     |  self.genomeof: {contig: genome} dict\n",
    "     |  self.breadth: Total length of all genomes\n",
    "     |  self.ngenomes\n",
    "     |  self.ncontigs\n",
    "\n",
    "        [ ... ]\n",
    "    \n",
    "Now, I can load in the taxonomy file:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acidaminococcus_D21_uid55871\tAcidaminococcus_D21_uid55871\tAcidaminococcus\n",
      "Acidaminococcus_fermentans_DSM_20731_uid43471\tAcidaminococcus fermentans\tAcidaminococcus\n",
      "Acidaminococcus_intestini_RyC_MR95_uid74445\tAcidaminococcus intestini\tAcidaminococcus\n",
      "Actinomyces_ICM47_uid170984\tActinomyces_ICM47_uid170984\tActinomyces\n",
      "Adlercreutzia_equolifaciens_DSM_19450_uid223286\tAdlercreutzia equolifaciens\tAdlercreutzia\n",
      "Aeromicrobium_JC14_uid199535\tAeromicrobium_JC14_uid199535\tAeromicrobium\n",
      "Akkermansia_muciniphila_ATCC_BAA_835_uid58985\tAkkermansia muciniphila\tAkkermansia\n",
      "Alcanivorax_hongdengensis_A_11_3_uid176602\tAlcanivorax hongdengensis\tAlcanivorax\n",
      "Alistipes_AP11_uid199714\tAlistipes_AP11_uid199714\tAlistipes\n",
      "Alistipes_HGB5_uid67587\tAlistipes_HGB5_uid67587\tAlistipes\n"
     ]
    }
   ],
   "source": [
    "taxonomy_path = '/Users/jakni/Downloads/vamb/data/metahit/taxonomy.tsv'\n",
    "\n",
    "!head $taxonomy_path # show first 10 lines of reference file\n",
    "\n",
    "with open(taxonomy_path) as taxonomy_file:\n",
    "    reference.load_tax_file(taxonomy_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now. we may load the binning itself. I'll automatically filter all bins smaller than 200,000 basepairs away:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/jakni/Downloads/vamb/results/default_vamb/metahit/clusters.tsv') as clusters_file:\n",
    "    vamb_clusters = vamb.cluster.read_clusters(clusters_file)\n",
    "    vamb_bins = vamb.benchmark.Binning(vamb_clusters, reference, minsize=200000)\n",
    "    \n",
    "with open('/Users/jakni/Downloads/vamb/results/default_metabat/metahit/clusters.tsv') as clusters_file:\n",
    "    metabat_clusters = vamb.cluster.read_clusters(clusters_file)\n",
    "    metabat_bins = vamb.benchmark.Binning(metabat_clusters, reference, minsize=200000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The Binning object keeps track of your bins and how they relate to the `Reference`:\n",
    "\n",
    "    help(vamb_bins)\n",
    "    \n",
    "    class Binning(builtins.object)\n",
    "     |  Binning(contigsof, reference, recalls=[0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99], precisions=[0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99], checkpresence=True, disjoint=True, binsplit_separator=None, minsize=None, mincontigs=None)\n",
    "     |  \n",
    "     |  The result of a set of clusters applied to a Reference.\n",
    "     |  >>> ref\n",
    "     |  (Reference(ngenomes=2, ncontigs=5)\n",
    "     |  >>> b = Binning({'bin1': {contig1, contig2}, 'bin2': {contig3, contig4}}, ref)\n",
    "     |  Binning(4/5 contigs, ReferenceID=0x7fe908180be0)\n",
    "     |  >>> b[0.5, 0.9] # num. genomes 0.5 recall, 0.9 precision\n",
    "     |  1\n",
    "     |  \n",
    "     |  Init arguments:\n",
    "     |  ----------- Required ---------\n",
    "     |  contigsof:     Dict of clusters, each sequence present in the Reference\n",
    "     |  reference:     Associated Reference object\n",
    "     |  ----------- Optional ---------\n",
    "     |  recalls:       Iterable of minimum recall thresholds\n",
    "     |  precisions:    Iterable of minimum precision thresholds\n",
    "     |  checkpresence: Whether to raise an error if a sequence if not present in Reference\n",
    "     |  disjoint:      Whether to raise an error if a sequence is in multiple bins\n",
    "     |  binsplit_separator: Split bins according to prefix before this separator in seq name\n",
    "     |  minsize:       Minimum sum of sequence lengths in a bin to not be ignored\n",
    "     |  mincontigs:    Minimum number of sequence in a bin to not be ignored\n",
    "     |  \n",
    "     |  Properties:\n",
    "     |  self.reference:       Reference object of this benchmark\n",
    "     |  self.recalls:         Sorted tuple of recall thresholds\n",
    "     |  self.precisions:      Sorted tuple of precision thresholds\n",
    "     |  self.nbins:           Number of bins\n",
    "     |  self.ncontigs:        Number of binned contigs\n",
    "     |  self.contigsof:       {bin_name: {contig set}}\n",
    "     |  self.binof:           {contig: bin_name(s)}, val is str or set\n",
    "     |  self.breadthof:       {bin_name: breadth}\n",
    "     |  self.intersectionsof: {genome: {bin:_name: intersection}}\n",
    "     |  self.breadth:         Total breadth of all bins\n",
    "     |  self.counters:        List of (rec, prec) Counters of genomes for each taxonomic rank\n",
    "     |  \n",
    "     |  Methods defined here:\n",
    "     \n",
    "     [ ... ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The easiest way to get information about the quality of a `Binning` is by using the `summary` method. This prints the number of genomes reconstructed for each taxonomic level for each recall threshold at a precision threshold of 0.9:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vamb bins:\n",
      "114\t112\t109\t106\t96\t67\t28\t8\t0\n",
      "129\t128\t125\t123\t112\t78\t35\t11\t0\n",
      "59\t59\t58\t58\t57\t50\t30\t11\t0\n",
      "\n",
      "METABAT2 bins:\n",
      "101\t93\t88\t82\t61\t29\t4\t0\t0\n",
      "116\t108\t102\t96\t73\t35\t5\t0\t0\n",
      "51\t48\t48\t48\t43\t32\t6\t2\t0\n"
     ]
    }
   ],
   "source": [
    "print('Vamb bins:')\n",
    "for rank in vamb_bins.summary():\n",
    "    print('\\t'.join(map(str, rank)))\n",
    "print('\\nMETABAT2 bins:')\n",
    "for rank in metabat_bins.summary():\n",
    "    print('\\t'.join(map(str, rank)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "On the [metabat webpage](https://bitbucket.org/berkeleylab/metabat/wiki/CAMI) they have a neat plot where they plot the number of observed bins at different recalls for a specific specificity. Just for fun, let's recreate that here with our data, using the species level.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "for precision in 0.95, 0.9:\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    colors = ['#DDDDDD', '#AAAAAA', '#777777', '#444444', '#000000']\n",
    "    recalls = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    for y, bins in zip((0, 1), (vamb_bins, metabat_bins)):\n",
    "        for color, recall in zip(colors, recalls):\n",
    "            plt.barh(y, bins.counters[1][(recall, precision)], color=color)\n",
    "\n",
    "    plt.title(str(precision), fontsize=18)\n",
    "    plt.yticks([0, 1], ['Vamb', 'MetaBAT2'], fontsize=16)\n",
    "    plt.xticks([i*25 for i in range(5)], fontsize=13)\n",
    "    plt.legend([str(i) for i in recalls], bbox_to_anchor=(1, 1.1), title='Recall', fontsize=12)\n",
    "    \n",
    "    if precision == 0.9:\n",
    "        plt.xlabel('# of Genomes Identified', fontsize=16)\n",
    "    plt.gca().set_axisbelow(True)\n",
    "    plt.grid()"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
