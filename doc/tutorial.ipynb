{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walkthough of Vamb from the Python interpreter\n",
    "\n",
    "The VAMB pipeline consist of a series of tasks each which have a dedicated module:\n",
    "\n",
    "1) Parse fasta file and get TNF of each sequence, as well as sequence length and names (module `parsecontigs`)\n",
    "\n",
    "2) Parse the BAM files and get abundance estimate for each sequence in the fasta file (module `parsebam`)\n",
    "\n",
    "3) Train a VAE wit the depths and TNF matrices, and encode it to a latent representation (module `encode`)\n",
    "\n",
    "4) Cluster the encoded inputs to metagenomic bins (modules `threshold` and `cluster`)\n",
    "\n",
    "Additionally, for developing and testing VAMB, we use:\n",
    "\n",
    "5) Benchmark the resulting bins against a gold standard (module `benchmark`)\n",
    "\n",
    "In the following chapters of this walkthrough, we will go through each step in more detail from within the Python interpreter. We will show how to use Vamb by example, and what each step does, some of the theory behind the actions, and the different parameters that can be set. With this knowledge, you should be able to extend or alter the behaviour of Vamb more easily.\n",
    "\n",
    "For the examples, we will assume the following relevant prerequisite files exists in the directory `/home/jakni/Downloads/example/`:\n",
    "\n",
    "* `contigs.fna` - The filtered FASTA contigs which were mapped against, and\n",
    "* `bamfiles/*.bam` - The 6 .bam files from mapping the reads to the contigs above.\n",
    "\n",
    "## Table of contents:\n",
    "\n",
    "### 1. [Importing Vamb and getting help](#importing)\n",
    "\n",
    "### 2. [Calculate the sequence tetranucleotide frequencies](#parsecontigs)\n",
    "\n",
    "### 3. [Calculate the abundance matrix](#parsebam)\n",
    "\n",
    "### 4. [Train the autoencoder and encode input data](#encode)\n",
    "\n",
    "### 5. [Binning the encoding](#cluster)\n",
    "\n",
    "### 6. [Postprocessing the bins](#postprocessing)\n",
    "\n",
    "### 7. [Summary of full workflow](#summary)\n",
    "\n",
    "### 8. [Running VAMB with low memory (RAM)](#memory)\n",
    "\n",
    "### 9. [Optional: Benchmarking your bins](#benchmark)\n",
    "\n",
    "<a id=\"importing\"></a>\n",
    "## Importing Vamb and getting help\n",
    "\n",
    "First step is to get Vamb imported\n",
    "    \n",
    "    [jakni@nissen:~]$ python\n",
    "    >>> import vamb\n",
    "    Traceback (most recent call last):\n",
    "      File \"<stdin>\", line 1, in <module>\n",
    "    ModuleNotFoundError: No module named 'vamb'\n",
    "    \n",
    "I'm not in the directory containing the vamb directory. That means the directory containing the vamb directory is not in my `sys.path`. I need to either move the vamb directory to one of my `sys.path` directories or add the directory containing the vamb directory to `sys.path`. I'll do the latter.\n",
    "    \n",
    "    [jakni@nissen:~]$ python\n",
    "    >>> import sys\n",
    "    >>> sys.path.append('/home/jakni/Documents/scripts/')\n",
    "    >>> import vamb\n",
    "\n",
    "Now we got it imported. When using Vamb, you'll almost certianly need help (we wish it was so easy you didn't, but making user friendly software is *hard!*).\n",
    "\n",
    "Luckily, there's the built-in `help` function in Python.\n",
    "\n",
    "---\n",
    "\n",
    "`>>> help(vamb)`\n",
    "    \n",
    "    Help on package vamb:\n",
    "\n",
    "    NAME\n",
    "        vamb - Variational Autoencoder for Metagenomic Binning\n",
    "\n",
    "    DESCRIPTION\n",
    "        Vamb does what it says on the tin - bins metagenomes using a variational autoencoder.\n",
    "        \n",
    "    [ lines elided ]\n",
    "    \n",
    "        General workflow:\n",
    "        1) Filter contigs by size using vamb.vambtools.filtercontigs\n",
    "        2) Map reads to contigs to obtain BAM file\n",
    "        3) Calculate TNF of contigs using vamb.parsecontigs\n",
    "        4) Create RPKM table using vamb.parsebam\n",
    "        5) Train autoencoder using vamb.encode\n",
    "        6) Cluster latent representation using vamb.cluster\n",
    "    \n",
    "    [ lines elided ]\n",
    "    \n",
    "---\n",
    "    \n",
    "The `PACKAGE CONTENTS` under `help(vamb)` is just a list of all *importable* files in the `vamb` directory - some of these aren't part of the Vamb package proper and really shouldn't be imported, so ignore that.\n",
    "\n",
    "---\n",
    "You can also get help for each of the modules, for example the `cluster` module:\n",
    "\n",
    "`>>> help(vamb.cluster)`\n",
    "\n",
    "    Help on module vamb.cluster in vamb:\n",
    "\n",
    "    NAME\n",
    "        vamb.cluster - Iterative medoid clustering.\n",
    "\n",
    "    DESCRIPTION\n",
    "        Usage:\n",
    "        >>> cluster_iterator = cluster(rpkms, tnfs, labels=contignames)\n",
    "        >>> clusters = dict(cluster_iterator)\n",
    "\n",
    "        Implements one core function, cluster, along with the helper\n",
    "        functions write_clusters and read_clusters.\n",
    "        For all functions in this module, a collection of clusters are represented as\n",
    "        a {clustername, set(elements)} dict.\n",
    "\n",
    "        cluster algorithm:\n",
    "    \n",
    "    [ lines elided ]\n",
    "        \n",
    "---\n",
    "And for functions:\n",
    "\n",
    "`>>> help(vamb.cluster.cluster)`\n",
    "\n",
    "    Help on function cluster in module vamb.cluster:\n",
    "\n",
    "    cluster(matrix, labels=None, threshold=None, maxsteps=25, destroy=False, normalized=False, nsamples=2500, maxsize=2500, logfile=None)\n",
    "        Iterative medoid cluster generator. Yields (medoid), set(labels) pairs.\n",
    "\n",
    "        Inputs:\n",
    "            matrix: A (obs x features) Numpy matrix of data type numpy.float32\n",
    "            labels: None or Numpy array with labels for matrix rows [None = indices]\n",
    "            threshold: Optimal medoid search in this distance from medoid [None = auto]\n",
    "            maxsteps: Stop searching for optimal medoid after N futile attempts [25]\n",
    "            destroy: Destroy input matrix, saving memory. [False]\n",
    "            normalized: Matrix is already zscore-normalized across axis 1 [False]\n",
    "            nsamples: Estimate threshold from N samples [2500]\n",
    "            maxsize: Discard sample if more than N contigs are within threshold [2500]\n",
    "            logfile: Print threshold estimates and certainty to file [None]\n",
    "\n",
    "        Output: Generator of (medoid, set(labels_in_cluster)) tuples.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need this to display images in this notebook.\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jakni/Documents/scripts/vamb')\n",
    "import vamb # Don't worry if it raises a warning about compiletime version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"parsecontigs\"></a>\n",
    "## Calculate the sequence tetranucleotide frequencies\n",
    "\n",
    "If you forget what to do at each step, remember that `help(vamb)` said:\n",
    "\n",
    "    General workflow:\n",
    "    1) Filter contigs by size using vamb.vambtools.filtercontigs\n",
    "    2) Map reads to contigs to obtain BAM file\n",
    "    3) Calculate TNF of contigs using vamb.parsecontigs\n",
    "    \n",
    "    [ lines elided ]\n",
    "\n",
    "Okay, we already have filtered contigs. I could have used the vamb.vambtools.filtercontigs to filter the FASTA file, but here, they were already filtered. We have also already mapped reads to them and gotten BAM files, so we begin with the third step, using the `vamb.parsecontigs` module. How do you use that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module vamb.parsecontigs in vamb:\n",
      "\n",
      "NAME\n",
      "    vamb.parsecontigs - Calculate z-normalized tetranucleotide frequency from a FASTA file.\n",
      "\n",
      "DESCRIPTION\n",
      "    Usage:\n",
      "    >>> with open('/path/to/contigs.fna', 'rb') as filehandle\n",
      "    ...     tnfs, contignames, lengths = read_contigs(filehandle)\n",
      "\n",
      "FUNCTIONS\n",
      "    read_contigs(filehandle, minlength=100, preallocate=True)\n",
      "        Parses a FASTA file open in binary reading mode.\n",
      "        \n",
      "        Input:\n",
      "            filehandle: Filehandle open in binary mode of a FASTA file\n",
      "            minlength: Ignore any references shorter than N bases [100]\n",
      "            preallocate: Read contigs twice, saving memory [True]\n",
      "        \n",
      "        Outputs:\n",
      "            tnfs: An (n_FASTA_entries x 136) matrix of tetranucleotide freq.\n",
      "            contignames: A list of contig headers\n",
      "            lengths: A Numpy array of contig lengths\n",
      "\n",
      "FILE\n",
      "    /Users/jakni/Documents/scripts/vamb/vamb/parsecontigs.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(vamb.parsecontigs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "I use `vamb.parsecontigs.read_contigs` with the inputs and outputs as written:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File must be opened in binary mode\n",
    "with open('/Users/jakni/Downloads/example/contigs.min2kbp.fna', 'rb') as filehandle:\n",
    "    tnfs, contignames, lengths = vamb.parsecontigs.read_contigs(filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let's have a look at the resulting data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of tnfs: <class 'numpy.ndarray'> of dtype float32\n",
      "Shape of tnfs: (50658, 136)\n",
      "\n",
      "Type of contignames: <class 'list'>\n",
      "Length of contignames: 50658\n",
      "\n",
      "First 5 elements of contignames:\n",
      "30_NODE_1_length_245508_cov_18.4904\n",
      "30_NODE_2_length_222690_cov_39.7685\n",
      "30_NODE_3_length_222459_cov_20.3665\n",
      "30_NODE_4_length_173155_cov_20.1181\n",
      "30_NODE_5_length_161239_cov_20.1237\n",
      "\n",
      "Type of lengths: <class 'numpy.ndarray'> of dtype int64\n",
      "Length of lengths: 50658\n",
      "\n",
      "First 5 elements of lengths:\n",
      "245508\n",
      "222690\n",
      "222459\n",
      "173155\n",
      "161239\n"
     ]
    }
   ],
   "source": [
    "print('Type of tnfs:', type(tnfs), 'of dtype', tnfs.dtype)\n",
    "print('Shape of tnfs:', tnfs.shape, end='\\n\\n')\n",
    "\n",
    "print('Type of contignames:', type(contignames))\n",
    "print('Length of contignames:', len(contignames), end='\\n\\n')\n",
    "\n",
    "print('First 5 elements of contignames:')\n",
    "for i in range(5):\n",
    "    print(contignames[i])\n",
    "\n",
    "print('\\nType of lengths:', type(lengths), 'of dtype', lengths.dtype)\n",
    "print('Length of lengths:', len(lengths), end='\\n\\n')\n",
    "\n",
    "print('First 5 elements of lengths:')\n",
    "for i in range(5):\n",
    "    print(lengths[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "__For a gzipped FASTA file__, simply `import gzip` and open the file with `gzip.open('/path/to/contigs.fna', 'rb')`. Alternatively, you can use `with open vamb.vambtools.Reader('/path/to/contigs.fna' ,'rb')`, which will automatically detect whether the file is gzipped or not and return a corresponding opened file object.\n",
    "\n",
    "Note that reading gzipped files will slow down the FASTA parsing quite a bit. But the time spent parsing the FASTA file will likely still be insignificant compared to the other steps of Vamb.\n",
    "\n",
    "__The rationale for parsing the contigs__ is that it turns out that related organisms tend to share a similar kmer-distribution across most of their genome. The reason for that is not understood, even though it's believed that common functional motifs, GC-content and presence/absence of endonucleases explains some of the observed similary.\n",
    "\n",
    "The `tnfs` is the tetranucleotide frequency - it's the frequency of the canonical kmer of each 4mer in the contig. We use 4-mers because there are 136 canonical 4-mers, which is an appropriate number of features to cluster - not so few that there's no signal and not so many it becomes unwieldy and the estimates of the frequencies become uncertain. We could also have used 3-mers. In tests we have made, 3-mers are _almost_, but not quite as good as 4-mers for separating different species. You could probably switch tetranucleotide frequency to trinucleotide frequency in Vamb without any significant drop of accuracy. However, there are 512 canonical 5-mers, that would be too many features to handle comfortably, and it could easily cause memory issues.\n",
    "\n",
    "__The argument `minlength`__ sets the filter removing any contigs shorter than this. Short contigs in general do not work well with Vamb since neither TNF nor the abundance (shown in next section) can be estimated reliably for short contigs. When choosing the correct theshold, there is some sweet spot between on one hand not allowing small contigs to act as a source of noise in the binning, and on the other hand to not throw away more contigs than you absolutely have to. We don't know what the sweet spot is, but it's probably somewhere around ~2000 bp.\n",
    "\n",
    "The problem with filtering contigs using `minlength` is that the smaller contigs which are thrown away will still recruit reads during the mapping that creates the BAM files, thus removing information from those reads. For that reason, we recommend filtering the contigs *before* mapping by using the function `vamb.vambtools.filtercontigs`.\n",
    "\n",
    "__With the argument `preallocate`__ set to `True` (as is default), the function will read the file *twice*, one time to count the number of sequences, then it will preallocate the arrays, and then it will read it a second time to fill the arrays. Having this set to `True` approximately doubles the reading time, but cuts down on memory.\n",
    "\n",
    "__The memory consumption of Vamb can be an issue__, so at this point, you should probably consider whether you have enough RAM. This is a small dataset, so there's no problem. With hundreds of samples and millions of contigs however, this becomes a problem, even though Vamb is fairly memory-friendly. If you think memory might be an issue, see the [Running VAMB with low memory (RAM)](#memory) section.\n",
    "\n",
    "<a id=\"parsebam\"></a>\n",
    "## Calculate the abundance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module vamb.parsebam in vamb:\n",
      "\n",
      "NAME\n",
      "    vamb.parsebam - Estimate RPKM (depths) from BAM files of reads mapped to contigs.\n",
      "\n",
      "DESCRIPTION\n",
      "    Usage:\n",
      "    >>> bampaths = ['/path/to/bam1.bam', '/path/to/bam2.bam', '/path/to/bam3.bam']\n",
      "    >>> rpkms = read_bamfiles(bampaths)\n",
      "\n",
      "FUNCTIONS\n",
      "    mergecolumns(pathlist)\n",
      "        Merges multiple npz files with columns to a matrix.\n",
      "        \n",
      "        All paths must be npz arrays with the array saved as name 'arr_0',\n",
      "        and with the same length.\n",
      "        \n",
      "        Input: pathlist: List of paths to find .npz files to merge\n",
      "        Output: Matrix with one column per npz file\n",
      "    \n",
      "    read_bamfiles(paths, dumpdirectory=None, minscore=None, minlength=100, subprocesses=8, logfile=None)\n",
      "        Spawns processes to parse BAM files and get contig rpkms.\n",
      "        \n",
      "        Input:\n",
      "            path: List or tuple of paths to BAM files\n",
      "            dumpdirectory: [None] Dir to create and dump per-sample depths NPZ files to\n",
      "            minscore [None]: Minimum alignment score (AS field) to consider\n",
      "            minlength [100]: Ignore any references shorter than N bases\n",
      "            subprocesses [8]: Number of subprocesses to spawn\n",
      "            logfile: [None] File to print progress to\n",
      "        \n",
      "        Output: A (n_contigs x n_samples) Numpy array with RPKM\n",
      "\n",
      "DATA\n",
      "    DEFAULT_SUBPROCESSES = 8\n",
      "\n",
      "FILE\n",
      "    /Users/jakni/Documents/scripts/vamb/vamb/parsebam.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Again, we can use the help function to see what we need to do\n",
    "help(vamb.parsebam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/jakni/Downloads/example/bam/101.bam',\n",
       " '/Users/jakni/Downloads/example/bam/178.bam',\n",
       " '/Users/jakni/Downloads/example/bam/179.bam',\n",
       " '/Users/jakni/Downloads/example/bam/196.bam',\n",
       " '/Users/jakni/Downloads/example/bam/198.bam',\n",
       " '/Users/jakni/Downloads/example/bam/30.bam']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bamfiles = !ls /Users/jakni/Downloads/example/bam\n",
    "bamfiles = ['/Users/jakni/Downloads/example/bam/' + p for p in bamfiles]\n",
    "bamfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of rpkms: <class 'numpy.ndarray'> of dtype float32\n",
      "Shape of rpkms (50658, 6)\n"
     ]
    }
   ],
   "source": [
    "# Yep, those file paths look right.\n",
    "\n",
    "rpkms = vamb.parsebam.read_bamfiles(bamfiles) # This takes some time.\n",
    "print('Type of rpkms:', type(rpkms), 'of dtype', rpkms.dtype)\n",
    "print('Shape of rpkms', rpkms.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The idea here is that two contigs from the same genome will always be physically present together, and so they should have a similar abundance in all samples. Some contigs represent repeats like duplicated segments - these contigs should have a fixed ratio of abundance to other contigs. Thus, even when considering repeated contigs, there should be a tight cosine distance correlation between abundances of contigs from the same genome.\n",
    "\n",
    "The `vamb.parsebam` module takes a rather crude approach to estimating abundance, namely by simply counting the number of mapped reads to each contig, normalized by total number of reads and the contig's length. This measure is in trancriptomics often called RPKM, *reads per kilobase per million mapped reads*. Other metagenomic binners like Metabat and Canopy uses an average of per-nucleotide depth of coverage instead. We do not believe there is any theoretical or practical advantage of using depth over RPKM. Because BWA handles redundant databases rather poorly, there is not even any advantage of using the technically more accurate FPKM over RPKM. We will use the terms *abundance*,  *depth* and *rpkm* interchangably.\n",
    "\n",
    "---\n",
    "We can see (in the default value for the `subprocesses` argument) that the default number of parallel BAM-reading processes it will spawn is 4. This is because Python detected 4 threads on my laptop. In general, Vamb's default here is to use the number of availble threads, or 8 threads if more than 8 is detected, as the BAM-reading function will almost certainly become IO bound at more than 8 threads.\n",
    "\n",
    "As with the `vamb.parsecontigs.read_contigs` function, I don't care about the `minlength` argument, since our fasta file is already filtered. Again, I will re-iterate that filtering the FASTA file _before_ mapping leads to the best results.\n",
    "\n",
    "The function ignores all alignments with alignment score less than `minscore` (as determined by the auxiliary `AS:i` field in the BAM file, which Vamb assumes is present if `minscore` is not `None`). Ideally, the user should construct the BAM files so that they only contain alignments that the user believes are true (i.e. set reasonable alignment and filtering criteria), however, in practice, many users will not know the importance of this, so by default, Vamb filters away any reads with an alignment score of less than 50 in order to mitigate the worst alignments.\n",
    "\n",
    "Lastly, the argument `logfile` should be `None` or the filehandle of an opened, writeable file. If the latter, it will print status updates to the logfile.\n",
    "\n",
    "---\n",
    "Now, I tend to be a bit ~~paranoid~~<sup>careful</sup>, so if I loaded in 500 GB of BAM files, I'd want to save the work I have now in case something goes wrong - and we're about to fire up the VAE so lots of things can go wrong.\n",
    "\n",
    "What importants objects do I have in memory right now?\n",
    "\n",
    "* `tnfs`: A Numpy array of tnfs\n",
    "* `contignames`: A list of contignames\n",
    "* `lengths`: A Numpy array of contig lengths\n",
    "* `rpkms`: A Numpy array of rpkms\n",
    "\n",
    "I'm going to use `vamb.vambtools.write_npz` to save the Numpy arrays (that function is just a thin convenience wrapper for `numpy.savez_compressed`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open('/Users/jakni/Downloads/example/contignames.npz', 'wb') as file:\n",
    "    vamb.vambtools.write_npz(file, np.array(contignames))\n",
    "\n",
    "with open('/Users/jakni/Downloads/example/lengths.npz', 'wb') as file:\n",
    "    vamb.vambtools.write_npz(file, lengths)\n",
    "\n",
    "with open('/Users/jakni/Downloads/example/tnfs.npz', 'wb') as file:\n",
    "    vamb.vambtools.write_npz(file, tnfs)\n",
    "    \n",
    "with open('/Users/jakni/Downloads/example/rpkms.npz', 'wb') as file:\n",
    "    vamb.vambtools.write_npz(file, rpkms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"encode\"></a>\n",
    "## Train the autoencoder and encode input data\n",
    "\n",
    "Again, you can use `help` to see how to use the module\n",
    "\n",
    "`>>> help(vamb.encode)`\n",
    "\n",
    "    Help on module vamb.encode in vamb:\n",
    "\n",
    "    NAME\n",
    "        vamb.encode - Encode a depths matrix and a tnf matrix to latent representation.\n",
    "\n",
    "    DESCRIPTION\n",
    "        Creates a variational autoencoder in PyTorch and tries to represent the depths\n",
    "        and tnf in the latent space under gaussian noise.\n",
    "\n",
    "        Usage:\n",
    "        >>> vae = VAE(nsamples=6)\n",
    "        >>> dataloader, mask = make_dataloader(depths, tnf)\n",
    "        >>> vae.trainmodel(dataloader)\n",
    "        >>> latent = vae.encode(dataloader) # Encode to latent representation\n",
    "        >>> latent.shape\n",
    "        (183882, 40)\n",
    "        \n",
    "    [ lines elided ]\n",
    "    \n",
    "---\n",
    "Aha, so we need to create the VAE, create the dataloader (and the mask), then use the `trainmodel` method first, then the `VAE.encode` method. You can call the `help` functions on those, but I'm not showing that here.\n",
    "\n",
    "Training networks always take some time. If you have a GPU and CUDA installed, you can pass `cuda=True` to the VAE to train on your GPU for increased speed. With a beefy GPU, this can make quite a difference. I run this on my laptop (with a puny GTX 860m GPU), so I'll just use my CPU. And I'll run just 10 epochs rather than the more suitable 200:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNetwork properties:\n",
      "\tCUDA: False\n",
      "\tAlpha: 0.05\n",
      "\tBeta: 200\n",
      "\tDropout: 0.2\n",
      "\tN hidden: 325, 325\n",
      "\tN latent: 40\n",
      "\n",
      "\tTraining properties:\n",
      "\tN epochs: 10\n",
      "\tStarting batch size: 64\n",
      "\tBatchsteps: \n",
      "\tLearning rate: 0.001\n",
      "\tN sequences: 50658\n",
      "\tN samples: 6\n",
      "\n",
      "\tEpoch: 1\tLoss: 0.439514\tCE: 0.7416030\tSSE: 99.485453\tKLD: 77.8929\tBatchsize: 64\n",
      "\tEpoch: 2\tLoss: 0.330684\tCE: 0.5477087\tSSE: 81.695766\tKLD: 82.0095\tBatchsize: 64\n",
      "\tEpoch: 3\tLoss: 0.320438\tCE: 0.5336555\tSSE: 75.387584\tKLD: 78.2020\tBatchsize: 64\n",
      "\tEpoch: 4\tLoss: 0.315184\tCE: 0.5271558\tSSE: 71.020998\tKLD: 76.5793\tBatchsize: 64\n",
      "\tEpoch: 5\tLoss: 0.311294\tCE: 0.5219055\tSSE: 68.638664\tKLD: 74.7391\tBatchsize: 64\n",
      "\tEpoch: 6\tLoss: 0.308488\tCE: 0.5182149\tSSE: 67.016978\tKLD: 72.7131\tBatchsize: 64\n",
      "\tEpoch: 7\tLoss: 0.306484\tCE: 0.5154374\tSSE: 66.038480\tKLD: 71.3392\tBatchsize: 64\n",
      "\tEpoch: 8\tLoss: 0.305217\tCE: 0.5135927\tSSE: 65.108134\tKLD: 71.7621\tBatchsize: 64\n",
      "\tEpoch: 9\tLoss: 0.303462\tCE: 0.5111113\tSSE: 64.546372\tKLD: 69.8997\tBatchsize: 64\n",
      "\tEpoch: 10\tLoss: 0.302359\tCE: 0.5095912\tSSE: 63.885808\tKLD: 69.4672\tBatchsize: 64\n"
     ]
    }
   ],
   "source": [
    "vae = vamb.encode.VAE(nsamples=rpkms.shape[1])\n",
    "dataloader, mask = vamb.encode.make_dataloader(rpkms, tnfs)\n",
    "\n",
    "with open('/tmp/model.pt', 'wb') as modelfile:\n",
    "    # Print progress to stdout by passing logfile=sys.stdout\n",
    "    vae.trainmodel(dataloader, nepochs=10, modelfile=modelfile, batchsteps=None, logfile=sys.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create the VAE, then we create the dataloader and the mask. The dataloader normalizes the TNF such that the mean and standard deviation for each tetranucleotide across all contigs (i.e. a column) is 0 and 1, respectively, and normalizes the rpkm such that each contig (i.e. a row) sums to 1. Furthermore, the dataloader shuffles the contigs at each epoch.\n",
    "\n",
    "The dataloader also discards all contigs where either the TNF vector or the depths vector is all zeros - we call these contigs *zero contigs*. The `mask` it returns is a boolean vector with `True` if a contig was kept, and `False` if it was discarded. We began with 50658 contigs, and the log above states that it was trained with 50658 contigs, implying that 50658 - 50658 = 0 contigs were discarded.\n",
    "\n",
    "Here, we passed the default value `False` to the `destroy` keyword of `make_dataloader`. If this is set to `True`, the input arrays are normalized and masked in-place, modifying them. This saves one or two copies of the data, which can be critical for large array sizes.\n",
    "\n",
    "The VAE encodes the high-dimensional (n_samples + 136 features) input data in a lower dimensional space (nlatent features). When training, it learns an encoding scheme, with which it encodes the input data to a series of normal distributions, and a decoding scheme, in which it uses one value sampled from each normal distribution to reconstruct the input data.\n",
    "\n",
    "The theory here is that if the VAE learns to reconstruct the input, the distributions must be a more efficient encoding of the input data, since the same information is contained in fewer neurons. If the input data for the contigs indeed do fall into bins, an efficient encoding would be to simply encode the bin they belong to, then use the \"bin identity\" to reconstruct the data. We force it to encode to *distributions* rather than single values because this makes it more robust - it will not as easily overfit to interpret slightly different values as being very distinct if there is an intrinsic noise in each encoding.\n",
    "\n",
    "### The loss function\n",
    "\n",
    "The loss of the VAE consists of three major terms:\n",
    "\n",
    "* Cross entropy (CE) measures the dissimilarity of the reconstructed abundances to observed abundances. This penalizes a failure to reconstruct the abundances accurately.\n",
    "* Sum of squared error (SSE) measures the dissimilary of reconstructed versus observed TNF. This penalizes failure to reconstruct TNF accurately.\n",
    "* Kullback-Leibler divergence (KLD) measures the dissimilarity between the encoded distributions and the standard gaussian distribution N(0, 1). This penalizes learning.\n",
    "\n",
    "All three terms are important. CE and SSE is necessary, because we believe the VAE can only learn to effectively reconstruct the input if it learns to encode the signal from the input into the latent layers. In other words, these terms incentivize the network to learn something. KLD is necessary because we care that the encoding is *efficient*, viz. it is contained in as little information as possible. The entire point of encoding is to encode a majority of the signal while shedding the noise, and this is only achieved if we place contrains on how much the network is allowed to learn. Without KLD, the network can theoretically learn an infinitely complex encoding, and the network will learn to encode both noise and signal.\n",
    "\n",
    "In normal autoencoders, people use binary cross-entropy rather than crossentropy. We believe crossentropy is more correct here, since we normalize by letting the depths sum to one across a contig. In this way, you can view the depths distribution across samples as a probability distribution that a random mapping read will come from each sample.\n",
    "\n",
    "In `encode.py`, the loss function is written as:\n",
    "\n",
    "$L = \\frac{(1 - \\alpha)}{ln(S)} \\cdot CE + \\frac{\\alpha}{136} \\cdot SSE + \\frac{1}{N_{L}\\beta} \\cdot KLD$\n",
    "\n",
    "where $N_{L}$ is number of latent neurons and S is number of samples. It is hardly obvious where the scaling factors come from, so let me try to explain it.\n",
    "\n",
    "As the learning rate is fixed and optimized for a specific gradient, this means the total reconstruction loss $\\frac{(1 - \\alpha)}{ln(S)} \\cdot CE + \\frac{\\alpha}{136} \\cdot SSE$ should sum to a constant, lest the training become ustable. To make things simpler, we want it to sum to 1. It would probably be more precise to say that L should be 1, but since $\\frac{1}{N_{L}\\beta} \\cdot KLD <<  \\frac{(1 - \\alpha)}{ln(S)} \\cdot CE + \\frac{\\alpha}{136} \\cdot SSE$ for any values of $\\alpha$ and $\\beta$ that seem to work, setting the reconstruction loss to 1 is basically the same.\n",
    "\n",
    "When optimizing the network, we want a single variable to control the ratio between $SSE$ and $CE$ - we call this $\\alpha$. This scales CE and SSE so that $\\alpha = \\frac{SSE}{SSE + CE}$ \n",
    "\n",
    "But here comes a problem. While we want to scale SSE and CE so that the two constrains above (namely $CE+SSE=1$ and  $\\alpha = \\frac{SSE}{SSE + CE}$) are true, we can't know *beforehand* what CE, KLD or SSE is. And, in any rate, these values changes across the training run (that's the point of training!).\n",
    "\n",
    "What we *can* reason about is the values of CE and SSE in a totally *naive*, network which had *no knowledge* of the input dataset. This represents the state of the network before *any* learning is done. What would such a network predict? Well, since we normalize the reconstructed depths across $S$ samples to be between 0 and 1 and sum to 1, the outputs would be close to $[S^{-1}, S^{-1} ... S^{-1}]^{T}$, which, by the definition of cross entropy, would yield a CE of $ln(S)$. We normalize both TNF inputs and outputs to follow an approximately normal distribution with mean around 0, which means the expected SSE for TNF is 1 per input TNF neuron, for a total of $SSE = 136$.\n",
    "\n",
    "Importantly, if we actually check the CE and SSE values for untrained networks, they are quite close to these expected values of $ln(S)$ and 136. Since neither CE nor SSE is reduced by more than an order of magnitude from their starting values during training on realistic datasets (usually much less), these expected values work as stand-ins for what we can expect CE and SSE to be across training.\n",
    "\n",
    "So the purpose of the scaling factors in front of the CE and SEE terms is to scale CE from the expected value of $ln(S)$ to the target value of (1-$\\alpha$) and SSE from 136 to $\\alpha$. Hence the scaling factors $\\frac{(1 - \\alpha)}{ln(S)}$ and $\\frac{\\alpha}{136}$.\n",
    "\n",
    "For KLD, we want the user to be able to control the ratio $\\frac{C+S}{K}$, where C, S and K are each of the terms from CE, SSE and KLD in the loss, respectively. Since KLD is defined as a sum of individual KLD for each of latent neurons, its value is proportional to $N_{L}$. So we let the user set a ratio\n",
    "\n",
    "$\\beta = \\frac{KLD}{N_{L}K} \\cdot \\frac{(1 - \\alpha)}{ln(S)} \\cdot CE + \\frac{\\alpha}{136} \\cdot SSE$,\n",
    "\n",
    "Where again, K is the weighed KLD in the loss. In other words, we allow the user to weigh the KLD-related loss relative to the reconstruction loss. However, as we just scaled the CE and SSE terms to make sure they sum to one, this simplifies to:\n",
    "\n",
    "$\\beta = \\frac{KLD}{N_{L}K} \\Leftrightarrow K = \\frac{1}{N_{L}\\beta} \\cdot KLD$.\n",
    "\n",
    "Hence the scaling factor $\\frac{1}{N_{L}\\beta}$ in front of KLD.\n",
    "\n",
    "If you look at the outputs from the 10 epochs, you can see the KL-divergence rises the first epoch as it learns the dataset and the latent layer drifts away from its prior. At epoch 2, the penalty associated with KL-divergence outweighs the CE and SSE losses. At this point, the KL will stall, and then fall. This point depends on $\\beta$ and the complexity of the dataset.\n",
    "\n",
    "Okay, so now we have the trained `vae` and gotten the `dataloader`. Let's feed the dataloader to the VAE in order to get the latent representation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50658, 40)\n"
     ]
    }
   ],
   "source": [
    "# No need to pass gpu=True to the encode function to encode on GPU\n",
    "# If you trained the VAE on GPU, it already resides there\n",
    "latent = vae.encode(dataloader)\n",
    "\n",
    "print(latent.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "That's 50658 contigs each represented by the mean of their latent distribution.\n",
    "\n",
    "Sometimes, you'll want to reuse a VAE you have already trained. For this, I've added the `VAE.save` method of the VAE class, as well as a `VAE.load` method. You will have noticed in the training example above that I defined a `modelfile`, a file the VAE will create and save its parameters to. We can always use that file to recreate the VAE and have a pretrained model. But remember - a trained VAE only works on the dataset it's been trained on, and not necessarily on any other!\n",
    "\n",
    "I want to **show** that we get the exact same network back that we trained, so here I encode the first contig, delete the VAE, reload the VAE and encode the first contig again. The two encodings should be identical.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-22.4446,  -3.9253, 106.8839,  14.7367, -83.4087, -68.7292,  48.5521,\n",
      "         25.4342, 115.5247,  76.3833, -42.4529,  46.1396,  14.3169, -14.1612,\n",
      "        -52.3239,  23.2744,  72.6793,  -7.7414, -44.7457,   1.7712,  56.0855,\n",
      "        -33.4397, -17.3890,  -0.4904, -55.2360,  61.7612, -28.6299, -98.5403,\n",
      "         40.7271, -11.6191, -33.7336, -22.0147,  83.9446,  13.7202,  64.4795,\n",
      "         15.0853,  -0.9790,  -0.5301, -22.1384, -65.8134],\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([-22.4446,  -3.9253, 106.8839,  14.7367, -83.4087, -68.7292,  48.5521,\n",
      "         25.4342, 115.5247,  76.3833, -42.4529,  46.1396,  14.3169, -14.1612,\n",
      "        -52.3239,  23.2744,  72.6793,  -7.7414, -44.7457,   1.7712,  56.0855,\n",
      "        -33.4397, -17.3890,  -0.4904, -55.2360,  61.7612, -28.6299, -98.5403,\n",
      "         40.7271, -11.6191, -33.7336, -22.0147,  83.9446,  13.7202,  64.4795,\n",
      "         15.0853,  -0.9790,  -0.5301, -22.1384, -65.8134],\n",
      "       grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Manually create the first mini-batch without shuffling\n",
    "rpkms_in = torch.Tensor(rpkms[:128]).reshape((128, -1))\n",
    "tnfs_in = torch.Tensor(tnfs[:128]).reshape((128, -1))\n",
    "\n",
    "# Put VAE in testing mode - strictly not necessary here, since it goes in test\n",
    "# mode when encoding latent, as we did above\n",
    "vae.eval()\n",
    "\n",
    "# Calling the VAE as a function encodes and decodes the arguments,\n",
    "# returning the outputs and the two distribution layers\n",
    "depths_out, tnf_out, mu, logsigma = vae(rpkms_in, tnfs_in)\n",
    "\n",
    "# The mu layer is the encoding itself\n",
    "print(mu[0])\n",
    "\n",
    "# Now, delete the VAE\n",
    "del vae\n",
    "\n",
    "# And reload it:\n",
    "# Here we can specify whether to put it on GPU and whether it should start\n",
    "# in training or evaluation (encoding) mode. By default, it's not on GPU and \n",
    "# in testing mode\n",
    "vae = vamb.encode.VAE.load('/tmp/model.pt')\n",
    "depths_out, tnf_out, mu, logsigma = vae(rpkms_in, tnfs_in)\n",
    "print(mu[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We get the same values back, meaning the saved network is the same as the loaded network!\n",
    "\n",
    "<a id=\"cluster\"></a>\n",
    "## Binning the encoding\n",
    "\n",
    "__The role of clustering in Vamb__\n",
    "\n",
    "Fundamentally, the process of binning is just clustering sequences based on some of their properties, with the expectation that sequences from the same organism cluster together.\n",
    "\n",
    "In the previous step, we encoded each sequence to a vector of real numbers. The idea behind encoding is that, because the neural network attempts to strike a balance between having low reconstruction error (i.e. high fidelity) and having low Kullback-Leibler divergence (i.e. containing as little information as possible), then the network will preferentially encode signal over noise. In this way, the VAE act like a denoiser. This has been explored in other papers by people before us. Furthermore, because the latent space has fewer dimensions than the input space, clustering is quicker and more memory efficient.\n",
    "\n",
    "With the latent representation conveniently represented by an (n_contigs x n_features) matrix, you could use any clustering algorithm to cluster them (such as the ones in `sklearn.cluster`). In practice though, you have perhaps a million contigs and prior constrains on the diameter, shape and size of the clusters, so non-custom clustering algorithms will probably be slow and inaccurate.\n",
    "\n",
    "The module `vamb.cluster` implements a iterative medoid clustering algorithm. The algorithm is complicated and not very elegant, so I will go through it here in painstaking detail.\n",
    "\n",
    "### Overview of Vamb clustering algorithm\n",
    "\n",
    "In very broad strokes, the algorithm works like this:\n",
    "\n",
    "    A) Begin with an arbitrary point P, and move from P to a center of a nearby cluster, called the medoid M.\n",
    "    B) Find the distance d from M where the density of other points drops according to certain criteria.\n",
    "    C) If no d can be found, restart from A). If this has happens too often, relax criteria in B).\n",
    "    D) Else, output all points within d of M, and remove them from dataset.\n",
    "    E) Continue from A) until no points remain in the dataset.\n",
    "\n",
    "The idea behind this algorithm is that we expect the clusters to be roughly spherical, have a high density near the middle, and be separed form other clusters by an area with lower density. Hence, we should be able to move from P to M in step A) by moving towards higher density. And we should be able to find the radius of the cluster from M by considering a distance from M where the density of points drops.\n",
    "\n",
    "This algorithm consists of several steps, discussed below. Most of the steps unfortunately have several parameters which have been chosen by a combination of guessing, gut feeling and testing.\n",
    "\n",
    "As a distance measure, we are using cosine distance. This is particularly useful for a couple of reasons:\n",
    "\n",
    "First, it is extremely quick to calculate. Given two points represented by vectors $\\textbf{a}$ and $\\textbf{b}$, we define cosine distance as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{1}{2} - \\frac{\\textbf{a} \\cdot \\textbf{b}}{2|\\textbf{a}| |\\textbf{b}|}\n",
    "\\end{equation*}\n",
    "\n",
    "This is slightly different from the ordinary definition, but has the advantage of always being between 0 and 1, rather than -1 and 1.\n",
    "\n",
    "Now if we preprocess all points by calculating:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\bar{\\textbf{a}} = \\textbf{a} \\circ \\frac{1}{\\sqrt{2} \\textbf{|a|}}\n",
    "\\end{equation*}\n",
    "\n",
    "Where $\\circ$ is elementwise multiplication, then the cosine distance is simply:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{1}{2} - \\bar{\\textbf{a}} \\cdot \\bar{\\textbf{b}}\n",
    "\\end{equation*}\n",
    "\n",
    "And since the points are represented by a matrix $\\bar{\\textbf{M}}$, with each row vector being a point, finding the distance to from the i'th point of $\\bar{\\textbf{M}}$ to all other points is then a series of vector products as in the equation above, which can be expressed in a single matrix multiply:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{1}{2} - \\bar{\\textbf{M}} \\times \\bar{\\textbf{M}}_{i}^{T}\n",
    "\\end{equation*}\n",
    "\n",
    "Second, with all the $N_{latent}$ dimensions being used by the neural network the surface of an $N_{latent}$-dimensional hyperphere is so large that it's not likely that multiple clusters are far in euclidian space but close in cosine space.\n",
    "\n",
    "And third, it works better than the other alternatives we tries in practice, although I don't understand why for instance Euclidian distance is not better.\n",
    "\n",
    "### Finding the medoid\n",
    "\n",
    "First, we shuffle the points pseudorandomly to avoid any bias introduced by the order of the sequences. Then, we use the following algorithm to find the medoid. The idea is that, as you move nearer to a cluster's centre, the mean distance to close points will decrease.\n",
    "\n",
    "Currently, the default parameters DC and MAX_STEPS are 0.05 and 25, respectively.\n",
    "\n",
    "Function `wander_medoid`\n",
    "    1. Pick next point S as the seed. If last point has been reached, begin from top again.\n",
    "    2. Let C be the set of contigs within a small distance DC of S\n",
    "    3. Calculate mean distance of points in C to S, ignoring the distance of S to itself. Let that be mean_S.\n",
    "    4. Sample a point P in C. While you have not yet futilely sampled MAX_STEPS times or exhaused all points in C:\n",
    "        Calculate mean distance of P to all points within DC of P, let that be mean_P\n",
    "        If mean_P < mean_S, set S to P and go to point 2.\n",
    "    7. Return S as medoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the right distance\n",
    "\n",
    "Having found a medoid above, we need to find out if it's the center of a suitable cluster, and if so, what the radius of that cluster is.\n",
    "\n",
    "In order to see how this is done, it is useful to look at these graphs first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.LineCollection at 0x12b88e748>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot(xs, ys, title, xlabel, ylabel):\n",
    "    plt.figure()\n",
    "    plt.plot(xs, ys, color='k')\n",
    "    plt.tick_params(left=False, labelleft=False, labelsize=13)\n",
    "    plt.ylim(0, plt.ylim()[1])\n",
    "    plt.xlim(0, xs.max())\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel(xlabel, fontsize=14)\n",
    "    plt.ylabel(ylabel, fontsize=14)\n",
    "\n",
    "histogram = vamb.vambtools.read_npz('histogram.npz')\n",
    "xs = np.linspace(0, 1, 400)\n",
    "plot(xs[1:], histogram, \"Figure A: Histogram of all distances\", 'distance', 'density')\n",
    "plot(xs[1:80], histogram[:79], \"Figure B: Zoom-in on low X values\", 'distance', 'density')\n",
    "threshold, success = vamb.cluster._find_threshold(histogram, 0.1, 0.09)\n",
    "plt.vlines(threshold, 0, plt.ylim()[1], linestyles='dotted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When picking an arbitrary contig as medoid (here contig 'S4C11236' in the CAMI2 toy human Airways short read dataset), and calculating the distances to all other contigs in the dataset, it follows a distribution as seen in Figure A. Unsurprisingly, the latent representation of most other contigs are mostly uncorrelated with the chosen contig, so the large majority of points are at a distance of around 0.5. No contigs have a larger distance than 0.8. \n",
    "\n",
    "But look at the lower left corner of Figure A: A small group of contigs appear to have a smaller distance to the medoid than most others - those of the cluster it belongs to. If the cluster is well-separated, this small initial peak should be mostly isolated from the main peak around 0.5 by a band of empty space, with a low density. Figure B confirms this is true.\n",
    "\n",
    "Vamb groups the observed distances from the medoid M to a histogram similar to that in Figure B, and then iterates over the columns histogram attempting to find first an initial peak, then a valley. At the bottom of the valley, the cutoff distance threshold is set, and all points closer to M than this point is clustered out. This threshold is depicted in Figure B by the dotted line.\n",
    "\n",
    "The algorithm for detecting the threshold is as follows:\n",
    "\n",
    "Parameter DEFAULT is 0.09 by default. RATIO is provided to the function and will be described later.\n",
    "\n",
    "Function `find_threshold`\n",
    "    1. Calculate the distances from M to all other points. Group to histogram with bin size of 0.005 and smooth using a simple Gaussian kernel.\n",
    "    2. If there are no other points within 0.03, return (0.025, None) end function\n",
    "    3. Set success to FALSE. For X = 0 to X = 0.3:\n",
    "        If the peak has not yet been declared over:\n",
    "            If density is increasing:\n",
    "                Set peak to X, peak density to density\n",
    "                If X > 0.1, there is no initial peak near X=0, end loop\n",
    "            Else if density < 0.6 * peak density\n",
    "                Declare initial peak to be over\n",
    "                Set minimum to density\n",
    "        Else\n",
    "            If density > 1.5 * minimum, we are entering another peak, end loop\n",
    "            If density < minimum\n",
    "                Set minimum to density\n",
    "                if density < RATIO * peak density, we have successfully found a potential threshold\n",
    "                    set success to TRUE\n",
    "                    set threshold to X\n",
    "                    \n",
    "    4. If threshold is set to above 0.14 + 0.1 * RATIO: Initial peak is not close enough to X=0, set success to FALSE\n",
    "    5. If no threshold has been found and RATIO > 0.55: We do not accept failure. Set success to None and threshold to DEFAULT\n",
    "    6. Return (threshold, success)\n",
    "    \n",
    "This function is annoyingly convoluted, but I was not able to find a simpler function that did not return nonsensical thresholds for a variety of different data points.\n",
    "\n",
    "You will notice that the parameter RATIO controls how strict the criteria for accepting a threshold as successful is. If RATIO is low, say, 0.1, the initial peak must lie close to zero, and must be separated from the bulk of the other points with a deep valley. In constrast, a larger RATIO makes the criteria more relatex. With a RATIO > 0.55, the function does not even accept failure, and will set the threshold to DEFAULT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "With the medoid search function and the threshold detection above, you can see how to begin clustering points. However, one more problem remains: Not all medoids separate the points to a neat small peak of close contigs and a larger one of far contigs as seen in Figure B. In fact, for the majority of medoids M sampled, the density is continuously increasing with distance, or the initial peak is too far away to consider M to be part of the peak, or there is clearly an initial double-peak rather than one. To make things worse, if a medoid M cannot successfully return a threshold, this does not mean M must be discarded - it is quite likely that M is contained in a cluster for which a threshold could be successfully found using another point as medoid.\n",
    "\n",
    "To handle this problem, Vamb takes the approach of beginning with strict requirements for when a threshold is accepted, by setting the RATIO parameter low. If the threshold detection for a medoid returns failure, that medoid is simply skipped in the hope of finding another medoid with better results. Vamb then keeps count of how many of the recent attempts that yielded success - if too few did, RATIO is increased to relax the requirements for clusters.\n",
    "\n",
    "The algorithm is as follows:\n",
    "\n",
    "Parameters WINDOWSIZE and MINSUCCESSES is 200 and 15 by default, respectively. RATIO is initialized as 0.1\n",
    "\n",
    "Function `cluster`\n",
    "    1. Pick a medoid M using function `wander_medoid`\n",
    "    2. Set threshold to None. While threshold is None:\n",
    "           Set (success, threshold) to result of function `find_threshold(M)`\n",
    "           If fewer than MINSUCCESSES of last WINDOWSIZE attempts at finding threshold succeded:\n",
    "               Increment RATIO by 0.1\n",
    "               Discard the memory of the success or failure of previous attempts\n",
    "    3. Output all points within threshold of M as a cluster. Remove these points from dataset\n",
    "    4. If no more points remain, end function. Else, go to point 1.\n",
    "    \n",
    "Let's have a look at the actual Python function that does the clustering in Vamb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function cluster in module vamb.cluster:\n",
      "\n",
      "cluster(matrix, labels=None, maxsteps=25, windowsize=200, minsuccesses=15, default=0.09, destroy=False, normalized=False, cuda=False, logfile=None)\n",
      "    Iterative medoid cluster generator. Yields (medoid), set(labels) pairs.\n",
      "    \n",
      "    Inputs:\n",
      "        matrix: A (obs x features) Numpy matrix of data type numpy.float32\n",
      "        labels: None or Numpy array/list with labels for seqs [None = indices+1]\n",
      "        maxsteps: Stop searching for optimal medoid after N futile attempts [25]\n",
      "        default: Fallback threshold if cannot be estimated [0.09]\n",
      "        windowsize: Length of window to count successes [200]\n",
      "        minsuccesses: Minimum acceptable number of successes [15]\n",
      "        destroy: Save memory by destroying matrix while clustering [False]\n",
      "        normalized: Matrix is already preprocessed [False]\n",
      "        cuda: Use CUDA (GPU acceleration) based on PyTorch. [False]\n",
      "        logfile: Print threshold estimates and certainty to file [None]\n",
      "    \n",
      "    Output: Generator of (medoid, set(labels_in_cluster)) tuples.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(vamb.cluster.cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "An explanation of a few of the parameters is in order:\n",
    "    \n",
    "`matrix` is the latent encoding.\n",
    "\n",
    "`labels` is an array or a list of the name of each sequence, such that the i'th row of `matrix` has the label `labels[i]`\n",
    "\n",
    "`maxsteps`, `default`, `windowsize`, and `minsuccesses` correspond to the parameters described in the algorithms above. It's not realistic to expect users to set these.\n",
    "\n",
    "The function iteratively destroys the input matrix. To avoid this, the function works on a copy of the matrix. To skip copying the matrix and save memory, set `destroy` to `True`.\n",
    "\n",
    "For clustering, the matrix needs to be preprocessed (as described above in the clustering algorithm). If this has already been done, set `normalized` to `True`.\n",
    "\n",
    "If `cuda` is set to `True`, the clustering will be run on GPU for a significant speedup. Currently, this is not implemented, but will hopefully be shortly.\n",
    "\n",
    "---\n",
    "Depending on the size of the latent encoding, clustering can take quite some time. The heavy lifting here is done in Numpy, so it might be worth making sure the BLAS library your Numpy is using is fast. The difference between a fast and a slow Numpy implementation can be quite remarkable. You can check it with `numpy.__config__.show()` and if it says anything other than `NOT AVAILABLE` under the `mkl` or `openblas` entries, you're golden. \n",
    "\n",
    "The output of `vamb.cluster.cluster` is a generator, yielding `(medoid, cluster)` tuples. The generator will compute the clusters on-the-fly, meaning it will only compute the next cluster *once you ask for it*. Having the clustering return a generator gives a lot of flexibility:\n",
    "\n",
    "You can manually iterate over the clusters:\n",
    "    \n",
    "    clusters = dict()\n",
    "    for n, (medoid, cluster) in enumerate(vamb.cluster.cluster(latent)):\n",
    "        clusters[medoid] = cluster\n",
    "        \n",
    "        if n + 1 == 1000: # Stop after 1000 clusters\n",
    "            break\n",
    "            \n",
    "You can just put it directly in a dictionary:\n",
    "\n",
    "    clusters = dict(vamb.cluster.cluster(latent))\n",
    "    \n",
    "Or you can use the `vamb.cluster.writeclusters` function to write the clusters to disk on the fly:\n",
    "\n",
    "    cluster_iterator = vamb.cluster.cluster(latent)\n",
    "    with open('clusters.tsv', 'w') as clusterfile:\n",
    "        vamb.cluster.writeclusters(clusterfile, cluster_iterator)\n",
    "        \n",
    "In this example, we will load it into a dictionary immediately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First key: 101_NODE_195_length_36739_cov_33.2899 (of type: <class 'numpy.str_'> )\n",
      "Type of values: <class 'set'>\n",
      "First element of value: 196_NODE_1255_length_6883_cov_8.3666 of type: <class 'numpy.str_'>\n"
     ]
    }
   ],
   "source": [
    "# Notice we mask the contignames, since the dataloader filtered some contigs away\n",
    "cluster_iterator = vamb.cluster.cluster(latent, labels=np.array(contignames)[mask])\n",
    "clusters = dict(cluster_iterator)\n",
    "\n",
    "medoid, contigs = next(iter(clusters.items()))\n",
    "print('First key:', medoid, '(of type:', type(medoid), ')')\n",
    "print('Type of values:', type(contigs))\n",
    "print('First element of value:', next(iter(contigs)), 'of type:', type(next(iter(contigs))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"postprocessing\"></a>\n",
    "## Postprocessing the clusters\n",
    "\n",
    "We haven't written any dedicated postprocessing modules because how to postprocess really depends on what you're looking for in your data.\n",
    "\n",
    "One of the greatest weaknesses of Vamb - probably of metagenomic binners in general - is that the bins tend to be highly fragmented. Unlike other binners, Vamb does not hide this fact, and will bin *all* input contigs. This means you'll have lots of tiny bins, some of which are legitimate (viruses, plasmids), but most are parts of larger genomes that didn't get binned properly - about 2/3 of the bins here, for example, are 1-contig bins. \n",
    "\n",
    "We're in the process of developing a tool for annotating, cleaning and merging bins based on phylogenetic analysis of the genes in the bins. That would be extremely helpful, but for now, we'll have to use more crude approaches.\n",
    "\n",
    "First, we will split up all bins by their sample of origin. This will cause some bins to be fragmented, but for bins of high per-sample coverage, it will deduplicate them effectively. Second, we'll throw away all bins with less than 200,000 basepairs because we're only interested in genome-sized bins.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bins before splitting and filtering: 562\n",
      "Number of bins after splitting and filtering: 133\n"
     ]
    }
   ],
   "source": [
    "def splitclusters(clusters, sep):\n",
    "    # First split by sample\n",
    "    split = dict()\n",
    "    for n, (clustername, contigs) in enumerate(clusters.items()):\n",
    "        for contig in contigs:\n",
    "            samplename = contig.partition(sep)[0]\n",
    "            newclustername = str(n) + '_' + samplename\n",
    "            if newclustername in split:\n",
    "                split[newclustername].add(contig)\n",
    "            else:\n",
    "                split[newclustername] = {contig}\n",
    "    return split\n",
    "\n",
    "def filterclusters(clusters, lengthof):\n",
    "    # Now filter away the small bins\n",
    "    filtered_bins = dict()\n",
    "    for medoid, contigs in clusters.items():\n",
    "        binsize = sum(lengthof[contig] for contig in contigs)\n",
    "    \n",
    "        if binsize >= 200000:\n",
    "            filtered_bins[medoid] = contigs\n",
    "    \n",
    "    return filtered_bins\n",
    "        \n",
    "lengthof = dict(zip(contignames, lengths))\n",
    "filtered_bins = filterclusters(splitclusters(clusters, '_'), lengthof)\n",
    "print('Number of bins before splitting and filtering:', len(clusters))\n",
    "print('Number of bins after splitting and filtering:', len(filtered_bins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Even as we split most bins in 6, we're still left with fewer than 1/4 of the starting number of bins! Now, let's save the clusters to disk. For this we will use two writer functions:\n",
    "\n",
    "1) `vamb.cluster.writeclusters`, that writes which clusters contains which contigs to a simple tab-separated file, and\n",
    "\n",
    "2) `vamb.vambtools.writebins`, that writes FASTA files corresponding to each of the bins to a directory. This might be useful for some types of analysis you want to do down the road.\n",
    "\n",
    "We will need to load all the contigs belonging to any bin into memory to use `vamb.vambtools.writebins`. If the contigs in your bins don't fit in memory, sorry, you gotta find another way to make those FASTA bins.\n",
    "\n",
    "The cluster name when printing either way will be the dictionary key of the bins.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This writes a .tsv file with the clusters and corresponding sequences\n",
    "with open('/Users/jakni/Downloads/example/clusters.tsv', 'w') as file:\n",
    "    vamb.cluster.write_clusters(file, filtered_bins)\n",
    "\n",
    "# Only keep contigs in any filtered bin in memory\n",
    "keptcontigs = set.union(*filtered_bins.values())\n",
    "\n",
    "with open('/Users/jakni/Downloads/example/contigs.min2kbp.fna', 'rb') as file:\n",
    "    fastadict = vamb.vambtools.loadfasta(file, keep=keptcontigs)\n",
    "    \n",
    "bindir = '/Users/jakni/Downloads/example/bins'\n",
    "vamb.vambtools.write_bins(bindir, filtered_bins, fastadict, maxbins=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "## Summary of full workflow\n",
    "\n",
    "This is the full default workflow from beginning to end. Calling Vamb from command line does essentially this, except with some logging & deleting & saving intermediate results to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('/home/jakni/Documents/scripts/vamb')\n",
    "import vamb\n",
    "\n",
    "bamdir = '/home/jakni/Downloads/example/bamfiles/'\n",
    "bampaths = [bamdir + filename for filename in os.listdir(bamdir) if filename.endswith('.bam')]\n",
    "\n",
    "with open('/home/jakni/Downloads/example/contigs.fna', 'rb') as contigfile:\n",
    "    tnfs, contignames, contiglengths = vamb.parsecontigs.read_contigs(contigfile)\n",
    "\n",
    "rpkms = vamb.parsebam.read_bamfiles(bampaths)\n",
    "\n",
    "vae = vamb.encode.VAE(nsamples=rpkms.shape[1])\n",
    "dataloader, mask = vamb.encode.make_dataloader(rpkms, tnfs)\n",
    "vae.trainmodel(dataloader)\n",
    "\n",
    "latent = vae.encode(dataloader)\n",
    "\n",
    "cluster_iterator = vamb.cluster.cluster(latent, labels=np.array(contignames)[mask])\n",
    "\n",
    "with open('/home/jakni/Downloads/example/bins.tsv', 'w') as binfile:\n",
    "    vamb.cluster.write_clusters(binfile, cluster_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"memory\"></a>\n",
    "## Running VAMB with low memory (RAM)\n",
    "\n",
    "In the VAMB software, a series of tradeoffs can be taken to decrease RAM consumption, usually to the detriment of some other property like speed or convenience. With all of those tradeoffs taken, VAMB is relatively memory efficient. By default, several of these tradeoffs are are not taken when running from a Python interpreter, however they are all enabled when running VAMB from command line.\n",
    "\n",
    "The memory consumption of the encoding step is usually the bottleneck. With all memory-saving options enabled, this uses approximately $5*N_{contigs}*(136+N_{latent}+N_{samples})$ bytes, including overhead. As a rule of thumb, if you don't have at least three times that amount of memory, you might want to enable some or all of these memory optimizations.\n",
    "\n",
    "Here's a short list of all the available memory saving options:\n",
    "\n",
    "- When using `vamb.parsecontigs.read_contigs`, set the `preallocate` keyword to `True`. This doubles the time spent in the function, but significantly reduces RAM usage during this step.\n",
    "- Pass a path to the `dumpdirectory` in the `vamb.parsecontigs.read_bamfiles` function. Temporary files will be written to files in this directory to avoid keeping them in memory. This takes a little bit of disk space, but saves lots of RAM\n",
    "- In the `encode.make_dataloader` function, set `destroy` to True. This will in-place modify your RPKM and TNF array, deleting unusable rows and normalizing them. This prevents the creation of a copy of the data.\n",
    "- Similarly, set `destroy=True` when using the `cluster.cluster` function. Again, the input (latent) datasets are modified in-place, saving a copy of the data. This function will completely consume the input array.\n",
    "- When clustering, instead of generating clusters in memory, instantiate the cluster iterator and pass it directly to `cluster.write_clusters`. This way, each cluster will be written to disk after creation and not accumulate in memory. This obviously requires disk space, and you will have to reload the clusters if you are planning on using them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"benchmark\"></a>\n",
    "## Optional: Benchmarking your bins\n",
    "\n",
    "If you want to tweak or enchance Vamb, you'll want to know how well it performs. For this to make any sense, you need to have a *reference*, that is, a list of bins that are deemed true and complete. Otherwise, what do you benchmark against?\n",
    "\n",
    "Figuring out a proper way to benchmark binning tools is actually really hard because it requires taking a lot of subjective choices. When is a bin properly predicted? If we only accept perfectly binned bins, basically all binners have a 0 % accuracy. For any one bin we can calculate the fraction of the true genome present and the fraction of contaminating DNA - but if e.g. transposons are harder to bin but have no significant phenotypical effect, should they really count? And if a true bin is split across 25 % in four different observed bins, does that mean 25 % of the true genome is present in those bins, or is it in fact 0 %, because each of those bins will be predicted to be something different? If contigs are present in multiple bins, how do you prevent a bin from having more than 100 % presence? If your reference contains a broad selection of similar strains, surely it's not as bad to mix strains of the same species as to mix genomes from different phyla? And should you be benchmarked on the number of good bins you construct, or the fraction of good bins out of all the bins you make?\n",
    "\n",
    "These choices have a significant impact on how you asses performance of your binner. For example, early on in the development of Vamb, we *did* allow the same contig present in multiple bins to both count towards completeness of bins. This led to a bias towards creating a huge number of overlapping bins until we changed the benchmarking.\n",
    "\n",
    "Vamb's benchmarking works in the following way: You count the number of genomes for which *any* bin has a competeness (recall) above a certain level and a contamination below a certain level (precision above a certain level). Recall and precision is calculated by the number of basepairs covered in the right or wrong genome. In other words, for a bin B and a genome G, the recall is (# basepairs of G covered by a contig of B) / (# basepairs of G covered by any contig given to the binner), and precision is (# basepairs of G covered by a contig of B) / (# basepairs of any genome covered by a contig in B). An bin *can* count towards multiple genomes given a low enough precision threshold.\n",
    "\n",
    "Let's go through how to benchmark your bins.\n",
    "\n",
    "---\n",
    "\n",
    "First, you need a proper reference. For each contig (or sequence) you are binning, you need to provide:\n",
    "* The name of the contig\n",
    "* The name of the genome the contig is coming from (genome names may be arbitrarily chosen)\n",
    "* The name of the reference contig from that genome (may also be arbitrarily chosen)\n",
    "* The leftmost position (start) of where the contig aligns to the reference contig\n",
    "* The rightmost position (end) of where the contig aligns to the reference contig.\n",
    "\n",
    "This information is given as a tab-separated file, with one line per contig. For example, suppose that I have a contig `test_contig_45` which I have located to belong to position 113501 to position 132884 in the reference contig `NC_013740.1` of the genome of `Acidaminococcus fermentans`. Then, one of the lines in the reference file will be:\n",
    "\n",
    "    test_contig_45     Acidaminococcus fermentans       NC_013740.1    113501     132884\n",
    "\n",
    "This file can then be loaded into a `Reference` object:\n",
    "    \n",
    "    with open('/path/to/ref_file.tsv') as ref_file:\n",
    "        reference = Reference.from_file(ref_file)\n",
    "\n",
    "Then you need to load your observed bins in. The format of this file is expected to be the same as the output of Vamb, namely a two-column file, with arbitrary bin names in the first column and contig names in the second column. Any contig seen in the observed bins is expected to be present in the reference as well. You also need to provide the reference:\n",
    "\n",
    "    with open('/path/to/clusters.tsv') as clusters_file:\n",
    "        bins = Binning.from_file(clusters_file, reference)\n",
    "        \n",
    "You can the query the bins for information about the quality of your bins. Below I'll show an example of this in practice: I will use Vamb's performance on the `metabat_errorfree` dataset and compare it to that of METABAT2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gi|224815735|ref|NZ_ACGB01000001.1|_[Acidaminococcus_D21_uid55871]_1-5871\tAcidaminococcus_D21_uid55871\tNZ_ACGB01000001.1\t1\t5871\r\n",
      "gi|224815735|ref|NZ_ACGB01000001.1|_[Acidaminococcus_D21_uid55871]_5841-8340\tAcidaminococcus_D21_uid55871\tNZ_ACGB01000001.1\t5841\t8340\r\n",
      "gi|224815735|ref|NZ_ACGB01000001.1|_[Acidaminococcus_D21_uid55871]_8310-10809\tAcidaminococcus_D21_uid55871\tNZ_ACGB01000001.1\t8310\t10809\r\n",
      "gi|224815735|ref|NZ_ACGB01000001.1|_[Acidaminococcus_D21_uid55871]_10779-29944\tAcidaminococcus_D21_uid55871\tNZ_ACGB01000001.1\t10779\t29944\r\n",
      "gi|224815735|ref|NZ_ACGB01000001.1|_[Acidaminococcus_D21_uid55871]_29914-33073\tAcidaminococcus_D21_uid55871\tNZ_ACGB01000001.1\t29914\t33073\r\n",
      "gi|224815735|ref|NZ_ACGB01000001.1|_[Acidaminococcus_D21_uid55871]_33043-41174\tAcidaminococcus_D21_uid55871\tNZ_ACGB01000001.1\t33043\t41174\r\n",
      "gi|224815735|ref|NZ_ACGB01000001.1|_[Acidaminococcus_D21_uid55871]_41144-44994\tAcidaminococcus_D21_uid55871\tNZ_ACGB01000001.1\t41144\t44994\r\n",
      "gi|224815735|ref|NZ_ACGB01000001.1|_[Acidaminococcus_D21_uid55871]_44964-53996\tAcidaminococcus_D21_uid55871\tNZ_ACGB01000001.1\t44964\t53996\r\n",
      "gi|224815735|ref|NZ_ACGB01000001.1|_[Acidaminococcus_D21_uid55871]_53966-59194\tAcidaminococcus_D21_uid55871\tNZ_ACGB01000001.1\t53966\t59194\r\n",
      "gi|224815735|ref|NZ_ACGB01000001.1|_[Acidaminococcus_D21_uid55871]_59164-65356\tAcidaminococcus_D21_uid55871\tNZ_ACGB01000001.1\t59164\t65356\r\n"
     ]
    }
   ],
   "source": [
    "# First load in the Reference\n",
    "reference_path = '/Users/jakni/Downloads/metahit/reference.tsv'\n",
    "\n",
    "!head $reference_path # show first 10 lines of reference file\n",
    "\n",
    "with open(reference_path) as reference_file:\n",
    "    reference = vamb.benchmark.Reference.from_file(reference_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The first 10 lines wrap, but you can see the information expected to be present.\n",
    "\n",
    "The `reference` object contains a bunch of attributes which keeps track of which contigs belongs to which bins. You can see which ones using good ol' `help`:\n",
    "\n",
    "`>>> help(reference)`\n",
    "\n",
    "    Help on Reference in module vamb.benchmark object:\n",
    "\n",
    "    class Reference(builtins.object)\n",
    "     |  A set of Genomes known to represent the ground truth for binning.\n",
    "     |  >>> print(my_genomes)\n",
    "     |  [Genome('E. coli'), ncontigs=95, breadth=5012521),\n",
    "     |   Genome('Y. pestis'), ncontigs=5, breadth=46588721)]\n",
    "     |  >>> Reference(my_genomes)\n",
    "     |  Reference(ngenomes=2, ncontigs=100)\n",
    "     |  \n",
    "     |  Properties:\n",
    "     |  self.genome: {genome_name: genome} dict\n",
    "     |  self.contigs: {contig_name: contig} dict\n",
    "     |  self.genomeof: {contig_name: genome} dict\n",
    "     |  self.ngenomes\n",
    "     |  self.ncontigs\n",
    "     |  \n",
    "     |  Methods defined here:\n",
    "\n",
    "    [ ... ]\n",
    "    \n",
    "Now the bins:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengthof = {k:v.end - v.start + 1 for k,v in reference.contigs.items()}\n",
    "\n",
    "with open('/Users/jakni/Downloads/metahit/clusters.tsv') as clusters_file:\n",
    "    vamb_bins = vamb.cluster.read_clusters(clusters_file)\n",
    "    vamb_bins = filterclusters(vamb_bins, lengthof)\n",
    "    vamb_bins = vamb.benchmark.Binning(vamb_bins, reference)\n",
    "    \n",
    "with open('/Users/jakni/Downloads/metahit/metabat_clusters.tsv') as clusters_file:\n",
    "    metabat_bins = vamb.cluster.read_clusters(clusters_file)\n",
    "    metabat_bins = filterclusters(metabat_bins, lengthof)\n",
    "    metabat_bins = vamb.benchmark.Binning(metabat_bins, reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The Binning object keeps track of your bins and how they relate to the `Reference`:\n",
    "\n",
    "    help(vamb_bins)\n",
    "    \n",
    "        class Binning(builtins.object)\n",
    "     |  The result of an Binning applied to a Reference.\n",
    "     |  >>> ref\n",
    "     |  (Reference(ngenomes=2, ncontigs=5)\n",
    "     |  >>> b = Binning({'bin1': {contig1, contig2}, 'bin2': {contig3, contig4}}, ref)\n",
    "     |  Binning(4/5 contigs, ReferenceID=0x7fe908180be0)\n",
    "     |  >>> b[(0.5, 0.9)] # num. genomes 0.5 recall, 0.9 precision\n",
    "     |  1\n",
    "     |  \n",
    "     |  Properties:\n",
    "     |  self.reference:       Reference object of this benchmark\n",
    "     |  self.recalls:         Sorted tuple of recall thresholds\n",
    "     |  self.precisions:      Sorted tuple of precision thresholds\n",
    "     |  self.nbins:           Number of bins\n",
    "     |  self.ncontigs:        Number of binned contigs\n",
    "     |  self.contigsof:       {bin_name: {contig set}}\n",
    "     |  self.binof:           {contig: bin_name}\n",
    "     |  self.breadthof:       {bin_name: breadth}\n",
    "     |  self.intersectionsof: {genome: {bin:_name: intersection}}\n",
    "     |  self.breadth:         Total breadth of all bins\n",
    "     |  self.mean_f1:         Mean F1 score among all Genomes\n",
    "     |  self.mean_mcc         Mean Matthew's correlation coef. among all Genomes\n",
    "     |  \n",
    "     |  Methods defined here:\n",
    "     \n",
    "     [ ... ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The easiest way to get information about the quality of a `Binning` is by using the `print_matrix` method:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vamb bins:\n",
      "\tRecall\n",
      "Prec.\t0.3\t0.4\t0.5\t0.6\t0.7\t0.8\t0.9\t0.95\t0.99\n",
      "0.3\t215\t212\t207\t194\t163\t104\t38\t11\t0\n",
      "0.4\t183\t180\t176\t166\t141\t93\t31\t9\t0\n",
      "0.5\t148\t145\t142\t133\t115\t77\t27\t8\t0\n",
      "0.6\t129\t127\t125\t117\t100\t65\t23\t7\t0\n",
      "0.7\t128\t126\t124\t116\t100\t65\t23\t7\t0\n",
      "0.8\t123\t122\t121\t113\t99\t64\t23\t7\t0\n",
      "0.9\t118\t117\t116\t108\t96\t63\t22\t7\t0\n",
      "0.95\t110\t109\t108\t103\t91\t59\t20\t6\t0\n",
      "0.99\t81\t80\t79\t77\t68\t49\t18\t5\t0\n",
      "\n",
      "METABAT2 bins:\n",
      "\tRecall\n",
      "Prec.\t0.3\t0.4\t0.5\t0.6\t0.7\t0.8\t0.9\t0.95\t0.99\n",
      "0.3\t175\t158\t138\t122\t88\t34\t6\t1\t0\n",
      "0.4\t154\t140\t125\t112\t79\t27\t4\t1\t0\n",
      "0.5\t137\t123\t110\t100\t68\t22\t3\t1\t0\n",
      "0.6\t127\t113\t101\t91\t62\t20\t3\t1\t0\n",
      "0.7\t121\t108\t99\t89\t60\t20\t3\t1\t0\n",
      "0.8\t116\t103\t94\t85\t57\t20\t3\t1\t0\n",
      "0.9\t105\t93\t87\t81\t53\t18\t2\t0\t0\n",
      "0.95\t91\t83\t78\t74\t48\t17\t2\t0\t0\n",
      "0.99\t68\t60\t57\t54\t37\t15\t2\t0\t0\n"
     ]
    }
   ],
   "source": [
    "print('Vamb bins:')\n",
    "vamb_bins.print_matrix()\n",
    "print('\\nMETABAT2 bins:')\n",
    "metabat_bins.print_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "On the [metabat webpage](https://bitbucket.org/berkeleylab/metabat/wiki/CAMI) they have a neat plot where they plot the number of observed bins at different recalls for a specific specificity. Just for fun, let's recreate that here with the metabat_errorfree data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "for precision in 0.95, 0.9:\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    colors = ['#DDDDDD', '#AAAAAA', '#777777', '#444444', '#000000']\n",
    "    recalls = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    for y, bins in zip((0, 1), (vamb_bins, metabat_bins)):\n",
    "        for color, recall in zip(colors, recalls):\n",
    "            plt.barh(y, bins[recall, precision], color=color)\n",
    "\n",
    "    plt.title(str(precision), fontsize=18)\n",
    "    plt.yticks([0, 1], ['Vamb', 'METABAT2'], fontsize=16)\n",
    "    plt.xticks([i*25 for i in range(5)], fontsize=13)\n",
    "    plt.legend([str(i) for i in recalls], bbox_to_anchor=(1, 1.1), title='Recall', fontsize=12)\n",
    "    \n",
    "    if precision == 0.9:\n",
    "        plt.xlabel('# of Genomes Identified', fontsize=16)\n",
    "    plt.gca().set_axisbelow(True)\n",
    "    plt.grid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
