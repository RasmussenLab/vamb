{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walkthough of Vamb from the Python interpreter\n",
    "\n",
    "The Vamb pipeline consist of a series of tasks each which have a dedicated module:\n",
    "\n",
    "1) Parse fasta file and get TNF of each sequence, as well as sequence length and names (module `parsecontigs`)\n",
    "\n",
    "2) Parse the BAM files and get abundance estimate for each sequence in the fasta file (module `parsebam`)\n",
    "\n",
    "3) Train a VAE witthe depths and TNF matrices, and encode it to a latent representation (module `encode`)\n",
    "\n",
    "4) Cluster the encoded inputs to metagenomic bins (modules `threshold` and `cluster`)\n",
    "\n",
    "Additionally, for developing and testing Vamb, we use:\n",
    "\n",
    "5) Benchmark the resulting bins against a gold standard using `benchmark`\n",
    "\n",
    "In the following chapters of this walkthrough, we will go through each step in more detail from within the Python interpreter. We will first show how to use Vamb by example, then explain what each step does, some of the theory behind the actions, and the different parameters that can be set. With this knowledge, you should be able to extend or alter the behaviour of Vamb more easily.\n",
    "\n",
    "For the examples, we will assume the following relevant prerequisite files exists in the directory `/home/jakni/Downloads/example/`:\n",
    "\n",
    "* `contigs.fna` - The filtered FASTA contigs which were mapped against, and\n",
    "* `bamfiles/*.bam` - The 6 .bam files from mapping the reads to the contigs above.\n",
    "\n",
    "## Table of contents:\n",
    "\n",
    "### 1. [Importing Vamb and getting help](#importing)\n",
    "\n",
    "### 2. [Parse the FASTA file](#parsecontigs)\n",
    "\n",
    "### 3. [Parse the BAM files](#parsebam)\n",
    "\n",
    "### 4. [Train the autoencoder and encode input data](#encode)\n",
    "\n",
    "### 5. [Binning the encoding](#cluster)\n",
    "\n",
    "### 6. [Summary of full workflow](#summary)\n",
    "\n",
    "### 7. [Optional: Benchmarking your bins](#benchmark)\n",
    "\n",
    "<a id=\"importing\"></a>\n",
    "## Importing Vamb and getting help\n",
    "\n",
    "First step is to get Vamb imported\n",
    "    \n",
    "    [jakni@nissen:~]$ python\n",
    "    >>> import vamb\n",
    "    Traceback (most recent call last):\n",
    "      File \"<stdin>\", line 1, in <module>\n",
    "    ModuleNotFoundError: No module named 'vamb'\n",
    "    \n",
    "We're not in the directory containing the vamb directory. That means the directory containing the vamb directory is not in your sys.path. We need to either move the vamb directory to one of your sys.path dirs or add the directory containing the vamb directory to sys.path. We'll do the latter.\n",
    "    \n",
    "    [jakni@nissen:~]$ python\n",
    "    >>> import sys\n",
    "    >>> sys.path.append('/home/jakni/Documents/scripts/')\n",
    "    >>> import vamb\n",
    "\n",
    "Now we got it imported. When using Vamb, you'll almost certianly need help (we wish it was so easy you didn't, but making user friendly software is *hard!*).\n",
    "\n",
    "Luckily, there's the built-in `help` function in Python.\n",
    "\n",
    "---\n",
    "\n",
    "`>>> help(vamb)`\n",
    "    \n",
    "    Help on package vamb:\n",
    "\n",
    "    NAME\n",
    "        vamb - Variational Autoencoder for Metagenomic Binning\n",
    "\n",
    "    DESCRIPTION\n",
    "        Vamb does what it says on the tin - bins metagenomes using a variational autoencoder.\n",
    "        \n",
    "    [ lines elided ]\n",
    "    \n",
    "        General workflow:\n",
    "        1) Filter contigs by size using vamb.vambtools.filtercontigs\n",
    "        2) Map reads to contigs to obtain BAM file\n",
    "        3) Calculate TNF of contigs using vamb.parsecontigs\n",
    "        4) Create RPKM table using vamb.parsebam\n",
    "        5) Train autoencoder using vamb.encode\n",
    "        6) Cluster latent representation using vamb.cluster\n",
    "    \n",
    "    [ lines elided ]\n",
    "    \n",
    "---\n",
    "    \n",
    "The `PACKAGE CONTENTS` under `help(vamb)` is just a list of all *importable* files in the `vamb` directory - some of these aren't part of the Vamb package proper and really shouldn't be imported, so ignore that.\n",
    "\n",
    "---\n",
    "You can also get help for each of the modules, for example the `cluster` module:\n",
    "\n",
    "`>>> help(vamb.cluster)`\n",
    "\n",
    "    Help on module vamb.cluster in vamb:\n",
    "\n",
    "    NAME\n",
    "        vamb.cluster - Iterative medoid clustering.\n",
    "\n",
    "    DESCRIPTION\n",
    "        Usage:\n",
    "        >>> cluster_iterator = cluster(rpkms, tnfs, labels=contignames)\n",
    "        >>> clusters = dict(cluster_iterator)\n",
    "\n",
    "        Implements two core functions: cluster and tandemcluster, along with the helper\n",
    "        functions writeclusters and readclusters.\n",
    "        For all functions in this module, a collection of clusters are represented as\n",
    "        a {clustername, set(elements)} dict.\n",
    "\n",
    "        cluster algorithm:\n",
    "    \n",
    "    [ lines elided ]\n",
    "        \n",
    "---\n",
    "And for functions:\n",
    "\n",
    "`>>> help(vamb.cluster.tandemcluster)`\n",
    "\n",
    "    Help on function cluster in module vamb.cluster:\n",
    "\n",
    "    cluster(matrix, labels=None, inner=None, outer=None, max_steps=15, normalized=False, nsamples=2000, maxsize=2500, logfile=None)\n",
    "        Iterative medoid cluster generator. Yields (medoid), set(labels) pairs.\n",
    "\n",
    "        Inputs:\n",
    "            matrix: A (obs x features) Numpy matrix of values\n",
    "            labels: None or Numpy array with labels for matrix rows [None = ints]\n",
    "            inner: Optimal medoid search within this distance from medoid [None = auto]\n",
    "            outer: Radius of clusters extracted from medoid. [None = inner]\n",
    "            max_steps: Stop searching for optimal medoid after N futile attempts [15]\n",
    "            normalized: Matrix is already zscore-normalized [False]\n",
    "            nsamples: Estimate threshold from N samples [1000]\n",
    "            maxsize: Discard sample if more than N contigs are within threshold [2500]\n",
    "            logfile: Print threshold estimates and certainty to file [None]\n",
    "\n",
    "        Output: Generator of (medoid, set(labels_in_cluster)) tuples.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jakni/Documents/scripts/')\n",
    "import vamb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"parsecontigs\"></a>\n",
    "## Parse the FASTA file\n",
    "\n",
    "If you forget what to do at each step, remember that `help(vamb)` said:\n",
    "\n",
    "    General workflow:\n",
    "    1) Filter contigs by size using vamb.vambtools.filtercontigs\n",
    "    2) Map reads to contigs to obtain BAM file\n",
    "    3) Calculate TNF of contigs using vamb.parsecontigs\n",
    "    \n",
    "    [ lines elided ]\n",
    "\n",
    "Okay, we already have filtered contigs. I could have used the vamb.vambtools.filtercontigs to filter the FASTA file, but here, they were already filtered. We have also already mapped reads to them and gotten BAM files, so we begin with the third step, using the `vamb.parsecontigs` module. How do you use that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module vamb.parsecontigs in vamb:\n",
      "\n",
      "NAME\n",
      "    vamb.parsecontigs - Calculate z-normalized tetranucleotide frequency from a FASTA file.\n",
      "\n",
      "DESCRIPTION\n",
      "    Usage:\n",
      "    >>> with open('/path/to/contigs.fna', 'rb') as filehandle\n",
      "    ...     tnfs, contignames, lengths = read_contigs(filehandle)\n",
      "\n",
      "FUNCTIONS\n",
      "    read_contigs(byte_iterator, minlength=100)\n",
      "        Parses a FASTA file open in binary reading mode.\n",
      "        \n",
      "        Input:\n",
      "            byte_iterator: Iterator of binary lines of a FASTA file\n",
      "            minlength[100]: Ignore any references shorter than N bases \n",
      "        \n",
      "        Outputs:\n",
      "            tnfs: A (n_FASTA_entries x 136) matrix of tetranucleotide freq.\n",
      "            contignames: A lNumpy array of contig headers\n",
      "            lengths: A Numpy array of contig lengths\n",
      "\n",
      "FILE\n",
      "    /home/jakni/Documents/scripts/vamb/parsecontigs.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(vamb.parsecontigs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "I use `vamb.parsecontigs.read_contigs` with the inputs and outputs as written:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File must be opened in binary mode\n",
    "with open('/home/jakni/Downloads/example/contigs.fna', 'rb') as filehandle:\n",
    "    tnfs, contignames, lengths = vamb.parsecontigs.read_contigs(filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let's have a look at the resulting data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of tnfs: <class 'numpy.ndarray'> of dtype float32\n",
      "Shape of tnfs: (39551, 136)\n",
      "\n",
      "Type of contignames: <class 'numpy.ndarray'> of dtype <U39\n",
      "Length of contignames: 39551\n",
      "\n",
      "First 5 elements of contignames:\n",
      "s30_NODE_1_length_245508_cov_18.4904\n",
      "s30_NODE_2_length_222690_cov_39.7685\n",
      "s30_NODE_3_length_222459_cov_20.3665\n",
      "s30_NODE_4_length_173155_cov_20.1181\n",
      "s30_NODE_5_length_161239_cov_20.1237\n",
      "\n",
      "Type of lengths: <class 'numpy.ndarray'> of dtype int64\n",
      "Length of lengths: 39551\n",
      "\n",
      "First 5 elements of lengths:\n",
      "245508\n",
      "222690\n",
      "222459\n",
      "173155\n",
      "161239\n"
     ]
    }
   ],
   "source": [
    "print('Type of tnfs:', type(tnfs), 'of dtype', tnfs.dtype)\n",
    "print('Shape of tnfs:', tnfs.shape, end='\\n\\n')\n",
    "\n",
    "print('Type of contignames:', type(contignames), 'of dtype', contignames.dtype)\n",
    "print('Length of contignames:', len(contignames), end='\\n\\n')\n",
    "\n",
    "print('First 5 elements of contignames:')\n",
    "for i in range(5):\n",
    "    print(contignames[i])\n",
    "\n",
    "print('\\nType of lengths:', type(lengths), 'of dtype', lengths.dtype)\n",
    "print('Length of lengths:', len(lengths), end='\\n\\n')\n",
    "\n",
    "print('First 5 elements of lengths:')\n",
    "for i in range(5):\n",
    "    print(lengths[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "__For a gzipped FASTA file__, simply `import gzip` and open the file with `gzip.open('/path/to/contigs.fna', 'rb')`. Alternatively, you can use `with open vamb.vambtools.Reader('/path/to/contigs.fna' ,'rb')`, which will automatically detect whether the file is gzipped or not.\n",
    "\n",
    "__If you have ambigious DNA bases__ such as Y or W in the FASTA file, `read_contigs` will not work, since it can only understand A, C, G, T and N. To easily fix this, you can use the function `vamb.vambtools.maskbases`, which will convert any ambigious DNA bases to N.\n",
    "\n",
    "To illustrate, the following code can handle both gzipped files and non-gzipped files, whether or not there are any ambigious bases:\n",
    "\n",
    "    with vamb.vambtools.Reader('/path/to/contigs.fna', 'rb') as filehandle:\n",
    "        masked_lines = vamb.vambtools.maskbases(filehandle)\n",
    "        tnfs, contignames, lengths = vamb.parsecontigs.read_contigs(masked_lines)\n",
    "        \n",
    "Note that reading gzipped files and masking the contigs will slow down the FASTA parsing quite a bit. But the time spent parsing the FASTA file will likely still be insignificant compared to the other steps of Vamb.\n",
    "\n",
    "__The rationale for parsing the contigs__ is that it turns out that related organisms tend to share a similar kmer-distribution across most of their genome. The reason for that is not understood, even though it's believed that common functional motifs, GC-content and presence/absence of endonucleases explains some of the observed similary.\n",
    "\n",
    "The `tnfs` is the tetranucleotide frequency - it's the frequency of the canonical kmer of each 4mer in the contig. We use 4-mers because there are 136 canonical 4-mers, which is an appropriate number of features to cluster - not so few that there's no signal and not so many it becomes unwieldy and the estimates of the frequencies become uncertain. We could also have used 3-mers. In tests we have made, 3-mers are _almost_, but not quite as good as 4-mers for separating different species. You could probably switch tetranucleotide frequency to trinucleotide frequency in Vamb without any significant drop of accuracy. However, there are 512 canonical 5-mers, that would be too many features to handle comfortably, and it could easily cause memory issues.\n",
    "\n",
    "__The argument `minlength`__ sets the filter removing any contigs shorter than this. Short contigs in general do not work well with Vamb since neither TNF nor the abundance (shown in next section) can be estimated reliably for short contigs. When choosing the correct theshold, there is some sweet spot between on one hand not allowing small contigs to act as a source of noise in the binning, and on the other hand to not throw away more contigs than you absolutely have to. We don't know what the sweet spot is, but it's probably somewhere around ~2000 bp.\n",
    "\n",
    "The problem with filtering contigs using `minlength` is that the smaller contigs which are thrown away will still recruit reads during the mapping that creates the BAM files, thus removing information from those reads. For that reason, we recommend filtering the contigs *before* mapping by using the function `vamb.vambtools.filtercontigs`.\n",
    "\n",
    "__The memory consumption of Vamb can be an issue__, so at this point, you should probably consider whether you have enough RAM. If not, all the relevant modules have reading and writing functions so you can dump the results to disk and delete them from memory. This is a small dataset, so there's no problem. With hundreds of samples and millions of contigs however, this becomes a problem, even though Vamb is fairly memory-friendly.\n",
    "\n",
    "As a rule of thumb, the memory consumption for the most memory intensive step of Vamb is approximately 8 × (n_samples + 136) × n_contigs bytes plus a little bit of overhead. If this is significantly lower than your RAM, don't worry about it. If it's within a factor 2 of your available RAM, you'll need to delete objects you don't need anymore.\n",
    "\n",
    "<a id=\"parsebam\"></a>\n",
    "## Parse the BAM files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module vamb.parsebam in vamb:\n",
      "\n",
      "NAME\n",
      "    vamb.parsebam - Estimate RPKM (depths) from BAM files of reads mapped to contigs.\n",
      "\n",
      "DESCRIPTION\n",
      "    Usage:\n",
      "    >>> bampaths = ['/path/to/bam1.bam', '/path/to/bam2.bam', '/path/to/bam3.bam']\n",
      "    >>> rpkms = read_bamfiles(bampaths)\n",
      "\n",
      "FUNCTIONS\n",
      "    read_bamfiles(paths, minscore=50, minlength=100, subprocesses=4, logfile=None)\n",
      "        Spawns processes to parse BAM files and get contig rpkms.\n",
      "            \n",
      "        Input:\n",
      "            path: Path to BAM file\n",
      "            minscore [50]: Minimum alignment score (AS field) to consider\n",
      "            minlength [100]: Ignore any references shorter than N bases \n",
      "            subprocesses [4]: Number of subprocesses to spawn\n",
      "            logfile: [None] File to print progress to\n",
      "        \n",
      "        Output: A (n_contigs x n_samples) Numpy array with RPKM\n",
      "\n",
      "DATA\n",
      "    DEFAULT_SUBPROCESSES = 4\n",
      "\n",
      "FILE\n",
      "    /home/jakni/Documents/scripts/vamb/parsebam.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Again, we can use the help function to see what we need to do\n",
    "help(vamb.parsebam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jakni/Downloads/example/bamfiles/e101.filtered.bam',\n",
       " '/home/jakni/Downloads/example/bamfiles/e178.filtered.bam',\n",
       " '/home/jakni/Downloads/example/bamfiles/e179.filtered.bam',\n",
       " '/home/jakni/Downloads/example/bamfiles/e196.filtered.bam',\n",
       " '/home/jakni/Downloads/example/bamfiles/e198.filtered.bam',\n",
       " '/home/jakni/Downloads/example/bamfiles/e30.filtered.bam']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bamfiles = !ls /home/jakni/Downloads/example/bamfiles\n",
    "bamfiles = ['/home/jakni/Downloads/example/bamfiles/' + p for p in bamfiles]\n",
    "bamfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of rpkms: <class 'numpy.ndarray'> of dtype float32\n",
      "Shape of rpkms (39551, 6)\n"
     ]
    }
   ],
   "source": [
    "# That looks right.\n",
    "\n",
    "rpkms = vamb.parsebam.read_bamfiles(bamfiles) # This takes some time.\n",
    "print('Type of rpkms:', type(rpkms), 'of dtype', rpkms.dtype)\n",
    "print('Shape of rpkms', rpkms.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The idea here is that two contigs from the same genome will always be physically present together, and so they should have a similar abundance across all samples. Some contigs represent repeats like duplicated segments - these contigs should have a fixed ratio of abundance to other contigs. Thus, even when considering repeated contigs, there should be a tight Pearson correlation between abundances of contigs from the same genome.\n",
    "\n",
    "The `vamb.parsebam` module takes a rather crude approach to estimating abundance, namely by simply counting the number of mapped reads to each contig, normalized by total number of reads and the contig's length. This measure is in trancriptomics often called RPKM, *reads per kilobase per million mapped reads*. Other metagenomic binners like Metabat and Canopy uses an average of per-nucleotide depth of coverage instead. We do not believe there is any theoretical or practical advantage of using depth over RPKM. Because BWA handles redundant databases rather poorly, there is not even any advantage of using the technically more accurate FPKM over RPKM. We will use the terms *abundance*,  *depth* and *rpkm* interchangably.\n",
    "\n",
    "---\n",
    "We can see (in the default value for the `subprocesses` argument) that the default number of parallel BAM-reading processes it will spawn is 4. This is because Python detected 4 threads on my laptop. In general, Vamb's default here is to use the number of availble threads, or 8 threads if more than 8 is detected, as the BAM-reading function will almost certainly become IO bound at more than 8 threads.\n",
    "\n",
    "As with the `vamb.parsecontigs.read_contigs` function, I don't care about the `minlength` argument, since our fasta file is already filtered. Again, I will re-iterate that filtering the FASTA file _before_ mapping leads to the best results.\n",
    "\n",
    "Lastly, the function ignores all alignments with alignment score less than `minscore` (as determined by the `AS:i` field in the BAM file, which Vamb assumes is present). That threshold seems reasonable here.\n",
    "\n",
    "Lastly, the argument `logfile` should be `None` or the filehandle of an opened, writeable file. If the latter, it will print status updates to the logfile.\n",
    "\n",
    "---\n",
    "Now, I tend to be a bit ~~paranoid~~<sup>careful</sup>, so if I loaded in 500 GB of BAM files, I'd want to save the work I have now in case something goes wrong - and we're about to fire up the VAE so lots of things can go wrong.\n",
    "\n",
    "What importants objects do I have in memory right now?\n",
    "\n",
    "* `tnfs`: A Numpy array of tnfs\n",
    "* `contignames`: A Numpy of contignames\n",
    "* `lengths`: A Numpy array of contig lengths\n",
    "* `rpkms`: A Numpy array of rpkms\n",
    "\n",
    "I'm going to use `vamb.vambtools.write_npz` to save the Numpy arrays (that function is just a thin convenience wrapper for `numpy.savez_compressed`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/jakni/Downloads/example/contignames.npz', 'wb') as file:\n",
    "    vamb.vambtools.write_npz(file, contignames)\n",
    "\n",
    "with open('/home/jakni/Downloads/example/lengths.npz', 'wb') as file:\n",
    "    vamb.vambtools.write_npz(file, lengths)\n",
    "\n",
    "with open('/home/jakni/Downloads/example/tnfs.npz', 'wb') as file:\n",
    "    vamb.vambtools.write_npz(file, tnfs)\n",
    "    \n",
    "with open('/home/jakni/Downloads/example/rpkms.npz', 'wb') as file:\n",
    "    vamb.vambtools.write_npz(file, rpkms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"encode\"></a>\n",
    "## Train the autoencoder and encode input data\n",
    "\n",
    "Again, you can use `help` to see how to use the module\n",
    "\n",
    "`>>> help(vamb.encode)`\n",
    "\n",
    "    Help on module vamb.encode in vamb:\n",
    "\n",
    "    NAME\n",
    "        vamb.encode - Encode a depths matrix and a tnf matrix to latent representation.\n",
    "\n",
    "    DESCRIPTION\n",
    "        Creates a variational autoencoder in PyTorch and tries to represent the depths\n",
    "        and tnf in the latent space under gaussian noise.\n",
    "\n",
    "        usage:\n",
    "        >>> vae, dataloader = trainvae(depths, tnf) # Make & train VAE on Numpy arrays\n",
    "        >>> latent = vae.encode(dataloader) # Encode to latent representation\n",
    "        >>> latent.shape\n",
    "        (183882, 40)\n",
    "        \n",
    "    [ lines elided ]\n",
    "    \n",
    "---\n",
    "Aha, so we need to use the `trainvae` function first, then the `VAE.encode` method. You can call the `help` functions on those, but I'm not showing that here.\n",
    "\n",
    "Training networks always take some time. If you have a GPU and CUDA installed, you can pass `cuda=True` to `encode.trainvae` to train on your GPU for increased speed. With a beefy GPU, this can make quite a difference. I run this on my laptop, so I'll just use my CPU. And I'll run just 10 epochs rather than the more suitable 400:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCapacity: 3000\n",
      "\tMSE ratio: 0.1\n",
      "\tCUDA: False\n",
      "\tN latent: 100\n",
      "\tN hidden: 325, 325, 325\n",
      "\tN contigs: 39551\n",
      "\tN samples: 6\n",
      "\tN epochs: 10\n",
      "\tBatch size: 128\n",
      "\n",
      "\tEpoch: 1\tLoss: 0.7189260\tCE: 0.2086210\tMSE: 0.9005014\tKLD: 0.4034061\n",
      "\tEpoch: 2\tLoss: 0.4596830\tCE: 0.1292415\tMSE: 0.6989205\tKLD: 0.8499560\n",
      "\tEpoch: 3\tLoss: 0.3897613\tCE: 0.1084376\tMSE: 0.6253800\tKLD: 1.2430935\n",
      "\tEpoch: 4\tLoss: 0.3628313\tCE: 0.1014239\tMSE: 0.5663641\tKLD: 1.5712010\n",
      "\tEpoch: 5\tLoss: 0.3477682\tCE: 0.0977972\tMSE: 0.5240692\tKLD: 1.8611184\n",
      "\tEpoch: 6\tLoss: 0.3379934\tCE: 0.0956196\tMSE: 0.4911189\tKLD: 2.1106429\n",
      "\tEpoch: 7\tLoss: 0.3309353\tCE: 0.0942092\tMSE: 0.4623038\tKLD: 2.3319422\n",
      "\tEpoch: 8\tLoss: 0.3244467\tCE: 0.0929471\tMSE: 0.4347698\tKLD: 2.5377989\n",
      "\tEpoch: 9\tLoss: 0.3195574\tCE: 0.0919933\tMSE: 0.4139583\tKLD: 2.7369234\n",
      "\tEpoch: 10\tLoss: 0.3155504\tCE: 0.0912628\tMSE: 0.3952361\tKLD: 2.9371349\n"
     ]
    }
   ],
   "source": [
    "with open('/tmp/model.pt', 'wb') as modelfile:\n",
    "    vae, dataloader = vamb.encode.trainvae(rpkms, tnfs, nepochs=10, modelfile=modelfile, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This function first creates a dataloader from the inputs, and then trains the VAE using the data from the dataloader. The dataloader normalizes the TNF such that the mean and standard for each tetranucleotide across all contigs is 0 and 1, respectively, and normalizes the rpkm such that each contig sums to 1. Furthermore, the dataloader shuffles the contigs at each epoch.\n",
    "\n",
    "The VAE encodes the high-dimensional (n_samples + 136 features) input data in a lower dimensional space (nlatent features). When training, it learns an encoding scheme, with which it encodes the input data to a series of normal distributions, and a decoding scheme, in which it uses one value sampled from each normal distribution to reconstruct the input data.\n",
    "\n",
    "The theory here is that if the VAE learns to reconstruct the input, the distributions must be a more efficient encoding of the input data, since the same information is contained in fewer neurons. If the input data for the contigs indeed do fall into bins, an efficient encoding would be to simply encode the bin they belong to, then use the \"bin identity\" to reconstruct the data. We force it to encode to *distributions* rather than single values because this makes it more robust - it will not as easily overfit to interpret slightly different values as being very distinct if there is an intrinsic noise in each encoding.\n",
    "\n",
    "### The loss function\n",
    "\n",
    "The loss of the VAE consists of three major terms:\n",
    "\n",
    "* Cross entropy (CE) measures the dissimilarity of the reconstructed abundances to observed abundances. This penalizes a failure to reconstruct the abundances accurately.\n",
    "* Mean squared error (MSE) measures the dissimilary of reconstructed versus observed TNF. This penalizes failure to reconstruct TNF accurately.\n",
    "* Kullback-Leibler divergence (KLD) measures the dissimilarity between the encoded distributions and the standard gaussian distribution N(0, 1). This penalizes learning.\n",
    "\n",
    "All three terms are important. CE and MSE is necessary, because we believe the VAE can only learn to effectively reconstruct the input if it learns to encode the signal from the input into the latent layers. In other words, these terms incentivize the network to learn something. KLD is necessary because we care that the encoding is *efficient*, viz. it is contained in as little information as possible. The entire point of encoding is to encode a majority of the signal while shedding the noise, and this is only achieved if we place contrains on how much the network is allowed to learn. Without KLD, the network can theoretically learn an infinitely complex encoding, and the network will learn to encode both noise and signal.\n",
    "\n",
    "In `encode.py`, the loss function is written as:\n",
    "\n",
    "$\\alpha \\cdot CE + \\beta \\cdot MSE + \\gamma \\cdot KLD$\n",
    "\n",
    "Where both CE, MSE and KLD is calculated as means over the vectors for which they are defined. The constants $\\alpha$, $\\beta$ and $\\gamma$ are subject to the following constrains:\n",
    "\n",
    "1. As the learning rate is fixed and optimized for a specific gradient, this means the total loss $\\alpha \\cdot CE + \\beta \\cdot MSE + \\gamma \\cdot KLD$ should sum to a constant, lest the training become ustable. In our code, we want it to sum to 1.\n",
    "\n",
    "2. The amount of information the network can learn depends on the ratio $\\alpha \\cdot CE + \\beta \\cdot MSE \\cdot (\\gamma \\cdot KLD)^{-1} = R$. Because we want our network to learn a *fixed* amount of stuff, the KLD of a network that has been trained for some time can be treated as a constant, and so we let the user define a constant `capacity` and constrain $\\alpha$, $\\beta$ and $\\gamma$ such that $capacity = R \\cdot KLD = \\alpha \\cdot CE + \\beta \\cdot MSE \\cdot \\gamma^{-1}$.\n",
    "\n",
    "3. The relative ratio $\\beta \\cdot MSE \\cdot (\\alpha \\cdot CE + \\beta \\cdot MSE)^{-1}$ controls the incentive to learn to reconstruct TNF as opposed to abundances. This is user defined and called `mseratio` in the code.\n",
    "\n",
    "Now comes a problem. We want to set $\\alpha$, $\\beta$ and $\\gamma$ such that the above equations are satisfied, but we can't know *beforehand* what the CE, KLD or MSE is. And, in any rate, these values changes across the training run.\n",
    "\n",
    "What we do is to set $\\alpha$, $\\beta$ and $\\gamma$ relative to what CE and MSE would be in a totally *naive*, network which had *no knowledge* of the input dataset. This represents the state of the network before *any* learning is done. What would such a network predict? By the nature of our normalization of the means are 0 for the TNF values and 1/n for abundances, so a null model predicts 0 as TNF and, abundance as being $[n^{-1}, n^{-1} ... n^{-1}]^{T}$. This would result in a CE of $ln(n) * n^{-1}$ and an expected MSE of 1. \n",
    "\n",
    "Importantly, these values are rather close to the starting (i.e. untrained) values of a VAE because we softmax the reconstructed depths, but not the TNF. The KLD of a naive network with the static predictions of above is unfortunately undefined (because such a network could have *any* values of the latent layer and still produce the given outputs). However, we noticed that for values of $\\alpha$, $\\beta$ and $\\gamma$ which seem to work, it's the case that $\\alpha \\cdot CE + \\beta \\cdot MSE >> \\gamma \\cdot KLD$. Therefore, we modify constraint 1:\n",
    "\n",
    "1. $\\alpha \\cdot CE + \\beta \\cdot MSE = 1$\n",
    "\n",
    "which from constraint 2 implies that:\n",
    "\n",
    "2. $\\gamma = capacity^{-1}$\n",
    "\n",
    "From the above constrains follows:\n",
    "\n",
    "1. $\\alpha = n \\cdot (1 - msefactor) * ln(n)^{-1}$\n",
    "\n",
    "2. $\\beta = msefactor$\n",
    "\n",
    "3. $\\gamma = capacity^{-1}$\n",
    "\n",
    "If you look at the outputs from the 10 epochs, you can see the KL-divergence rises as it learns the dataset and the latent layer drifts away from its prior. At some point, it will begin to overfit too much, and the penalty associated with KL-divergence outweighs the CE and MSE losses. At this point, the KL will stall, and then fall. This point depends on `capacity` and the complexity of the dataset.\n",
    "\n",
    "Okay, so now we have the trained `vae` and the `dataloader`. Let's feed the dataloader to the VAE in order to get the latent representation-\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39551, 100)\n"
     ]
    }
   ],
   "source": [
    "# No need to pass gpu=True to the encode function to encode on GPU\n",
    "# If you trained the VAE on GPU, it already resides there\n",
    "latent = vae.encode(dataloader)\n",
    "\n",
    "print(latent.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "That's 39551 contigs each represented by the (non-noisy) value of 100 latent neurons.\n",
    "\n",
    "---\n",
    "Sometimes, you'll want to reuse a VAE you have already trained. For this, I've added the `VAE.save` method of the VAE class, as well as a `VAE.load` method. You will have noticed in the example above that I defined a `modelfile`, a file the VAE will create and save its parameters to. We can always use that file to recreate the VAE and have a pretrained model. But remember - a trained VAE only works on the dataset it's been trained on, and not necessarily on any other!\n",
    "\n",
    "I want to **show** that we get the exact same network back that we trained, so here I encode the first contig, delete the VAE, reload the VAE and encode the first contig again. The two encodings should be identical.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  52.5297,   -5.0748,   71.4529, -112.3738,  -43.6660,  -58.7396,\n",
      "         144.9612,   -9.5974,   63.5129, -100.5783,   80.3401,   74.2548,\n",
      "         -69.9850,  -20.0144,   37.0927,  -42.7093,  115.2146,  144.0393,\n",
      "         -43.8326,   63.3534,  -54.8382,   19.2759, -144.9675,  113.3119,\n",
      "          83.6159,   70.6486,   48.6149,  -76.9540,  -17.2142,   66.5361,\n",
      "         115.0852,  -87.4125,   13.2114, -112.2490,   12.7619,  -67.2522,\n",
      "         -53.5226,  -55.3891,   86.2332,  105.0563,  -60.0123,   16.5586,\n",
      "         153.8862,   85.6871, -118.9257,  -61.4709,   27.7082,   18.5354,\n",
      "          57.3013,    2.5907,  112.5324,   57.7504, -108.0587,  -26.7976,\n",
      "         -35.2389,   10.7903,  -14.4839,  -88.6098,   95.3541,   16.4837,\n",
      "        -142.6568,   56.7784, -102.6249,  -66.9900,   -2.5215,   78.7925,\n",
      "         -64.7740,  111.9969,  163.1116,  -41.4731,  -15.7520,  123.1359,\n",
      "          83.3274,  145.0083,   15.1947,  -35.6422, -139.2863,  106.4449,\n",
      "        -127.3381,  -10.8931,  -10.6133,   64.7673,  126.7772,    7.2210,\n",
      "        -135.3158, -143.2361,    8.1511,   99.0102,  107.1516,   52.5467,\n",
      "         115.0298,  134.7375,   48.7909,  -90.9524,   20.1191,   39.7496,\n",
      "          59.0448,    5.5404,    5.9231,   10.6074])\n",
      "tensor([  52.5297,   -5.0748,   71.4529, -112.3738,  -43.6660,  -58.7396,\n",
      "         144.9612,   -9.5974,   63.5129, -100.5783,   80.3401,   74.2548,\n",
      "         -69.9850,  -20.0144,   37.0927,  -42.7093,  115.2146,  144.0393,\n",
      "         -43.8326,   63.3534,  -54.8382,   19.2759, -144.9675,  113.3119,\n",
      "          83.6159,   70.6486,   48.6149,  -76.9540,  -17.2142,   66.5361,\n",
      "         115.0852,  -87.4125,   13.2114, -112.2490,   12.7619,  -67.2522,\n",
      "         -53.5226,  -55.3891,   86.2332,  105.0563,  -60.0123,   16.5586,\n",
      "         153.8862,   85.6871, -118.9257,  -61.4709,   27.7082,   18.5354,\n",
      "          57.3013,    2.5907,  112.5324,   57.7504, -108.0587,  -26.7976,\n",
      "         -35.2389,   10.7903,  -14.4839,  -88.6098,   95.3541,   16.4837,\n",
      "        -142.6568,   56.7784, -102.6249,  -66.9900,   -2.5215,   78.7925,\n",
      "         -64.7740,  111.9969,  163.1116,  -41.4731,  -15.7520,  123.1359,\n",
      "          83.3274,  145.0083,   15.1947,  -35.6422, -139.2863,  106.4449,\n",
      "        -127.3381,  -10.8931,  -10.6133,   64.7673,  126.7772,    7.2210,\n",
      "        -135.3158, -143.2361,    8.1511,   99.0102,  107.1516,   52.5467,\n",
      "         115.0298,  134.7375,   48.7909,  -90.9524,   20.1191,   39.7496,\n",
      "          59.0448,    5.5404,    5.9231,   10.6074])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Manually create the first mini-batch without shuffling\n",
    "rpkms_in = torch.Tensor(rpkms[:128]).reshape((128, -1))\n",
    "tnfs_in = torch.Tensor(tnfs[:128]).reshape((128, -1))\n",
    "\n",
    "# Calling the VAE as a function encodes and decodes the arguments,\n",
    "# returning the outputs and the two distribution layers\n",
    "depths_out, tnf_out, mu, logsigma = vae(rpkms_in, tnfs_in)\n",
    "\n",
    "# The mu layer is the encoding itself\n",
    "print(mu[0])\n",
    "\n",
    "# Now, delete the VAE\n",
    "del vae\n",
    "\n",
    "# And reload it:\n",
    "# We need to manually specify whether it should use GPU or not\n",
    "# And whether the network show begin in training or evaluation mode.\n",
    "vae = vamb.encode.VAE.load('/tmp/model.pt', cuda=False, evaluate=True)\n",
    "depths_out, tnf_out, mu, logsigma = vae(rpkms_in, tnfs_in)\n",
    "print(mu[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We get the same values back, meaning the saved network is the same as the loaded network!\n",
    "\n",
    "<a id=\"cluster\"></a>\n",
    "## Binning the encoding\n",
    "\n",
    "__The role of clustering in Vamb__\n",
    "\n",
    "Fundamentally, the process of binning is just clustering sequences based on some of their properties. The purpose of encoding the contigs to a lossy latent representation is to ease the process of clustering because contigs with similar properties are placed close together in latent space, and the latent space is smaller than the input feature space. We believe clustering is thus eased, as the lossy latent representation prerefably loses noise over signal, and because a lower dimensional representation is simply easier to cluster.\n",
    "\n",
    "With the latent representation conveniently represented by an (n_contigs x n_features) matrix, you could use any clustering algorithm to cluster them (such as the ones in `sklearn.cluster`). In practice though, you have perhaps a million contigs and prior constrains on the diameter, shape and size of the clusters, so non-custom clustering algorithms will probably be slow and inaccurate.\n",
    "\n",
    "The module `vamb.cluster` implements a simple and fast iterative medoid clustering algorithm. It is well suited for spherical clusters with a maximum size and for many samples. The algorithm is similar, but subtly different from that used in the metagenomic binner Canopy:\n",
    "\n",
    "![title](algorithm.png)\n",
    "\n",
    "__Determining clustering threshold__\n",
    "\n",
    "You will notice that this algorithm depends on the parameters $T_{inner}$ and $T_{outer}$. This corresponds to the thresholds for Pearson distance, any contigs within which is considered part of the same bin. Getting this measure right is crucial for the binning to work well. Put them too low, and the bins will be highly fragmented. Too high, and distinct genomes will be binned together.\n",
    "\n",
    "In order to estimate a good threshold for this distance, we have written the module `threshold`, in where you can find the function `getthreshold`:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function getthreshold in module vamb.threshold:\n",
      "\n",
      "getthreshold(latent, distfunction, samples, maxsize)\n",
      "    Estimate the clustering threshold from random sampling.\n",
      "    \n",
      "    Inputs:\n",
      "        latent: An (n_contigs x n_latent) Numpy array\n",
      "        distfunction: f such that f(latent, index) returns Numpy array with\n",
      "                      distances to each corresponding contig in latent.\n",
      "        samples: Number of random contigs to sample [1000]\n",
      "        maxsize: Discard sample if more than N contigs are within estimated\n",
      "                 sample threshold [2500]\n",
      "        \n",
      "    Output:\n",
      "        median: Median of estimated clustering threshold\n",
      "        support: Fraction of contigs with any estimated threshold\n",
      "        separation: Fraction of contigs well separated\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(vamb.threshold.getthreshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "How does this work? If you pick a random contig and calculate the distance to all other contigs, the distributions will, roughly speaking follow one of the three distributions, plotted below in blue, orange and green:\n",
    "\n",
    "![title](thresholdcurve.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "This curve is with faked data just for demonstration purposes - in reality, it's a *lot* more noisy.\n",
    "\n",
    "Anyway, the basic idea here is that the distances relative to any contig sometimes separate neatly into a small group of close contigs and all the other contigs. The close contigs can then be assumed to be the within-bin and the far contigs the ones outside the bin.\n",
    "\n",
    "When sampling, we typically see one of three patterns:\n",
    "\n",
    "* The well-separating, here plotted in __green__. For these samples, the smaller peak of close contigs is clearly separated from the further contigs, and the optimal threshold distance is clearly somewhere in the range 0.1 - 0.15. Formally, we define the distances as well-separated as if the density falls to 0.025 or below between the two peaks. The threshold for this sample is then defined as the distance where the density falls to 0.025.\n",
    "\n",
    "* The poorly-seperating samples, here plotted in __orange__. You can see a noticable dip in density at about distance = 0.1, so it appears there indeed is a group of close contigs and a group of far contigs, but the separation is less clear. These are the ones where the density falls to <= 80% of the peak density (in this plot, the valley at ~0.1 is < 0.8 * the peak at ~0.6). The threshold for poorly-separating samples is the point where the density falls below 80% of peak density.\n",
    "\n",
    "* The non-separating samples, here in __blue__. The density at different distances is monotonically increasing, and we cannot find any suitable threshold for these contigs. Formally, we define these as where there are no valleys where the density fall below 80% of the peak value. No threshold is returned for these non-separating samples.\n",
    "\n",
    "We then have defined two terms:\n",
    "\n",
    "* The __support__ is the fraction of well-separating or poorly-separating samples.\n",
    "\n",
    "* The __separation__ is the fraction of well-separating samples.\n",
    "\n",
    "---\n",
    "The `vamb.threshold.getthreshold` function samples a number of random contigs (1000 by default) and returns the median threshold, support and separation. Of course, if the support is too low (< 50%) or the separation too low (< 25%), you can't really trust the threshold.\n",
    "\n",
    "Okay, let's give it a spin\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated threshold: 0.036\n",
      "Support: 56.8 %\n",
      "Separation: 1.5 %\n"
     ]
    }
   ],
   "source": [
    "# We need to normalize the array first across axis 1\n",
    "# otherwise thePearson distance function will not work.\n",
    "normalized = vamb.vambtools.zscore(latent, axis=1)\n",
    "\n",
    "_ = vamb.threshold.getthreshold(normalized, distfunction=vamb.cluster._pearson_distances,\n",
    "                                maxsize=2500, samples=1000)\n",
    "threshold, support, separation = _\n",
    "\n",
    "print('Estimated threshold:', round(threshold, 3))\n",
    "print('Support:', round(support * 100, 1), '%')\n",
    "print('Separation:', round(separation * 100, 1), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Uh oh, only 1.5 % separation, but more support. So less than half the contigs has a little dip in the beginning, and basically none of them are fully separated. This is not very good.\n",
    "\n",
    "It's not that surprising it acts that badly. I only trained the VAE for 10 epochs, far from enough, so the latent representation is quite unrepresentative of the underlying data.\n",
    "\n",
    "I'll cheat here and load in a latent representation I made using the `metabat_errorfree` dataset that comes with [the metabat binner](https://bitbucket.org/berkeleylab/metabat/src/master/).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(182388, 40)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the latent encoding of the metabat dataset\n",
    "# We use the read_tsv instead of read_npz to read .tsv files. Same idea.\n",
    "with open('/home/jakni/Downloads/example/metabat_latent.tsv') as file:\n",
    "    latent = vamb.vambtools.read_tsv(file)\n",
    "\n",
    "# And this is the names of the contigs in a Numpy array\n",
    "with open('/home/jakni/Downloads/example/metabat_contignames.npz', 'rb') as file:\n",
    "    contignames = vamb.vambtools.read_npz(file)\n",
    "    \n",
    "latent.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "180,000 contigs with 40 latent neurons each. I've trained this for 200 epochs. This should separate more easily.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated threshold: 0.109\n",
      "Support: 70.3 %\n",
      "Separation: 52.0 %\n"
     ]
    }
   ],
   "source": [
    "# Again, we need to normalize across axis 1 to be able to get threshold\n",
    "normalized = vamb.vambtools.zscore(latent, axis=1)\n",
    "_ = vamb.threshold.getthreshold(normalized, distfunction=vamb.cluster._pearson_distances,\n",
    "                                maxsize=2500, samples=1000)\n",
    "threshold, support, separation = _\n",
    "\n",
    "print('Estimated threshold:', round(threshold, 3))\n",
    "print('Support:', round(support * 100, 1), '%')\n",
    "print('Separation:', round(separation * 100, 1), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Much* better. Now with a threshold, we can cluster. In Vamb, we have implemented the algorithm as two distinct functions: \n",
    "\n",
    "* `vamb.cluster.cluster`, simply clusters a matrix, and so scales approximately O(n<sup>2</sup>).\n",
    "\n",
    "* `vamb.cluster.tandemcluster` does some very rough preclustering and then clusters each precluster using `vamb.cluster.cluster`. Each observation is then assigned uniquely to the largest cluster it's a member of. This scales better with number of contigs, but is also significantly less accurate.\n",
    "\n",
    "You can use the slow-but-accurate with up to one or two million contigs depending on your patience or ~10 million contigs if you're alright with running it for days.\n",
    "\n",
    "The heavy lifting here is done in Numpy, so it might be worth making sure the BLAS library your Numpy is using is fast. The difference between a fast and a slow Numpy implementation can be quite remarkable. You can check it with `numpy.__config__.show()` and if it says anything other than `NOT AVAILABLE` under the `mkl` or `openblas` entries, you're golden. \n",
    "\n",
    "\n",
    "Let's look at the default clustering function.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function cluster in module vamb.cluster:\n",
      "\n",
      "cluster(matrix, labels=None, inner=None, outer=None, max_steps=15, normalized=False, nsamples=2000, maxsize=2500, logfile=None)\n",
      "    Iterative medoid cluster generator. Yields (medoid), set(labels) pairs.\n",
      "    \n",
      "    Inputs:\n",
      "        matrix: A (obs x features) Numpy matrix of values\n",
      "        labels: None or Numpy array with labels for matrix rows [None = ints]\n",
      "        inner: Optimal medoid search within this distance from medoid [None = auto]\n",
      "        outer: Radius of clusters extracted from medoid. [None = inner]\n",
      "        max_steps: Stop searching for optimal medoid after N futile attempts [15]\n",
      "        normalized: Matrix is already zscore-normalized [False]\n",
      "        nsamples: Estimate threshold from N samples [1000]\n",
      "        maxsize: Discard sample if more than N contigs are within threshold [2500]\n",
      "        logfile: Print threshold estimates and certainty to file [None]\n",
      "    \n",
      "    Output: Generator of (medoid, set(labels_in_cluster)) tuples.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(vamb.cluster.cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First key: gi|224815780|ref|NZ_ACGB01000046.1|_[Acidaminococcus_D21_uid55871]_6593-16101 (of type: <class 'numpy.str_'> )\n",
      "Type of values: <class 'set'>\n",
      "First element of value: gi|224815774|ref|NZ_ACGB01000040.1|_[Acidaminococcus_D21_uid55871]_10199-12698 of type: <class 'numpy.str_'>\n"
     ]
    }
   ],
   "source": [
    "cluster_iterator = vamb.cluster.cluster(latent, contignames, threshold)\n",
    "clusters = dict(cluster_iterator)\n",
    "\n",
    "medoid, contigs = next(iter(clusters.items()))\n",
    "print('First key:', medoid, '(of type:', type(medoid), ')')\n",
    "print('Type of values:', type(contigs))\n",
    "print('First element of value:', next(iter(contigs)), 'of type:', type(next(iter(contigs))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "__The arguments to `cluster.cluster`__\n",
    "\n",
    "You shouldn't have to change any arguments other than `matrix` and `labels`, but for completeness, here's what they are:\n",
    "\n",
    "You already know what `matrix` and `labels` does. If you pass `None` to `labels`, it will use the numbers 1 to `len(matrix)` as labels.\n",
    "\n",
    "`inner` and `outer` correspond to $T_{inner}$ and $T_{outer}$. If `Ǹone` is passed as `inner`, the clustering functions automatically normalize the input and run `vamb.threshold.getthreshold`, then sets $T_{inner} = T_{outer} = threshold$, where $threshold$ is the median returned by `vamb.threshold.getthreshold`. Note that if `inner ==  outer` as is the default, the bins will be disjoint sets of contigs and if not, the bins will have some overlap.\n",
    "\n",
    "`max_steps` is the $N_{samples}$ in the algorithm. In short, it determines how long time the algorithm will search for the optimal medoid. It is not necessary to tweak this, just leave it to the default.\n",
    "\n",
    "If `normalized` is `True`, it will assume the input is already zscore-normalized across axis 1 and skip this. Else, it will create a normalized copy of the data and work on that. The Pearson distance calculation only works for normalized data, so if `normalized` is set to `True` and the input is in fact not normalized, results will definitely be bad.\n",
    "\n",
    "`nsamples` and `maxsize` determines the behaviour of `getthreshold` if `inner` is `None`. It controls the number of sampled contigs to get the median threshold for, and the upper limit for any reported single threshold.\n",
    "\n",
    "If `logfile` is not `None` and `inner` is `None`, the function will print the outputs of `vamb.threshold.getthreshold` to the logfile, and print a warning if the support and separation is too low. If `logfile` is `None` it will print to stderr only if the support and separation is too low.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"postprocessing\"></a>\n",
    "## Step five: Postprocessing the clusters\n",
    "\n",
    "We haven't written any dedicated postprocessing modules because how to postprocess really depends on what you're looking for in your data.\n",
    "\n",
    "One of the greatest weaknesses of Vamb - probably of metagenomic binners in general - is that the bins tend to be highly fragmented. You'll have lots of tiny bins, some of which are legitimate (viruses, plasmids), but most are parts of larger genomes that didn't get binned properly - about 2/3 of the bins here, for example, are 1-contig bins.\n",
    "\n",
    "We're in the process of developing a tool for annotating, cleaning and merging bins based on phylogenetic analysis of the genes in the bins. That would be extremely helpful, but for now, we'll have to use more crude approaches:\n",
    "\n",
    "We throw away all bins with less than 250,000 basepairs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bins before filtering: 27319\n",
      "Number of bins after filtering: 310\n"
     ]
    }
   ],
   "source": [
    "# First let's make a contignames: length dict\n",
    "# Remember, we get these lengths from vamb.parsecontigs.read_contigs\n",
    "with open('/home/jakni/Downloads/example/metabat_lengths.npz', 'rb') as file:\n",
    "    lengths = vamb.vambtools.read_npz(file)\n",
    "\n",
    "lengthof = dict(zip(contignames, lengths))\n",
    "\n",
    "# Now filter away the small bins\n",
    "filtered_bins = dict()\n",
    "for medoid, contigs in clusters.items():\n",
    "    binsize = sum(lengthof[contig] for contig in contigs)\n",
    "    \n",
    "    if binsize >= 250000:\n",
    "        filtered_bins[medoid] = contigs\n",
    "\n",
    "print('Number of bins before filtering:', len(clusters))\n",
    "print('Number of bins after filtering:', len(filtered_bins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "That was almost 99% of the bins we filtered away! Now, let's save the clusters to disk. For this we will use two writer functions:\n",
    "\n",
    "1) `vamb.cluster.writeclusters`, that writes which clusters contains which contigs to a simple tab-separated file, and\n",
    "\n",
    "2) `vamb.vambtools.writebins`, that writes FASTA files corresponding to each of the bins to a directory. This might be useful for some types of analysis you want to do down the road.\n",
    "\n",
    "We will need to load all the contigs belonging to any bin into memory to use `vamb.vambtools.writebins`. If the contigs in your bins don't fit in memory, sorry, you gotta find another way to make those FASTA bins.\n",
    "\n",
    "The cluster name when printing either way will be the dictionary key of the bins. Right now, our bins have names like `gi|345651601|ref|NZ_JH114322.1|_[Bacteroides_dorei_5_1_36_D4_uid55593]_1189469-1207316` - not exactly poetic. We'll rename the bins first.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename bin keys to something less horrible as a file name\n",
    "filtered_bins = {'cluster_' + str(i+1): v for i, v in enumerate(filtered_bins.values())}\n",
    "\n",
    "with open('/home/jakni/Downloads/example/bins.tsv', 'w') as file:\n",
    "    vamb.cluster.write_clusters(file, filtered_bins)\n",
    "\n",
    "# Only keep contigs in any filtered bin in memory\n",
    "allcontigs = set.union(*filtered_bins.values())\n",
    "\n",
    "with open('/home/jakni/Downloads/example/contigs.fna', 'rb') as file:\n",
    "    fastadict = vamb.vambtools.loadfasta(file, keep=allcontigs)\n",
    "    \n",
    "bindir = '/home/jakni/Downloads/example/bins'\n",
    "vamb.vambtools.write_bins(bindir, filtered_bins, fastadict, maxbins=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "## Summary of full workflow\n",
    "\n",
    "This is the full default workflow from beginning to end. The script `run.py` does essentially this, except with some logging and saving intermediate results to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('/home/jakni/Documents/scripts/vamb')\n",
    "import vamb\n",
    "\n",
    "contigpath = '/home/jakni/Downloads/example/contigs.fna'\n",
    "bamdir = '/home/jakni/Downloads/example/bamfiles/'\n",
    "bampaths = [bamdir + filename for filename in os.listdir(bamdir) if filename.endswith('.bam')]\n",
    "\n",
    "with open(contigpath, 'rb') as contigfile:\n",
    "    tnfs, contignames, contiglengths = vamb.parsecontigs.read_contigs(contigfile)\n",
    "\n",
    "rpkms = vamb.parsebam.read_bamfiles(bampaths)\n",
    "vae, dataloader = vamb.encode.trainvae(tnfs, rpkms)\n",
    "latent = vae.encode(dataloader)\n",
    "clusters = dict(vamb.cluster.cluster(latent))\n",
    "\n",
    "with open('/home/jakni/Downloads/example/bins.tsv', 'w') as binfile:\n",
    "    vamb.cluster.write_clusters(binfile, clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"benchmark\"></a>\n",
    "## Optional: Benchmarking your bins\n",
    "\n",
    "If you want to tweak or enchance Vamb, you'll want to know how well it performs. For this to make any sense, you need to have a *reference*, that is, a list of bins that are deemed true and complete. Otherwise, what do you benchmark against?\n",
    "\n",
    "Figuring out a proper way to benchmark binning tools is actually really hard because it requires taking a lot of subjective choices. When is a bin properly predicted? If we only accept perfectly binned bins, basically all binners have a 0 % accuracy. For any one bin we can calculate the fraction of the true genome present and the fraction of contaminating DNA - but if e.g. transposons are harder to bin but have no significant phenotypical effect, should they really count? And if a true bin is split across 25 % in four different observed bins, does that mean 25 % of the true genome is present in those bins, or is it in fact 0 %, because each of those bins will be predicted to be something different? If contigs are present in multiple bins, how do you prevent a bin from having more than 100 % presence? If your reference contains a broad selection of similar strains, surely it's not as bad to mix strains of the same species as to mix genomes from different phyla? And should you be benchmarked on the number of good bins you construct, or the fraction of good bins out of all the bins you make?\n",
    "\n",
    "These choices have a significant impact on how you asses performance of your binner. For example, early on in the development of Vamb, we *did* allow the same contig present in multiple bins to both count towards completeness of bins. This led to a bias towards creating a huge number of overlapping bins until we changed the benchmarking.\n",
    "\n",
    "Vambs benchmarking works in the following way: You count the number of reference bins for which *any* observed bin has a competeness (recall) above a certain level and a contamination below a certain level (precision above a certain level). Recall and precision is calculated with each contig weighed by its length, i.e. on a nucleotide level. An observed bin *can* count towards multiple reference bins given a low enough recall threshold.\n",
    "\n",
    "---\n",
    "The reference you need to benchmark Vamb must be either:\n",
    "\n",
    "* A {clustername: set(contignames)} dict along with a {contigname: length} dict, just like the `clusters` and `lengthof` we made in the previous chapter, *or*\n",
    "\n",
    "* A tab-separated file with (clustername, contigname, length)-rows, one row per contig.\n",
    "\n",
    "The tab-sep file might look like this:\n",
    " \n",
    "    # Any number of headers with a # sign\n",
    "    bin1     contig1      51934\n",
    "    bin1     contig2      2019\n",
    "    bin2     contig3      108716\n",
    "    \n",
    "    [ lines elided ]\n",
    "\n",
    "---\n",
    "\n",
    "Let's take a look at the `Reference` object and the `BenchMarkResult` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_path = '/home/jakni/Downloads/example/metabat_reference.tsv'\n",
    "\n",
    "with open(reference_path) as filehandle:\n",
    "    reference = vamb.benchmark.Reference.fromfile(filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The `reference` object contains a bunch of attributes which keeps track of which contigs belongs to which bins. You can see which ones using good ol' `help`:\n",
    "\n",
    "`>>> help(reference)`\n",
    "\n",
    "    Help on Reference in module vamb.benchmark object:\n",
    "\n",
    "        class Reference(builtins.object)\n",
    "         |  Reference clusters.\n",
    "         |  \n",
    "         |  Init with {name: set_of_contig} dict (a) and {contigname: length} dict (b):\n",
    "         |  >>> reference = Reference(a, b)\n",
    "         |  \n",
    "         |  Init with iterator of tab-sep lines of clustername, contigname, contiglength:\n",
    "         |  >>> with open('/path/to/reference.tsv') as line_iterator:\n",
    "         |  ...     filtered_lines = filter(str.startswith('HUMAN'), line_iterator)\n",
    "         |  ...     reference = Reference.fromfile(filtered_lines) # or with filehandle\n",
    "         |  \n",
    "         |  Attributes:\n",
    "         |      self.nbins: Number of bins\n",
    "         |      self.ncontigs: Number of contigs\n",
    "         |      self.contigsof: binname: set(contigsnames) dict\n",
    "         |      self.binof: contigname: binname dict\n",
    "         |      self.contiglength: contigname: contiglength dict\n",
    "         |      self.binlength: binname: sum_of_contiglengths_for_bin dict\n",
    "\n",
    "    [ ... ]\n",
    "    \n",
    "We also need to instantiate the observed bins.\n",
    "\n",
    "For this, we also need a {clustername: set(contigs)} dict, where each contig is present in the reference. We could load this to memory using the `vamb.cluster.readclusters` function, or we could also instantiate the Observed using `Observed.fromfile` like we did the reference - in that case, we would have to use the metabat_bins.tsv we created in the postprocessing chapter. The `Observed` object is quite similar to the `Reference` object, except it doesn't keep track of contiglengths.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"bins\" is a dict like the one we've made easier with vamb.cluster.cluster\n",
    "with open('/home/jakni/Downloads/example/metabat_bins.tsv') as file:\n",
    "    bins = vamb.cluster.read_clusters(file)\n",
    "observed = vamb.benchmark.Observed(bins, reference)\n",
    "\n",
    "# Keyword-only arguments to make sure you don't accidentally swap them around.\n",
    "# It'll raise an error if you use non-keyword arguments.\n",
    "result = vamb.benchmark.BenchMarkResult(reference=reference, observed=observed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This object also has a bunch of attributes - it calculates the number of reference bins at specified recalls and precisions, the Matthew's correlation coefficient and F-score of each reference bin and keeps some stats on the number of reference vs. observed bins. Again, you can see all the attributes with `help`:\n",
    "\n",
    "`>>> help(result)`\n",
    "\n",
    "    Help on BenchMarkResult in module vamb.benchmark object:\n",
    "\n",
    "        class BenchMarkResult(builtins.object)\n",
    "         |  An object holding some benchmark results:\n",
    "         |  \n",
    "         |  Init from Reference object and Observed object using keywords:\n",
    "         |  >>> result = BenchmarkResult(reference=reference, observed=observed)\n",
    "         |  \n",
    "         |  recall_weight is the weight of recall relative to precision; to weigh\n",
    "         |  precision higher than recall, use a value between 0 and 1.\n",
    "         |  \n",
    "         |  Get number of references found at recall, precision:\n",
    "         |  >>> result[(recall, precision)]\n",
    "         |  Get number of references found at recall or precision\n",
    "         |  >>> result.atrecall/atprecision(recall/precision)\n",
    "         |  Print number of references at all recalls and precisions:\n",
    "         |  >>> result.printmatrix()\n",
    "         |      \n",
    "         |  Attributes:\n",
    "         |      self.nreferencebins: Number of reference bins\n",
    "         |      self.nobservedbins: Number of observed bins\n",
    "         |      self.recalls: Tuple of sorted recalls used to init the object\n",
    "         |      self.precisions: Tuple of sorted precisions used to init the object\n",
    "         |      self.recall_weight: Weight of recall when computing Fn-score\n",
    "         |      self.fscoreof: ref_bin_name: float dict of reference bin Fn-scores\n",
    "         |      self.fmean: Mean fscore\n",
    "         |      self.mccof: ref_bin_name: float dict of MCC values\n",
    "         |      self.mccmean: Mean Matthew's Correlation Coefficient (MCC)\n",
    "         |      self._binsfound: (recall, prec): n_bins Counter\n",
    "     \n",
    "     [ ... ]\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "On the [metabat webpage](https://bitbucket.org/berkeleylab/metabat/wiki/CAMI) they have a neat plot where they plot the number of observed bins at different recalls for a specific specificity. Just for fun, let's recreate that here with the metabat_errorfree data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x144 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x144 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Load results from our run of Metabat2 on Metabat_errorfree dataset\n",
    "with open('/mnt/computerome/projects/deep_bin/paper/metabat_on_metabat/observed.tsv') as file:\n",
    "    obs_metabat = vamb.benchmark.Observed.fromfile(file, reference)\n",
    "    \n",
    "metabatresult = vamb.benchmark.BenchMarkResult(reference=reference, observed=obs_metabat)\n",
    "\n",
    "for precision in 0.95, 0.9:\n",
    "\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    colors = ['#DDDDDD', '#BBBBBB', '#888888', '#666666', '#222222']\n",
    "    recalls = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    for y, benchmark in zip((0, 1), (result, metabatresult)):\n",
    "        for color, recall in zip(colors, recalls):\n",
    "            plt.barh(y, benchmark[recall, precision], color=color)\n",
    "\n",
    "    plt.title(str(precision), fontsize=18)\n",
    "    plt.yticks([0, 1], ['Vamb', 'MetaBat2'], fontsize=16)\n",
    "    plt.xticks([i*25 for i in range(5)], fontsize=13)\n",
    "    plt.legend([str(i) for i in reversed(recalls)], bbox_to_anchor=(1, 1.1), title='Recall', fontsize=12)\n",
    "    \n",
    "    if precision == 0.9:\n",
    "        plt.xlabel('# of Genomes Identified', fontsize=16)\n",
    "    plt.gca().set_axisbelow(True)\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
