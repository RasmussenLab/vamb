{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step three: Train the autoencoder and encode input data\n",
    "\n",
    "Again, you can use `help` to see how to use the module\n",
    "\n",
    "`>>> help(vamb.encode)`\n",
    "\n",
    "    Help on module vamb.encode in vamb:\n",
    "\n",
    "    NAME\n",
    "        vamb.encode - Encode a depths matrix and a tnf matrix to latent representation.\n",
    "\n",
    "    DESCRIPTION\n",
    "        Creates a variational autoencoder in PyTorch and tries to represent the depths\n",
    "        and tnf in the latent space under gaussian noise.\n",
    "\n",
    "        usage:\n",
    "        >>> vae, dataloader = trainvae(depths, tnf) # Make & train VAE on Numpy arrays\n",
    "        >>> latent = vae.encode(dataloader) # Encode to latent representation\n",
    "        >>> latent.shape\n",
    "        (183882, 40)\n",
    "        \n",
    "    [ lines elided ]\n",
    "    \n",
    "---\n",
    "Aha, so we need to use the `trainvae` function first, then the `VAE.encode` method. You can call the `help` functions on those, but I'm not showing that here.\n",
    "\n",
    "Training networks always take some time. If you have a GPU and CUDA installed, you can pass `cuda=True` to `encode.trainvae` to train on your GPU for increased speed. With a beefy GPU, this can make quite a difference. I run this on my laptop, so I'll just use my CPU.\n",
    "\n",
    "Often, you'll want to reuse a pre-trained VAE. For this, I've added the `VAE.save` method of the VAE class, as well as a `VAE.load` method. In this example, I'll ask to write the trained model weights to a file in `/tmp` and show how to reload the VAE again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, we import stuff\n",
    "import sys\n",
    "sys.path.append('/home/jakni/Documents/scripts/')\n",
    "import vamb\n",
    "\n",
    "# And load the data we just saved - of course, if this wasn't in different\n",
    "# notebooks, we could have just kept it in memory\n",
    "with open('/home/jakni/Downloads/example/rpkms.npz', 'rb') as file:\n",
    "    rpkms = vamb.vambtools.read_npz(file)\n",
    "    \n",
    "with open('/home/jakni/Downloads/example/tnfs.npz', 'rb') as file:\n",
    "    tnfs = vamb.vambtools.read_npz(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tLoss: 6.1650\tBCE: 2.0077\tCE: 0.0020501\tMSE: 0.00531\tKLD: 0.00940\n",
      "Epoch: 2\tLoss: 3.6706\tBCE: 1.3009\tCE: 0.0012164\tMSE: 0.01227\tKLD: 0.00911\n",
      "Epoch: 3\tLoss: 3.1486\tBCE: 1.1498\tCE: 0.0010405\tMSE: 0.01821\tKLD: 0.00891\n",
      "Epoch: 4\tLoss: 2.9865\tBCE: 1.1019\tCE: 0.0009851\tMSE: 0.02245\tKLD: 0.00865\n",
      "Epoch: 5\tLoss: 2.9117\tBCE: 1.0800\tCE: 0.0009592\tMSE: 0.02556\tKLD: 0.00851\n"
     ]
    }
   ],
   "source": [
    "# I'm training just 5 epochs for this demonstration.\n",
    "# When actually using the VAE, 200-300 epochs are suitable\n",
    "with open('/tmp/model', 'wb') as modelfile:\n",
    "    vae, dataloader = vamb.encode.trainvae(rpkms, tnfs, nepochs=5, verbose=True, modelfile=modelfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The VAE encodes the high-dimensional (n_samples + 136 features) input data in a lower dimensional space (nlatent features). When training, it learns both the encoding scheme and attempts to reconstruct the input data given the latent representation influenced by gaussian noise.\n",
    "\n",
    "The theory here is that the latent representation should be a more efficient encoding of the input data. If the input data for the contigs indeed do fall into bins, an efficient encoding should be to simply encode the bin they belong to, then use the \"bin identity\" to reconstruct the data. We add noise to prevent it from learning a huge number of slightly different bins, in the most extreme, each bin contains only one contig.\n",
    "\n",
    "The loss of the VAE is the sum of three measures:\n",
    "\n",
    "* Cross entropy (CE) measures the dissimilarity of the reconstructed abundances to observed abundances\n",
    "* Mean squared error (MSE) measures the dissimilary of reconstructed versus observed TNF\n",
    "* Kullback-Leibler divergence (KLD) measures the dissimilarity between the standard normal distribution and the distribution of encoded values with noise added\n",
    "\n",
    "At least in principle, the latter term indudes the VAE to not crazily overfit by imposing some sensible prior on the kind of encodings it can choose.\n",
    "\n",
    "CE is weighted by a factor of 3000 - this constant is pretty ad-hoc and just to make sure that the VAE does not prioritize MSE, which is high but fairly uninformative over CE, which is low but highly informative.\n",
    "\n",
    "We can see the Mean Squared Error (which is the TNF-related loss) is rising these first 5 epochs, presumably as it sacrifices an efficient representation of the TNF in order to learn the depths (whose loss is CE) better. This happens sometimes, and it's alright - after all, co-abundance usually contain more information that TNF, and so we have chosen the CE to be several orders of magnitude higher than the MSE in order for the VAE to be able to make this choice.\n",
    "\n",
    "Also, do note that binary cross entropy (BCE) is also reported. This is another common measure for dissimilarity of reconstruction. It is not used, just displayed for reference.\n",
    "\n",
    "Okay, so now we have the trained `vae` and the `dataloader`. Let's feed the dataloader to the VAE in order to get the latent representation:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39551, 40)\n"
     ]
    }
   ],
   "source": [
    "# No need to pass gpu=True to the encode function to encode on GPU\n",
    "# If you trained the VAE on GPU, it already resides there\n",
    "latent = vae.encode(dataloader)\n",
    "\n",
    "print(latent.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "That's 39551 contigs each represented by the (non-noisy) value of 40 latent neurons.\n",
    "\n",
    "Now we need to cluster this. That's for the next notebook, so again, I'll save the results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/jakni/Downloads/example/latent.npz', 'wb') as file:\n",
    "    vamb.vambtools.write_npz(file, latent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Alright, let me show how to load the trained VAE given the model file we made above.\n",
    "\n",
    "I want to **show** that we get the same network back that we trained, so let's try to feed it the same data twice.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "rpkms_in = torch.Tensor(rpkms[0]).reshape((1, -1))\n",
    "tnfs_in = torch.Tensor(tnfs[0]).reshape((1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.0755, -4.0628,  0.7696,  2.6850,  2.8026,  1.4880, -0.9003,\n",
      "         -3.3665, -2.1976,  4.9359, -4.4380,  2.6925,  4.7105,  1.4558,\n",
      "          1.7984, -1.8792, -3.6274, -0.4964,  2.9361, -0.8234, -3.7444,\n",
      "         -5.0871, -1.1772,  0.2317, -2.1899, -0.4474,  2.8878,  3.1479,\n",
      "          1.8702, -3.7109,  4.4921, -0.4929, -2.2598, -3.8289, -4.3172,\n",
      "          1.1155,  2.9243,  0.3677,  0.8327,  1.7788]])\n"
     ]
    }
   ],
   "source": [
    "depths_out, tnf_out, mu, logsigma = vae(rpkms_in, tnfs_in)\n",
    "print(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.0755, -4.0628,  0.7696,  2.6850,  2.8026,  1.4880, -0.9003,\n",
      "         -3.3665, -2.1976,  4.9359, -4.4380,  2.6925,  4.7105,  1.4558,\n",
      "          1.7984, -1.8792, -3.6274, -0.4964,  2.9361, -0.8234, -3.7444,\n",
      "         -5.0871, -1.1772,  0.2317, -2.1899, -0.4474,  2.8878,  3.1479,\n",
      "          1.8702, -3.7109,  4.4921, -0.4929, -2.2598, -3.8289, -4.3172,\n",
      "          1.1155,  2.9243,  0.3677,  0.8327,  1.7788]])\n"
     ]
    }
   ],
   "source": [
    "# Now, delete the VAE\n",
    "del vae\n",
    "\n",
    "# And reload it:\n",
    "# Annoyingly, PyTorch only works with paths, not with filehandles.\n",
    "# We need to manually specify whether it should use GPU or not\n",
    "# And whether the network show begin in training or evaluation mode\n",
    "vae = vamb.encode.VAE.load('/tmp/model', cuda=False, evaluate=True)\n",
    "depths_out, tnf_out, mu, logsigma = vae(rpkms_in, tnfs_in)\n",
    "print(mu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
