{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step three: Train the autoencoder and encode input data\n",
    "\n",
    "Again, you can use `help` to see how to use the module\n",
    "\n",
    "`>>> help(vamb.encode)`\n",
    "\n",
    "    Help on module vamb.encode in vamb:\n",
    "\n",
    "    NAME\n",
    "        vamb.encode - Encode a depths matrix and a tnf matrix to latent representation.\n",
    "\n",
    "    DESCRIPTION\n",
    "        Creates a variational autoencoder in PyTorch and tries to represent the depths\n",
    "        and tnf in the latent space under gaussian noise.\n",
    "\n",
    "        usage:\n",
    "        >>> vae, dataloader = trainvae(depths, tnf) # Make & train VAE on Numpy arrays\n",
    "        >>> latent = vae.encode(dataloader) # Encode to latent representation\n",
    "        >>> latent.shape\n",
    "        (183882, 40)\n",
    "        \n",
    "    [ lines elided ]\n",
    "    \n",
    "---\n",
    "Aha, so we need to use the `trainvae` function first, then the `VAE.encode` method. You can call the `help` functions on those, but I'm not showing that here.\n",
    "\n",
    "Training networks always take some time. If you have a GPU and CUDA installed, you can pass `cuda=True` to `encode.trainvae` to train on your GPU for increased speed. With a beefy GPU, this can make quite a difference. I run this on my laptop, so I'll just use my CPU.\n",
    "\n",
    "Often, you'll want to reuse a pre-trained VAE. For this, I've added the `VAE.save` method of the VAE class, as well as a `VAE.load` method. In this example, I'll ask to write the trained model weights to a file in `/tmp` and show how to reload the VAE again. But remember - a trained VAE only works on the dataset it's been trained on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, we import stuff\n",
    "import sys\n",
    "sys.path.append('/home/jakni/Documents/scripts/')\n",
    "import vamb\n",
    "\n",
    "# And load the data we just saved - of course, if this wasn't in different\n",
    "# notebooks, we could have just kept it in memory\n",
    "with open('/home/jakni/Downloads/example/rpkms.npz', 'rb') as file:\n",
    "    rpkms = vamb.vambtools.read_npz(file)\n",
    "    \n",
    "with open('/home/jakni/Downloads/example/tnfs.npz', 'rb') as file:\n",
    "    tnfs = vamb.vambtools.read_npz(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE factor:  60.27594766753471\n",
      "MSE factor:  1.0\n",
      "CUDA: False\n",
      "N latent:  40\n",
      "N hidden:  325, 325\n",
      "N contigs:  39551\n",
      "N samples:  6\n",
      "Time is:  2018-07-27 14:57:03.757052\n",
      "Epoch: 1\tLoss: 13.6593\tCE: 0.20374\tMSE: 0.92556\tKLD: 0.45293\n",
      "Epoch: 2\tLoss: 9.0567\tCE: 0.12438\tMSE: 0.87441\tKLD: 0.68541\n",
      "Epoch: 3\tLoss: 8.0029\tCE: 0.10890\tMSE: 0.79769\tKLD: 0.64135\n",
      "Epoch: 4\tLoss: 7.5572\tCE: 0.10388\tMSE: 0.70308\tKLD: 0.59264\n",
      "Epoch: 5\tLoss: 7.3596\tCE: 0.10170\tMSE: 0.66427\tKLD: 0.56503\n"
     ]
    }
   ],
   "source": [
    "# I'm training just 5 epochs for this demonstration.\n",
    "# When actually using the VAE, 200-300 epochs are suitable\n",
    "with open('/tmp/model', 'wb') as modelfile:\n",
    "    vae, dataloader = vamb.encode.trainvae(rpkms, tnfs, nepochs=5, verbose=True, modelfile=modelfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The VAE encodes the high-dimensional (n_samples + 136 features) input data in a lower dimensional space (nlatent features). When training, it learns both the encoding scheme and attempts to reconstruct the input data given the latent representation influenced by gaussian noise.\n",
    "\n",
    "The theory here is that the latent representation should be a more efficient encoding of the input data. If the input data for the contigs indeed do fall into bins, an efficient encoding should be to simply encode the bin they belong to, then use the \"bin identity\" to reconstruct the data. We add noise to prevent it from learning a huge number of slightly different bins, in the most extreme, each bin contains only one contig.\n",
    "\n",
    "The loss of the VAE is the sum of three measures:\n",
    "\n",
    "* Cross entropy (CE) measures the dissimilarity of the reconstructed abundances to observed abundances\n",
    "* Mean squared error (MSE) measures the dissimilary of reconstructed versus observed TNF\n",
    "* Kullback-Leibler divergence (KLD) measures the dissimilarity between the standard normal distribution and the distribution of encoded values with noise added\n",
    "\n",
    "At least in principle, the latter term indudes the VAE to not crazily overfit by imposing some sensible prior on the kind of encodings it can choose.\n",
    "\n",
    "These terms are weigthed with the keyword arguments `errorsum` and `msefraction`. These numbers adjusts CE and MSE for a naïve network such that CE+MSE = `errorsum` and MSE/(CE+MSE) = `msefraction`. A naïve networks predicts MSE ~ N(0,1) and depth = 1/n_samples for every contig. Such a naïve network has an expected uncorrected CE of log(n_samples)/n_samples and an expected MSE of 2. Broadly speaking, `errorsum` gauges how much the network is allowed to learn - a low value constrains the latent layer to its prior - and `msefraction` gauges how much the network cares about TNF as opposed to depth.\n",
    "\n",
    "We can see the KL-divergence rises right in the beginning as it learns the dataset and the latent layer drifts away from its prior. At epoch 3, the penalty associated with KL-divergence outweighs the CE and MSE losses, and the KL divergence falls.\n",
    "\n",
    "Okay, so now we have the trained `vae` and the `dataloader`. Let's feed the dataloader to the VAE in order to get the latent representation:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39551, 40)\n"
     ]
    }
   ],
   "source": [
    "# No need to pass gpu=True to the encode function to encode on GPU\n",
    "# If you trained the VAE on GPU, it already resides there\n",
    "latent = vae.encode(dataloader)\n",
    "\n",
    "print(latent.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "That's 39551 contigs each represented by the (non-noisy) value of 40 latent neurons.\n",
    "\n",
    "Now we need to cluster this. That's for the next notebook, so again, I'll save the results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/jakni/Downloads/example/latent.npz', 'wb') as file:\n",
    "    vamb.vambtools.write_npz(file, latent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Alright, let me show how to load the trained VAE given the model file we made above.\n",
    "\n",
    "I want to **show** that we get the same network back that we trained, so let's try to feed it the same data twice.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "rpkms_in = torch.Tensor(rpkms[:100]).reshape((100, -1))\n",
    "tnfs_in = torch.Tensor(tnfs[:100]).reshape((100, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2797, -0.5739, -0.8438, -1.7344, -0.5020,  1.3880, -0.1967,\n",
      "        -1.4400, -1.2383,  1.2905,  1.4744,  0.1630,  1.3457, -0.3916,\n",
      "         0.7508, -2.6650, -1.3275,  0.0155,  1.1138,  1.0715,  0.4415,\n",
      "        -1.8217, -1.0472, -0.4817, -0.2983,  2.2535,  1.5923,  1.2273,\n",
      "        -0.8661,  0.7274, -0.7957, -1.0649, -0.0674, -0.4920, -0.4447,\n",
      "         0.3880,  1.2463,  2.1039,  0.9956,  0.2469])\n"
     ]
    }
   ],
   "source": [
    "depths_out, tnf_out, mu, logsigma = vae(rpkms_in, tnfs_in)\n",
    "print(mu[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2797, -0.5739, -0.8438, -1.7344, -0.5020,  1.3880, -0.1967,\n",
      "        -1.4400, -1.2383,  1.2905,  1.4744,  0.1630,  1.3457, -0.3916,\n",
      "         0.7508, -2.6650, -1.3275,  0.0155,  1.1138,  1.0715,  0.4415,\n",
      "        -1.8217, -1.0472, -0.4817, -0.2983,  2.2535,  1.5923,  1.2273,\n",
      "        -0.8661,  0.7274, -0.7957, -1.0649, -0.0674, -0.4920, -0.4447,\n",
      "         0.3880,  1.2463,  2.1039,  0.9956,  0.2469])\n"
     ]
    }
   ],
   "source": [
    "# Now, delete the VAE\n",
    "del vae\n",
    "\n",
    "# And reload it:\n",
    "# Annoyingly, PyTorch only works with paths, not with filehandles.\n",
    "# We need to manually specify whether it should use GPU or not\n",
    "# And whether the network show begin in training or evaluation mode.\n",
    "# Also, we need to specify the errorsum and mseratio - we used the defaults,\n",
    "# so I skip that here.\n",
    "vae = vamb.encode.VAE.load('/tmp/model', cuda=False, evaluate=True)\n",
    "depths_out, tnf_out, mu, logsigma = vae(rpkms_in, tnfs_in)\n",
    "print(mu[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
