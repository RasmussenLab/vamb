{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step three: Train the autoencoder and encode input data\n",
    "\n",
    "Again, you can use `help` to see how to use the module\n",
    "\n",
    "`>>> help(vamb.encode)`\n",
    "\n",
    "    Help on module vamb.encode in vamb:\n",
    "\n",
    "    NAME\n",
    "        vamb.encode - Encode a depths matrix and a tnf matrix to latent representation.\n",
    "\n",
    "    DESCRIPTION\n",
    "        Creates a variational autoencoder in PyTorch and tries to represent the depths\n",
    "        and tnf in the latent space under gaussian noise.\n",
    "\n",
    "        usage:\n",
    "        >>> vae, dataloader = trainvae(depths, tnf) # Make & train VAE on Numpy arrays\n",
    "        >>> latent = vae.encode(dataloader) # Encode to latent representation\n",
    "        >>> latent.shape\n",
    "        (183882, 40)\n",
    "        \n",
    "    [ lines elided ]\n",
    "    \n",
    "---\n",
    "Aha, so we need to use the `trainvae` function first, then the `VAE.encode` method. You can call the `help` functions on those, but I'm not showing that here.\n",
    "\n",
    "Training networks always take some time. If you have a GPU and CUDA installed, you can pass `cuda=True` to `encode.trainvae` to train on your GPU for increased speed. With a beefy GPU, this can make quite a difference. I run this on my laptop, so I'll just use my CPU.\n",
    "\n",
    "Sometimes, you'll want to reuse a VAE you have already trained. For this, I've added the `VAE.save` method of the VAE class, as well as a `VAE.load` method. In this example, I'll write the trained model weights to a file in `/tmp` and show how to reload the VAE again. But remember - a trained VAE only works on the dataset it's been trained on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, we import stuff\n",
    "import sys\n",
    "sys.path.append('/home/jakni/Documents/scripts/')\n",
    "import vamb\n",
    "\n",
    "# And load the data we just saved in tutorial part 1 - of course, if this was\n",
    "# the same notebook, we could have just kept it in memory\n",
    "with open('/home/jakni/Downloads/example/rpkms.npz', 'rb') as file:\n",
    "    rpkms = vamb.vambtools.read_npz(file)\n",
    "    \n",
    "with open('/home/jakni/Downloads/example/tnfs.npz', 'rb') as file:\n",
    "    tnfs = vamb.vambtools.read_npz(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCapacity: 3000\n",
      "\tMSE ratio: 0.25\n",
      "\tCUDA: False\n",
      "\tN latent: 100\n",
      "\tN hidden: 325, 325, 325\n",
      "\tN contigs: 39551\n",
      "\tN samples: 6\n",
      "\tN epochs: 5\n",
      "\tBatch size: 128\n",
      "\n",
      "\tEpoch: 1\tLoss: 0.7304685\tCE: 0.2104285\tMSE: 0.8073147\tKLD: 0.4471146\n",
      "\tEpoch: 2\tLoss: 0.4845564\tCE: 0.1317251\tMSE: 0.6136941\tKLD: 0.9170021\n",
      "\tEpoch: 3\tLoss: 0.4078121\tCE: 0.1094842\tMSE: 0.5295789\tKLD: 1.3440626\n",
      "\tEpoch: 4\tLoss: 0.3748378\tCE: 0.1021385\tMSE: 0.4709753\tKLD: 1.7203019\n",
      "\tEpoch: 5\tLoss: 0.3550885\tCE: 0.0985572\tMSE: 0.4275323\tKLD: 2.0380203\n"
     ]
    }
   ],
   "source": [
    "# I'm training just 5 epochs for this demonstration.\n",
    "# When actually using the VAE, 400 epochs are suitable\n",
    "with open('/tmp/model.pt', 'wb') as modelfile:\n",
    "    vae, dataloader = vamb.encode.trainvae(rpkms, tnfs, nepochs=5, modelfile=modelfile, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The VAE encodes the high-dimensional (n_samples + 136 features) input data in a lower dimensional space (nlatent features). When training, it learns an encoding scheme, with which it encodes the input data to a series of normal distributions, and a decoding scheme, in which it uses one value sampled from each normal distribution to reconstruct the input data given the latent representation while influenced by gaussian noise.\n",
    "\n",
    "The theory here is that the latent representation is a more efficient encoding of the input data. If the input data for the contigs indeed do fall into bins, an efficient encoding would be to simply encode the bin they belong to, then use the \"bin identity\" to reconstruct the data. We force it to encode to *distributions* rather than single values because this makes it more robust - it will not as easily overfit to interpret slightly different values as being very distinct if there is an intrinsic noise in each encoding.\n",
    "\n",
    "### The loss function\n",
    "\n",
    "The loss of the VAE consists of three major terms:\n",
    "\n",
    "* Cross entropy (CE) measures the dissimilarity of the reconstructed abundances to observed abundances. This penalizes a failure to reconstruct the abundances accurately.\n",
    "* Mean squared error (MSE) measures the dissimilary of reconstructed versus observed TNF. This penalizes failure to reconstruct TNF accurately.\n",
    "* Kullback-Leibler divergence (KLD) measures the dissimilarity between the encoded distributions and the standard gaussian distribution N(0, 1). This penalizes learning.\n",
    "\n",
    "All three terms are important. CE and MSE is necessary, because we believe the VAE can only learn to effectively reconstruct the input if it learns to encode the signal from the input into the latent layers. In other words, these terms incentivize the network to learn something. KLD is necessary because we care that the encoding is *efficient*, viz. it is contained in as little information as possible. The entire point of encoding is to encode a majority of the signal while shedding the noise, and this is only achieved if we place contrains on how much the network is allowed to learn. Without KLD, the network can theoretically learn an infinitely complex encoding, and the network will learn to encode both noise and signal.\n",
    "\n",
    "In `encode.py`, the loss function is written as:\n",
    "\n",
    "$\\alpha \\cdot CE + \\beta \\cdot MSE + \\gamma \\cdot KLD$\n",
    "\n",
    "Where both CE, MSE and KLD is calculated as means over the vectors for which they are defined. The constants $\\alpha$, $\\beta$ and $\\gamma$ are subject to the following constrains:\n",
    "\n",
    "1. As the learning rate is fixed and optimized for a specific gradient, this means the total loss $\\alpha \\cdot CE + \\beta \\cdot MSE + \\gamma \\cdot KLD$ should sum to a constant, lest the training become ustable. In our code, we want it to sum to 1.\n",
    "\n",
    "2. The amount of information the network can learn depends on the ratio $\\alpha \\cdot CE + \\beta \\cdot MSE \\cdot (\\gamma \\cdot KLD)^{-1} = R$. Because we want our network to learn a *fixed* amount of stuff, KLD can be treated as a constant, and so we let the user define a constant `capacity` and constrain $\\alpha$, $\\beta$ and $\\gamma$ such that $capacity = R \\cdot KLD = \\alpha \\cdot CE + \\beta \\cdot MSE \\cdot \\gamma^{-1}$.\n",
    "\n",
    "3. The relative ratio $\\beta \\cdot MSE \\cdot (\\alpha \\cdot CE + \\beta \\cdot MSE)^{-1}$ controls the incentive to learn to reconstruct TNF as opposed to abundances. This is user defined and called `mseratio` in the code.\n",
    "\n",
    "Now comes a problem. We want to set $\\alpha$, $\\beta$ and $\\gamma$ such that the above equations are satisfied, but we can't know *beforehand* what the CE, KLD or MSE is. And, in any rate, these values changes across the training run.\n",
    "\n",
    "What we do is to set $\\alpha$, $\\beta$ and $\\gamma$ relative to what CE and MSE would be in a totally *naive*, network which had *no knowledge* of the input dataset. This represents the state of the network before *any* learning is done. What would such a network predict? By the nature of our normalization of the means are 0 for the TNF values and 1/n for abundances, so a null model predicts 0 as TNF and, abundance as being $[n^{-1}, n^{-1} ... n^{-1}]^{T}$. This would result in a CE of $ln(n) * n^{-1}$ and an expected MSE of 1. \n",
    "\n",
    "Importantly, these values are rather close to the starting (i.e. untrained) values of a VAE. The KLD of a naive network with the static predictions of above is unfortunately undefined (because such a network could have *any* values of the latent layer and still produce the given outputs). However, we noticed that for reasonable values of $\\alpha$, $\\beta$ and $\\gamma$, it's the case that $\\alpha \\cdot CE + \\beta \\cdot MSE >> \\gamma \\cdot KLD$. Therefore, we modify constraint 1:\n",
    "\n",
    "1. $\\alpha \\cdot CE + \\beta \\cdot MSE = 1$\n",
    "\n",
    "which from constraint 2 implies that:\n",
    "\n",
    "2. $\\gamma = capacity^{-1}$\n",
    "\n",
    "From the above constrains follows:\n",
    "\n",
    "1. $\\alpha = n \\cdot (1 - msefactor) * ln(n)^{-1}$\n",
    "\n",
    "2. $\\beta = msefactor$\n",
    "\n",
    "3. $\\gamma = capacity^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the KL-divergence rises as it learns the dataset and the latent layer drifts away from its prior. At some point, it will begin to overfit too much, and the penalty associated with KL-divergence outweighs the CE and MSE losses. At this point, the KL will stall, and then fall. This point depends on `capacity` and the complexity of the dataset.\n",
    "\n",
    "Okay, so now we have the trained `vae` and the `dataloader`. Let's feed the dataloader to the VAE in order to get the latent representation:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39551, 40)\n"
     ]
    }
   ],
   "source": [
    "# No need to pass gpu=True to the encode function to encode on GPU\n",
    "# If you trained the VAE on GPU, it already resides there\n",
    "latent = vae.encode(dataloader)\n",
    "\n",
    "print(latent.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "That's 39551 contigs each represented by the (non-noisy) value of 40 latent neurons.\n",
    "\n",
    "Now we need to cluster this. That's for the next notebook, so again, I'll save the results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/jakni/Downloads/example/latent.npz', 'wb') as file:\n",
    "    vamb.vambtools.write_npz(file, latent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Alright, let me show how to load the trained VAE given the model file we made above.\n",
    "\n",
    "I want to **show** that we get the same network back that we trained, so let's try to feed it the same data twice.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Manually create the first mini-batch without randomization\n",
    "rpkms_in = torch.Tensor(rpkms[:128]).reshape((128, -1))\n",
    "tnfs_in = torch.Tensor(tnfs[:128]).reshape((128, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6341,  0.8006, -1.2837,  2.2479,  3.0842,  2.0184,  0.3096,\n",
      "        -0.7320,  1.7008, -0.8898,  2.0501,  0.4636, -0.8683,  0.4024,\n",
      "        -1.2859,  0.3301,  0.8071, -1.0957,  0.4424, -1.1223, -0.7120,\n",
      "        -2.1790, -1.9727, -0.8413, -2.6715, -1.0463, -1.6019, -1.8441,\n",
      "         1.7171,  0.3378, -0.8309,  1.1683,  2.1508, -2.0515, -0.3983,\n",
      "        -0.4869, -1.5248, -1.6428,  2.3076, -1.4227])\n"
     ]
    }
   ],
   "source": [
    "# Calling the VAE as a function encodes and decodes the arguments,\n",
    "# returning the outputs and the two distribution layers\n",
    "depths_out, tnf_out, mu, logsigma = vae(rpkms_in, tnfs_in)\n",
    "print(mu[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6341,  0.8006, -1.2837,  2.2479,  3.0842,  2.0184,  0.3096,\n",
      "        -0.7320,  1.7008, -0.8898,  2.0501,  0.4636, -0.8683,  0.4024,\n",
      "        -1.2859,  0.3301,  0.8071, -1.0957,  0.4424, -1.1223, -0.7120,\n",
      "        -2.1790, -1.9727, -0.8413, -2.6715, -1.0463, -1.6019, -1.8441,\n",
      "         1.7171,  0.3378, -0.8309,  1.1683,  2.1508, -2.0515, -0.3983,\n",
      "        -0.4869, -1.5248, -1.6428,  2.3076, -1.4227])\n"
     ]
    }
   ],
   "source": [
    "# Now, delete the VAE\n",
    "del vae\n",
    "\n",
    "# And reload it:\n",
    "# We need to manually specify whether it should use GPU or not\n",
    "# And whether the network show begin in training or evaluation mode.\n",
    "vae = vamb.encode.VAE.load('/tmp/model.pt', cuda=False, evaluate=True)\n",
    "depths_out, tnf_out, mu, logsigma = vae(rpkms_in, tnfs_in)\n",
    "print(mu[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We get the same values back, meaning the saved network is the same as the loaded network!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
