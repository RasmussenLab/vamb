{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walkthough of Vamb from the Python interpreter\n",
    "\n",
    "The Vamb pipeline consist of a handful of tasks each which have a dedicated module:\n",
    "\n",
    "---\n",
    "1) Parse fasta file and get TNF of each sequence, as well as sequence length and names\n",
    "\n",
    "2) Parse the BAM files and get abundance estimate for each sequence in the fasta file\n",
    "\n",
    "3) Train a VAE with the depths and TNF matrices\n",
    "\n",
    "4) Encode the depths and TNF matrices using the VAE\n",
    "\n",
    "5) Cluster the encoded inputs to metgenomic bins\n",
    "\n",
    "---\n",
    "In the following chapters of this walkthrough, we will go through each step in more detail from within the Python interpreter. We will explain what each step does, some of the theory behind the actions, and the different parameters that can be set. With this knowledge, you should be able to extend Vamb relatively easily.\n",
    "\n",
    "For the examples, we will assume the two relevant prerequisite files exists in the directory `/home/jakni/Downloads/example`:\n",
    "\n",
    "* `contigs.fna` - The filtered FASTA contigs which were mapped against, and\n",
    "* `bamfiles/*.bam` - The 6 .bam files from mapping the reads to the contigs above.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step zero: Importing Vamb and getting help\n",
    "\n",
    "First step is to get Vamb imported\n",
    "    \n",
    "    [jakni@nissen:~]$ python\n",
    "    >>> import vamb\n",
    "    Traceback (most recent call last):\n",
    "      File \"<stdin>\", line 1, in <module>\n",
    "    ModuleNotFoundError: No module named 'vamb'\n",
    "    >>> # We're not in the directory containing the vamb directory.\n",
    "    >>> # That means the directory containing the vamb dir is not in out sys.path.\n",
    "    >>> # Either move the vamb directory to one of your sys.path dirs \n",
    "    >>> # or add the vamb directory to sys.path. We'll do the latter.\n",
    "    >>> import sys\n",
    "    >>> sys.path.append('/home/jakni/Documents/scripts/')\n",
    "    >>> import vamb\n",
    "    >>> # No error message - success!\n",
    "\n",
    "You'll almost certianly need help when using Vamb (we wish it was so easy you didn't, but making user friendly software is *hard!*).\n",
    "\n",
    "Luckily, there's the built-in `help` function in Python.\n",
    "\n",
    "---\n",
    "\n",
    "`>>> help(vamb)`\n",
    "    \n",
    "    Help on package vamb:\n",
    "\n",
    "    NAME\n",
    "        vamb - Variational Autoencoder for Metagenomic Binning\n",
    "\n",
    "    DESCRIPTION\n",
    "        Vamb does what it says on the tin - bins metagenomes using a variational autoencoder.\n",
    "        \n",
    "    [ lines elided ]\n",
    "    \n",
    "        General workflow:\n",
    "        1) Filter contigs by size using vamb.filtercontigs\n",
    "        2) Map reads to contigs to obtain BAM file\n",
    "        3) Calculate TNF of contigs using vamb.parsecontigs\n",
    "        4) Create RPKM table using vamb.parsebam\n",
    "        5) Train autoencoder using vamb.encode\n",
    "        6) Cluster latent representation using vamb.cluster\n",
    "    \n",
    "    [ lines elided ]\n",
    "    \n",
    "---\n",
    "    \n",
    "The `PACKAGE CONTENTS` under `help(vamb)` is just a list of all importable files in the `vamb` directory - some of these really shouldn't be imported, so ignore that.\n",
    "\n",
    "---\n",
    "You can also get help for the modules:\n",
    "\n",
    "`>>> help(vamb.cluster)`\n",
    "\n",
    "    Help on module vamb.cluster in vamb:\n",
    "\n",
    "    NAME\n",
    "        vamb.cluster - Iterative medoid clustering of Numpy arrays.\n",
    "\n",
    "    DESCRIPTION\n",
    "        Implements two core functions: cluster and tandemcluster, along with the helper\n",
    "        functions writeclusters and readclusters.\n",
    "        For all functions in this module, a collection of clusters are represented as\n",
    "        a {clustername, set(elements)} dict.\n",
    "\n",
    "        Clustering algorithm:\n",
    "    \n",
    "    [ lines elided ]\n",
    "        \n",
    "---\n",
    "And for functions:\n",
    "\n",
    "`>>> help(vamb.cluster.tandemcluster)`\n",
    "\n",
    "    Help on function tandemcluster in module vamb.cluster:\n",
    "\n",
    "    tandemcluster(matrix, labels, inner, outer=None, max_steps=15, normalized=False)\n",
    "        Splits the datasets, then clusters each partition before merging\n",
    "        the resulting clusters. This is faster, especially on larger datasets, but\n",
    "        less accurate than normal clustering.\n",
    "\n",
    "        Inputs:\n",
    "            matrix: A (obs x features) Numpy matrix of values\n",
    "            labels: Numpy array with labels for matrix rows. None or 1-D array\n",
    "            inner: Optimal medoid search within this distance from medoid\n",
    "            outer: Radius of clusters extracted from medoid. If None, same as inner\n",
    "            max_steps: Stop searching for optimal medoid after N futile attempts\n",
    "            normalized: Matrix is already zscore-normalized [False]\n",
    "\n",
    "        Output: {(partition, medoid): set(labels_in_cluster) dictionary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jakni/Documents/scripts/')\n",
    "import vamb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step one: Parse the FASTA file\n",
    "\n",
    "If you forget what to do at each step, remember that `help(vamb)` said:\n",
    "\n",
    "    General workflow:\n",
    "    1) Filter contigs by size using vamb.filtercontigs\n",
    "    2) Map reads to contigs to obtain BAM file\n",
    "    3) Calculate TNF of contigs using vamb.parsecontigs\n",
    "    \n",
    "    [ lines elided ]\n",
    "\n",
    "Okay, we already have filtered contigs. I could have used the `filtercontigs.py` script in the Vamb directory (only usable from command line) to filter the contigs, but here, they were already filtered. We have already mapped reads to them and gotten BAM files, so we begin with the `vamb.parsecontigs` module. How do you use that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module vamb.parsecontigs in vamb:\n",
      "\n",
      "NAME\n",
      "    vamb.parsecontigs - Calculate z-normalized tetranucleotide frequency from a FASTA file.\n",
      "\n",
      "DESCRIPTION\n",
      "    Usage:\n",
      "    >>> with open('/path/to/contigs.fna', 'rb') as filehandle\n",
      "    ...     tnfs, contignames, lengths = read_contigs(filehandle)\n",
      "\n",
      "FUNCTIONS\n",
      "    read_contigs(byte_iterator, minlength=100)\n",
      "        Parses a FASTA file open in binary reading mode.\n",
      "        \n",
      "        Input:\n",
      "            byte_iterator: Iterator of binary lines of a FASTA file\n",
      "            minlength[100]: Ignore any references shorter than N bases \n",
      "        \n",
      "        Outputs:\n",
      "            tnfs: A (n_FASTA_entries x 136) matrix of tetranucleotide freq.\n",
      "            contignames: A list of contig headers\n",
      "            lengths: A Numpy array of contig lengths\n",
      "\n",
      "DATA\n",
      "    TNF_HEADER = '#contigheader\\tAAAA/TTTT\\tAAAC/GTTT\\tAAAG/CTTT\\tAAAT...A...\n",
      "    __cmd_doc__ = 'Calculate z-normalized tetranucleotide frequency...eoti...\n",
      "\n",
      "FILE\n",
      "    /home/jakni/Documents/scripts/vamb/parsecontigs.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(vamb.parsecontigs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "I use `vamb.parsecontigs.read_contigs` with the inputs and outputs as written:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of tnfs: <class 'numpy.ndarray'> of dtype float32\n",
      "Shape of tnfs: (39551, 136)\n",
      "\n",
      "Type of contignames: <class 'list'>\n",
      "Length of contignames: 39551\n",
      "\n",
      "First 10 elements of contignames:\n",
      "s30_NODE_1_length_245508_cov_18.4904\n",
      "s30_NODE_2_length_222690_cov_39.7685\n",
      "s30_NODE_3_length_222459_cov_20.3665\n",
      "s30_NODE_4_length_173155_cov_20.1181\n",
      "s30_NODE_5_length_161239_cov_20.1237\n",
      "s30_NODE_6_length_157102_cov_20.734\n",
      "s30_NODE_7_length_156768_cov_44.8078\n",
      "s30_NODE_8_length_152691_cov_19.6759\n",
      "s30_NODE_9_length_121154_cov_21.6491\n",
      "s30_NODE_10_length_119726_cov_136.834\n",
      "\n",
      "\n",
      "Type of lengths: <class 'numpy.ndarray'> of dtype int64\n",
      "Length of lengths: 39551\n",
      "\n",
      "First 10 elements of lengths:\n",
      "245508\n",
      "222690\n",
      "222459\n",
      "173155\n",
      "161239\n",
      "157102\n",
      "156768\n",
      "152691\n",
      "121154\n",
      "119726\n"
     ]
    }
   ],
   "source": [
    "# Open the file in binary mode - you can use the vamb.vambtools.Reader to read\n",
    "# from normal or gzipped file seamlessly. Here I just use the open function\n",
    "with open('/home/jakni/Downloads/example/contigs.fna', 'rb') as filehandle:\n",
    "    tnfs, contignames, lengths = vamb.parsecontigs.read_contigs(filehandle)\n",
    "\n",
    "# Let's have a look at the resulting data\n",
    "\n",
    "print('Type of tnfs:', type(tnfs), 'of dtype', tnfs.dtype)\n",
    "print('Shape of tnfs:', tnfs.shape, end='\\n\\n')\n",
    "\n",
    "print('Type of contignames:', type(contignames))\n",
    "print('Length of contignames:', len(contignames), end='\\n\\n')\n",
    "\n",
    "print('First 10 elements of contignames:')\n",
    "for i in range(10):\n",
    "    print(contignames[i])\n",
    "    \n",
    "print('\\n')\n",
    "    \n",
    "print('Type of lengths:', type(lengths), 'of dtype', lengths.dtype)\n",
    "print('Length of lengths:', len(lengths), end='\\n\\n')\n",
    "\n",
    "print('First 10 elements of lengths:')\n",
    "for i in range(10):\n",
    "    print(lengths[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "It turns out that related organisms tend to share a similar kmer-distribution across most of their genome. The reason for that is not understood, even though it's believed that common functional motifs, GC-content and presence/absence of endonucleases explains some of the observed similary.\n",
    "\n",
    "The `tnfs` is the tetranucleotide frequency - it's the frequency of the canonical kmer of each 4mer in the contig. The matrix is z-score normalized across contigs for each sample such that the frequency of e.g. 'AGGC' is measured relative to other contigs in that sample - this increases the signal-to-noise ratio.\n",
    "\n",
    "We use 4-mers because there are 136 canonical 4-mers, which is an appropriate number of features to cluster - not so few that there's no signal and not so many it becomes unwieldy and the estimates of the frequencies become uncertain. We could also have used 3-mers and 5-mers. In tests we have made, 3-mers are _almost_, but not quite as good as 4-mers for separating different species. There are 512 canonical 5-mers, that would be too many features to handle comfortably, and it could easily cause memory issues. You could probably switch tetranucleotide frequency to trinucleotide frequency in Vamb without any significant drop of accuracy.\n",
    "\n",
    "At this points, you should probably consider whether you can keep everything in memory. If not, all the relevant modules have reading and writing functions so you can dump the results to disk and delete them from memory. This is a small dataset, so there's no problem. With hundreds of samples and millions of contigs however, this becomes a problem, even though Vamb is fairly memory-friendly.\n",
    "\n",
    "As a rule of thumb, the memory consumption for the most memory intensive step is approximately 8 × (n_samples + 136) × n_contigs bytes plus a little bit of overhead. If this is much lower than your RAM, don't worry about it. If it's within a factor 2 of your available RAM, you'll need to delete objects you don't need anymore.\n",
    "\n",
    "In my example, I have 6 samples and 39551 contigs for a total memory usage of ~45 MB.\n",
    "\n",
    "---\n",
    "\n",
    "## Step two: Parse the BAM files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function read_bamfiles in module vamb.parsebam:\n",
      "\n",
      "read_bamfiles(paths, minscore=50, minlength=100, processes=4)\n",
      "    Spawns processes to parse BAM files and get contig rpkms.\n",
      "        \n",
      "    Input:\n",
      "        path: Path to BAM file\n",
      "        minscore [50]: Minimum alignment score (AS field) to consider\n",
      "        minlength [100]: Ignore any references shorter than N bases \n",
      "        processes [4]: Number of processes to spawn\n",
      "    \n",
      "    Outputs:\n",
      "        sample_rpkms: A {path: Numpy-32-float-RPKM} dictionary\n",
      "        contignames: A list of contignames from first BAM header\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Again, we can use the help function to see what we need to do\n",
    "help(vamb.parsebam.read_bamfiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We can see (in the default value for the `processes` argument) that the default number of parallel BAM-reading processes it will spawn is 4. This is because Python detected 4 threads on my laptop. In general, VAMBs default here is to use the number of availbel threads, or 8 threads if more than 8 is detected. At this point, the BAM-reading will almost certainly become IO bound.\n",
    "\n",
    "As with the `vamb.parsecontigs.read_contigs` function, I don't care about the `minlength` argument, since our fasta file is already filtered. Again, I will re-iterate that filtering the FASTA file _before_ mapping leads to the best results.\n",
    "\n",
    "Lastly, the function ignores all alignments with alignment score less than 50 (as determined by the optional `AS:i` field in the BAM file). That seems reasonable here.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jakni/Downloads/example/bamfiles/e101.filtered.bam',\n",
       " '/home/jakni/Downloads/example/bamfiles/e178.filtered.bam',\n",
       " '/home/jakni/Downloads/example/bamfiles/e179.filtered.bam',\n",
       " '/home/jakni/Downloads/example/bamfiles/e196.filtered.bam',\n",
       " '/home/jakni/Downloads/example/bamfiles/e198.filtered.bam',\n",
       " '/home/jakni/Downloads/example/bamfiles/e30.filtered.bam']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bamfiles = !ls /home/jakni/Downloads/example/bamfiles\n",
    "bamfiles = ['/home/jakni/Downloads/example/bamfiles/' + p for p in bamfiles]\n",
    "bamfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of sample_rpkms: <class 'dict'>\n",
      "Content of the dict:\n",
      "First key: /home/jakni/Downloads/example/bamfiles/e101.filtered.bam\n",
      "First value: [0.11535063 0.17579393 0.58409214 ... 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "# That looks right.\n",
    "\n",
    "# We already have the contignames, so who cares about saving that\n",
    "sample_rpkms, _ = vamb.parsebam.read_bamfiles(bamfiles)\n",
    "del _\n",
    "\n",
    "print('Type of sample_rpkms:', type(sample_rpkms))\n",
    "\n",
    "print('Content of the dict:')\n",
    "print('First key:', next(iter(sample_rpkms.keys())))\n",
    "print('First value:', next(iter(sample_rpkms.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The idea here is that two contigs from the same genome will always be physically present together, and so they should have a similar abundance across all samples. Some contigs represent repeats like duplicated segments - these contigs should have a fixed ratio of abundance to other contigs. Thus, even when considering repeated contigs, there should be a tight Pearson correlation between abundances of contigs from the same genome.\n",
    "\n",
    "The `vamb.parsebam` module takes a rather crude approach to estimating abundance, namely by simply counting the number of mapped reads to each contig, divided by total number of reads and the contig's length. This measure is in trancriptomics often called RPKM, *reads per kilobase per million mapped reads*. Other metagenomic binners like Metabat and Canopy uses an average of per-nucleotide depth of coverage instead. We do not believe there is any theoretical or practical advantage of using depth over RPKM. Because BWA handles redundant databases rather poorly, there is not even any advantage of using FPKM over RPKM. We will use the terms *depth* and *rpkm* interchangably.\n",
    "\n",
    "The object we just created, `sample_rpkms` is the depth table in the form of a dictionary with one column (representing one BAM file) per entry. Just like the TNF, this needs to be converted to a (n_contigs x n_features) Numpy array in order for it to be used in the variational autoencoder.\n",
    "\n",
    "We can use the function `vamb.parsebam.toarray` to do this. This will create a new array in memory while retaining `sample_rpkms`. Each object requires approximately 4 x n_contigs x n_samples bytes. If having two of these objects in memory will consume all your RAM, you can dump the `sample_rpkms` to disk, delete it, and reload it in a matrix using the functions `write_rpkms` and `array_fromnpz` in the `vamb.parsebam` module.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpkms = vamb.parsebam.toarray(sample_rpkms, bamfiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now, I tend to be a bit ~~paranoid~~<sup>careful</sup>, so if I loaded in 500 GB of BAM files, I'd want to save the work I have now in case something goes wrong - and we're about to fire up the VAE so lots of things can go wrong.\n",
    "\n",
    "What importants objects do I have in memory right now?\n",
    "\n",
    "* `contignames`: A list of contignames\n",
    "* `lengths`: A Numpy array of contig lengths\n",
    "* `rpkms`: A Numpy array of rpkms\n",
    "* `tnfs`: A Numpy array of tnfs\n",
    "\n",
    "I'm going to use `pickle` to save the Python list and `vamb.vambtools.write_npz` to save the Numpy arrays (the latter is just a wrapper for `numpy.savez_compressed`). Of course, I could have used `pickle` for it all.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('/home/jakni/Downloads/example/contignames.pickle', 'wb') as file:\n",
    "    pickle.dump(contignames, file, protocol=4)\n",
    "\n",
    "with open('/home/jakni/Downloads/example/lengths.npz', 'wb') as file:\n",
    "    vamb.vambtools.write_npz(file, lengths)\n",
    "\n",
    "with open('/home/jakni/Downloads/example/tnfs.npz', 'wb') as file:\n",
    "    vamb.vambtools.write_npz(file, tnfs)\n",
    "    \n",
    "with open('/home/jakni/Downloads/example/rpkms.npz', 'wb') as file:\n",
    "    vamb.vambtools.write_npz(file, rpkms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
